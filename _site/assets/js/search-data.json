{"0": {
    "doc": "📆 Calendar",
    "title": "📆 Calendar",
    "content": "Welcome to Practical Data Science in Winter 2025! This site is currently under construction. Until this disclaimer is removed, all information here is subject to change. See you on January 8th! . ",
    "url": "/calendar/",
    
    "relUrl": "/calendar/"
  },"1": {
    "doc": "⚙️ Environment Setup",
    "title": "⚙️ Environment Setup",
    "content": "Welcome to Practical Data Science in Winter 2025! This site is currently under construction. Until this disclaimer is removed, all information here is subject to change. See you on January 8th! . ",
    "url": "/env-setup/",
    
    "relUrl": "/env-setup/"
  },"2": {
    "doc": "🙋 FAQs",
    "title": "🙋FAQs",
    "content": "Welcome to Practical Data Science in Winter 2025! This site is currently under construction. Until this disclaimer is removed, all information here is subject to change. See you on January 8th! . ",
    "url": "/faqs/#faqs",
    
    "relUrl": "/faqs/#faqs"
  },"3": {
    "doc": "🙋 FAQs",
    "title": "🙋 FAQs",
    "content": " ",
    "url": "/faqs/",
    
    "relUrl": "/faqs/"
  },"4": {
    "doc": "Portfolio Homework 📊",
    "title": "Portfolio Homework 📊",
    "content": ". | Checkpoint due Monday, November 25th at 11:59PM (no slip days allowed!) | Final Submission due Saturday, December 7th at 11:59PM (no slip days allowed!) | . last updated November 5th at 9:49PM . Welcome to Portfolio Homework, the final homework assignment of the semester! 👋 . This homework is a mini-homework of sorts, and aims to be a culmination of everything you’ve learned this semester. In the homework, you will conduct an open-ended investigation into one of the three datasets (Recipes and Ratings 🍽, League of Legends ⌨️, or Power Outages 🔋), or – upon permission of the instructor – choose your own. Specifically, you’ll draw several visualizations to help understand the distributions of key variables, build and improve a predictive model, and share your findings with everyone on the internet. The Portfolio Homework is worth 100 points total; the breakdown is described in the Rubric section at the bottom of this page. The Portfolio Homework is different from other homeworks in a few crucial ways: . | You can work with one partner (but don’t have to). If you choose to work with a partner, read the Partner Guidelines section at the bottom. | There is a checkpoint, due right before Thanksgiving, worth 15 points out of the 100 points the homework is scored out of. It exists to make sure you’ve picked a dataset and started some preliminary work. See the Checkpoint Submission section for more details and for the Gradescope submission link. | You cannot use slip days on either the checkpoint or the final submission, since they’re due so close to the end of the semester that we need all the time we can get to grade them. All components of the homework are manually graded. | You cannot drop the Portfolio Homework – it will be a part of your overall homework grade no matter what. Your lowest score among Homeworks 1-11 will be the one that is dropped. | As your final deliverables, you’ll submit two things to us: . | a public-facing website. We’ll eventually create a public “showcase” site that has links to everyone’s submissions – that is, your website will be available to the entire internet! | a PDF of your Jupyter Notebook. | . | . Before we get into the details of what’s expected of you, note that this homework in particular is meant to encourage you to build something you’re proud of. This is a chance to put something concrete on your resume to show potential employers. Grading will likely be lenient, but your work will be publicly available forever! . As alluded to above, the homework is broken into two parts: . | Part 1: An analysis, submitted as a Jupyter Notebook. This will contain the details of your work. Focus on completing your analysis before moving to Part 2, as the analysis is the bulk of the homework. | Part 2: A report, submitted as a website. This will contain a narrative “story” with visuals. Focus on this after finishing most of your analysis. | . The homework is worth a total of 100 points. You can see the distribution of points in the Rubric section at the very bottom. ",
    "url": "/portfolio/#portfolio-homework-",
    
    "relUrl": "/portfolio/#portfolio-homework-"
  },"5": {
    "doc": "Portfolio Homework 📊",
    "title": "Table of Contents",
    "content": ". | Choosing a Dataset . | Default Options | Choosing Your Own Dataset | . | Part 1: Analysis . | Step 1: Introduction | Step 2: Data Cleaning and Exploratory Data Analysis | Step 3: Framing a Prediction Problem | Step 4: Baseline Model | Step 5: Final Model | Style | . | Part 2: Report . | Step 1: Initializing a Jekyll GitHub Pages Site | Step 2: Choosing a Theme | Step 3: Embedding Content | Local Setup | . | Submission and Rubric . | Checkpoint Submission | Final Submission | Rubric | . | Partner Guidelines | . ",
    "url": "/portfolio/#table-of-contents",
    
    "relUrl": "/portfolio/#table-of-contents"
  },"6": {
    "doc": "Portfolio Homework 📊",
    "title": "Choosing a Dataset",
    "content": "In this homework, you will perform an open-ended investigation into a single dataset. Default Options . We expect that the majority of students will choose one of the following three datasets. Recipes and Ratings 🍽    League of Legends ⌨️    Power Outages 🔋 . The dataset description pages linked above each have three sections: . | Getting the Data: Describes how to access the data and, in some cases, what various features mean. (In general, you’re going to have to understand what your data means on your own!) You’re welcome to download additional data to help with your analyses, in addition to using the data that’s provided for you. | Example Questions and Prediction Problems: Use these as inspiration, but feel free to come up with your own questions and prediction problems! | Special Considerations: Things to be aware of when working with the given dataset, e.g. some additional requirements. | . When selecting which dataset you are going to use for your homework, try choosing the one whose topic appeals to you the most as that will make finishing the homework a lot more enjoyable. To help contextualize the kinds of analysis you can do in this homework, it might help to look at these examples from a related course taught at UC San Diego. These examples offer insights into crafting effective research questions, but bear in mind that they have their own strengths and weaknesses. Treat them as a foundation for inspiration, but don’t just repeat or copy their work – be original! Also note that the UC San Diego version of this assignment had slightly more requirements, so you’ll see sections involving (for example) hypothesis tests that you aren’t expected to have. | League of Legends First Blood Statistical Analysis: This homework excelled in clarifying their research aims, making the study understandable to a broader audience. In your own homework, ensure that you provide a lucid and detailed explanation of your research focus. | Analyzing Power Outages: This homework presents a noval way to do the data visualization. In your homework, please think about what is the best way to present your data. | . Before choosing a dataset, read the rest of this page to see what’s required of you! . Choosing Your Own Dataset . You may not be interested in any of the above datasets, or may already be doing research/other work involving a dataset that is of particular interest to you. If that’s the case, you may be permitted to use a different dataset. To request approval, send Suraj (rampure@umich.edu) an email with the following: . | A one paragraph description of why you’ve chosen this dataset and your plans for analysis. Comment on what draws you to this dataset over the default options, what preliminary questions you plan to answer, and what features you plan to use in a predictive model. Part of our filtering is making sure that your plans are sufficiently scoped for the homework – that is, your proposal is neither too easy nor too difficult – so the more details, the better. | Proof that you already have access to the data you want to work with. Ideally, attach a link to the data source or the file so that we can see it ourselves and verify that your plans are possible. | . The deadline to request approval to use a different dataset is the same as the deadline of the checkpoint, November 25th. Once you email Suraj with the above, you should hear back within 48 hours with your approval or denial and reasoning. Note that submitting the checkpoint is not the same as requesting approval; the only way to request approval to use a different dataset is to email Suraj with answers to the questions above. After November 25th, if you haven’t requested approval, you must choose one of the default three options. ",
    "url": "/portfolio/#choosing-a-dataset",
    
    "relUrl": "/portfolio/#choosing-a-dataset"
  },"7": {
    "doc": "Portfolio Homework 📊",
    "title": "Part 1: Analysis",
    "content": "Before beginning your analysis, you’ll need to set up a few things. | Pull the latest version of the course GitHub repo, github.com/practicaldsc/fa24. Within the homeworks/portfolio folder, there is a template.ipynb notebook that you will use as a template for the homework. If you delete the file or want another copy of the template, you can re-download it from here. This is where your analysis will live; you will submit this entire notebook to us. | Download the dataset you chose and load it into your template notebook. | . Once you have your dataset loaded in your notebook, it’s time for you to find meaning in the real-world data you’ve collected! Follow the steps below. For each step, we specify what must be done in your notebook and what must go on your website, which we expand on in Part 2. We recommend you write everything in your notebook first, and then move things over to your website once you’ve completed your analysis. In Steps 1-2, you’ll develop a deeper understanding of your dataset while trying to answer a single question. Step 1: Introduction . | Step | Analysis in Notebook | Report on Website | . | Introduction and Question Identification | Understand the data you have access to. Brainstorm a few questions that interest you about the dataset. Pick one question you plan to investigate further. (As the data science lifecycle tells us, this question may change as you work on your homework.) | Provide an introduction to your dataset, and clearly state the one question your homework is centered around. Why should readers of your website care about the dataset and your question specifically? Report the number of rows in the dataset, the names of the columns that are relevant to your question, and descriptions of those relevant columns. | . Step 2: Data Cleaning and Exploratory Data Analysis . | Step | Analysis in Notebook | Report on Website | . | Data Cleaning | Clean the data appropriately. For instance, you may need to replace data that should be missing with NaN or create new columns out of given ones (e.g. compute distances, apply numerical-to-numerical transformations, or get time information from time stamps). | Describe, in detail, the data cleaning steps you took and how they affected your analyses. The steps should be explained in reference to the data generating process. Show the head of your cleaned DataFrame (see Part 2: Report for instructions). | . | Univariate Analysis | Look at the distributions of relevant columns separately by using DataFrame operations and drawing at least two relevant plots. | Embed at least one plotly plot you created in your notebook that displays the distribution of a single column (see Part 2: Report for instructions). Include a 1-2 sentence explanation about your plot, making sure to describe and interpret any trends present, and how they answer your initial question. (Your notebook will likely have more visualizations than your website, and that’s fine. Feel free to embed more than one univariate visualization in your website if you’d like, but make sure that each embedded plot is accompanied by a description.) | . | Bivariate Analysis | Look at the statistics of pairs of columns to identify possible associations. For instance, you may create scatter plots and plot conditional distributions, or box plots. You must plot at least two such plots in your notebook. Be creative! | Embed at least one plotly plot that displays the relationship between two columns. Include a 1-2 sentence explanation about your plot, making sure to describe and interpret any trends present and how they answer your initial question. (Your notebook will likely have more visualizations than your website, and that’s fine. Feel free to embed more than one bivariate visualization in your website if you’d like, but make sure that each embedded plot is accompanied by a description.) | . | Interesting Aggregates | Choose columns to group and pivot by and examine aggregate statistics. | Embed at least one grouped table or pivot table in your website and explain its significance. | . | Imputation | If needed for further analyses, impute any missing values. | If you imputed any missing values, visualize the distributions of the imputed columns before and after imputation. Describe which imputation technique you chose to use and why. If you didn’t fill in any missing values, discuss why not. | . In Steps 3-5, you will build a predictive model, based on the knowledge of your dataset you developed in Steps 1-2. Step 3: Framing a Prediction Problem . | Step | Analysis in Notebook | Report on Website | . | Problem Identification | Identify a prediction problem. Feel free to use one of the example prediction problems stated in the “Example Questions and Prediction Problems” section of your dataset’s description page or pose one of your own. The prediction problem you come up with doesn’t have to be related to the question you were answering in Steps 1-2, but ideally, your entire homework has some sort of coherent theme. | Clearly state your prediction problem and type (classification or regression). If you are building a classifier, make sure to state whether you are performing binary classification or multiclass classification. Report the response variable (i.e. the variable you are predicting) and why you chose it, the metric you are using to evaluate your model and why you chose it over other suitable metrics (e.g. accuracy vs. F1-score).Note: Make sure to justify what information you would know at the “time of prediction” and to only train your model using those features. For instance, if we wanted to predict your Final Exam grade, we couldn’t use your Portfolio Homework grade, because we (probably) won’t have the Portfolio Homework graded before the Final Exam! Feel free to ask questions if you’re not sure. | . Step 4: Baseline Model . | Step | Analysis in Notebook | Report on Website | . | Baseline Model | Train a “baseline model” for your prediction task that uses at least two features. (For this requirement, two features means selecting at least two unique columns from your original dataset.) You can leave numerical features as-is, but you’ll need to take care of categorical columns using an appropriate encoding. Implement all steps (feature transforms and model training) in a single sklearn Pipeline. Note: Both now and in Step 5: Final Model, make sure to evaluate your model’s ability to generalize to unseen data! There is no “required” performance metric that your baseline model needs to achieve. | Describe your model and state the features in your model, including how many are quantitative, ordinal, and nominal, and how you performed any necessary encodings. Report the performance of your model and whether or not you believe your current model is “good” and why.Tip: Make sure to hit all of the points above: many Portfolio Homeworks in the past have lost points for not doing so. | . Step 5: Final Model . | Step | Analysis in Notebook | Report on Website | . | Final Model | Create a “final” model that improves upon the “baseline” model you created in Step 4. Do so by engineering at least two new features from the data, on top of any categorical encodings you performed in Baseline Model Step. (For instance, you may use a StandardScaler on a quantitative column and a QuantileTransformer transformer on a different column to get two new features.) Again, implement all steps in a single sklearn Pipeline. While deciding what features to use, you must perform a search for the best hyperparameters (e.g. tree depth) to use amongst a list(s) of options, either by using GridSearchCV or through some manual iterative method. In your notebook, state which hyperparameters you plan to tune and why before actually tuning them.Optional: You are encouraged to try many different modeling algorithms for your final model (i.e. LinearRegression, RandomForestClassifier, Lasso, SVC, etc.) If you do this, make sure to clearly indicate in your notebook which model is your actual final model as that will be used to grade the above requirements.Note 1: When training your model, make sure you use the same training and testing sets from your baseline model. This way, the evaluation metric you get on your final model can be compared to your baseline’s on the basis of the model itself and not the dataset it was trained on. Based on which method you use for hyperparameter tuning, this may mean that you will need to use some of your training data as your validation data. If this is the case, make sure to train your final model on the whole training set to evaluation.Note 2: You will not be graded on “how much” your model improved from Step 4: Baseline Model to Step 5: Final Model. What you will be graded on is on whether or not your model improved, as well as your thoughtfulness and effort in creating features, along with the other points above.Note 3: Don’t try to improve your model’s performance just by blindly transforming existing features into new ones. Think critically about what each transformation you’re doing actually does. For example, there’s no use in using a StandardScaler transformer if your goal is to reduce the MSE of a linear model: as we learned in Lecture 18, standardizing features in a regression model does not change the model’s predictions, only its coefficients! | State the features you added and why they are good for the data and prediction task. Note that you can’t simply state “these features improved my accuracy”, since you’d need to choose these features and fit a model before noticing that – instead, talk about why you believe these features improved your model’s performance from the perspective of the data generating process. Describe the modeling algorithm you chose, the hyperparameters that ended up performing the best, and the method you used to select hyperparameters and your overall model. Describe how your Final Model’s performance is an improvement over your Baseline Model’s performance.Optional: Include a visualization that describes your model’s performance, e.g. a confusion matrix, if applicable. | . Style . While your website will be neatly organized and tailored for public consumption, it is important to keep your analysis notebook organized as well. Follow these guidelines: . | Your work for each of the five homework steps described above (Introduction, Data Cleaning and Exploratory Data Analysis, …, Final Model) should be completed in code cells underneath the Markdown header of that section’s name. | You should only include work that is relevant to posing, explaining, and answering the question(s) you state in your website. You should include data quality, cleaning, and missingness assessments, and intermediate models and features you tried, though these should broadly be relevant to the tasks at hand. | Make sure to clearly explain what each component of your notebook means. Specifically: . | All plots should have titles, labels, and a legend (if applicable), even if they don’t make it into your website. Plots should be self-contained – readers should be able to understand what they describe without having to read anything else. | All code cells should contain a comment describing how the code works (unless the code is self-explanatory – use your best judgement). | . | . ",
    "url": "/portfolio/#part-1-analysis",
    
    "relUrl": "/portfolio/#part-1-analysis"
  },"8": {
    "doc": "Portfolio Homework 📊",
    "title": "Part 2: Report",
    "content": "The purpose of your website is to provide the general public – your classmates, friends, family, recruiters, and random internet strangers – with an overview of your homework and its findings, without forcing them to understand every last detail. We don’t expect the website creation process to take very much time, but it will certainly be rewarding. Once you’ve completed your analysis and know what you will put in your website, start reading this section. Your website must clearly contain the following five headings, corresponding to the five steps mentioned in Part 1: . | Introduction | Data Cleaning and Exploratory Data Analysis | Framing a Prediction Problem | Baseline Model | Final Model | . Don’t include “Step 1”, “Step 2”, etc. in your headings – the headings should be exactly as they are above. The specific content your website needs to contain is described in the “Report on Website” columns above. Make sure to also give your website a creative title that relates to the question you’re trying to answer, and clearly state your name(s) and email(s). Your report will be in the form of a static website, hosted for free on GitHub Pages. More specifically, you’ll use Jekyll, a framework built into GitHub Pages that allows you to create professional-looking websites just by writing Markdown (practicaldsc.org is built using Jekyll!). GitHub Pages does the “hard” part of converting your Markdown to HTML. If you’d like to follow the official GitHub Pages &amp; Jekyll tutorial, you’re welcome to, though we will provide you with a perhaps simpler set of instructions here. A very basic site with an embedded visualization can be found at rampure.org/dsc80-proj3-test1/; the source code for the site is here. Note that this example site doesn’t have the same headings that you’re required to have. Step 1: Initializing a Jekyll GitHub Pages Site . | Create a GitHub account, if you don’t already have one. | Create a new GitHub repository for your homework. Give it a descriptive name, like league-of-legends-analysis, not eecs398-portfolio-hw. | GitHub Pages sites live at &lt;username&gt;.github.io/&lt;reponame&gt; (for instance, the site for github.com/practicaldsc.org/fa24 is practicaldsc.github.io/fa24, which we redirect to from practicaldsc.org). | As such, don’t include “EECS 398” or “Portfolio Homework” in your repo’s name – this looks unprofessional to future employers, and gives you a generic-sounding URL. Instead, mention that this is a homework for EECS 398 at U-M in the repository description. | Make sure to make your repository public. | Select “ADD a README file.” This ensures that your repository starts off non-empty, which is necessary to continue. | . | Click “Settings” in the repository toolbar (next to “Insights”), then click “Pages” in the left menu. | Under “Branch”, click the “None” dropdown, change the branch to “main”, and then click “Save.” You should soon see “GitHub Pages source saved.” in a blue banner at the top of the page. This initiates GitHub Pages in your repository. | After 30 seconds, reload the page again. You should see “Your site is live at http://username.github.io/reponame/”. Click that link – you now have a site! | Click “Code” in the repo toolbar to return to the source code for your repo. | . Note that the source code for your site lives in README.md. As you push changes to README.md, they will update on your site automatically within a few minutes! Before moving forward, make sure that you can make basic edits: . | Clone your repo locally. | Make some edits to README.md. | Push those changes back to GitHub, using the following steps: . | Add your changes to “staging” using git add README.md (repeat this for any other files you add). | Commit your changes using git commit -m '&lt;some message here&gt;', e.g. git commit -m 'changed title of website'. | Push your changes using git push. | . | . Moving forward, we recommend making edits to your website source code locally, rather than directly on GitHub. This is in part due to the fact that you’ll be creating folders and adding files to your source code. Step 2: Choosing a Theme . The default “theme” of a Jekyll site is not all that interesting. To change the theme of your site: . | Create a file in your repository called _config.yml. | Go here, and click the links of various themes until you find one you like. | Open the linked repository for the theme you’d like to use and scroll down to the “Usage” section of the README. Copy the necessary information from the README to your _config.yml and push it to your site. | . For instance, if I wanted to use the Merlot theme, I’d put the following in my _config.yml: . remote_theme: pages-themes/merlot@v0.2.0 plugins: - jekyll-remote-theme # add this line to the plugins list if you already have one . Note that you’re free to use any Jekyll theme, not just the ones that appear here. You are required to choose some theme other than the default, though. See more details about themes here. Step 3: Embedding Content . Now comes the interesting part – actually including content in your site. The Markdown cheat sheet contains tips on how to format text and other page components in Markdown (and if you’d benefit by seeing an example, you could always look at the Markdown source of this very page – meta!). What will be a bit trickier is embedding plotly plots in your site so that they are interactive. Note that you are required to do this, you cannot simply take screenshots of plots from your notebooks and embed them in your site. Here’s how to embed a plotly plot directly in your site. | First, you’ll need to convert your plot to HTML. If fig is a plotly Figure object (for instance, the result of calling px.express, go.Figure, or .plot when pd.options.plotting.backend = \"plotly\" has been run), then the method fig.write_html saves the plot as HTML to a file. Call it using fig.write_html('file-name.html', include_plotlyjs='cdn'). | Change 'file-name.html' to the path where you’d like to initially save your plot. | include_plotlyjs='cdn' tells write_html to load the source code for the plotly library from a server, rather than including the entire source code in your HTML file. This drastically reduces the size of the resulting HTML file, keeping your repo size down. | . | Move the .html file(s) you’ve created into a folder in your website repo called assets (or something similar). | Depending on where your template notebook is saved, you could combine this step with the step above by calling fig.write_html with the correct path (e.g. `fig.write_html(‘../league-match-analysis/assets/matches-per-year.html’)). | . | In README.md, embed your plot using the following syntax: | . &lt;iframe src=\"assets/file-name.html\" width=\"800\" height=\"600\" frameborder=\"0\" &gt;&lt;/iframe&gt; . | iframe stands for “inline frame”; it allows us to embed HTML files within other HTML files. | You can change the width and height arguments, but don’t change the frameBorder argument. | . Refer here for a working example (and here for its source code). Try your best to make your plots look professional and unique to your group – add axis labels, change the font and colors, add annotations, etc. Remember, potential employers will see this – you don’t want your plots to look like everyone else’s! If you’d like to match the styles of the plotly plots used in lecture (e.g. with the white backgrounds), you can import and use the lec_utils.py file that’s in the homeworks/portfolio folder of our public repo, alongside template.ipynb. To convert a DataFrame in your notebook to Markdown source code (which you need to do for both the Data Cleaning and Interesting Aggregates sections of Step 2: Data Cleaning and Exploratory Data Analysis in Part 1), use the .to_markdown() method on the DataFrame. For instance, . print(counts[['semester', 'Count']].head().to_markdown(index=False)) . displays a string, containing the Markdown representation of the first 5 rows of counts, including only the 'semester' and 'Count' columns (and not including the index). You can see how this appears here; remember, no screenshots (and also remember that the “Assessment of Missingness” title is not something you need to have, that’s just an example website). You may need to play with this a little bit so that you don’t show DataFrames that are super, super wide and look ugly. Local Setup . The above instructions give you all you need to create and make updates to your site. However, you may want to set up Jekyll locally, so that you can look at how changes to your site would look without having to push and wait for GitHub to re-build your site. To do so, follow the steps here and then here. If, after running the above steps, running bundle exec jekyll serve in your local website repository doesn’t work, then follow these steps. | In your Terminal, cd to your local website repository (folder). | Run bundle init to create a Gemfile (a file that specifies which Ruby extensions your project needs). | Open the Gemfile created in your local repository, delete everything that’s currently there, and replace it all with: source \"https://rubygems.org\" gem \"github-pages\", group: :jekyll_plugins . | Run bundle install and then bundle exec jekyll serve. If, after that, you still can’t render your site locally, let us know what error bundle exec jekyll serve throws for you and we’ll try and troubleshoot! | . ",
    "url": "/portfolio/#part-2-report",
    
    "relUrl": "/portfolio/#part-2-report"
  },"9": {
    "doc": "Portfolio Homework 📊",
    "title": "Submission and Rubric",
    "content": "Overall, the homework is worth 100 points. We describe the breakdown in the Rubric section below. Checkpoint Submission . As mentioned at the top of this page, this homework has a required checkpoint, worth 15 of the 100 points. You can submit the checkpoint here on Gradescope. The checkpoint asks you to answer the following questions: . | (3 points) Which of the three datasets did you choose and why? Or did you get pre-approval to choose your own dataset, and why? | (6 points) Upload a screenshot of a plotly visualization you’ve created while completing Part 1, Step 2: Data Cleaning and Exploratory Data Analysis. | (6 points) What is the column you plan on trying to predict in Part 1, Steps 3-5? Is it a classification or regression problem? | . Final Submission . You will ultimately submit your homework in two ways: . | By uploading a PDF version of your notebook to the specific “Portfolio Homework Notebook PDF (Dataset)” assignment on Gradescope for your dataset. | To export your notebook as a PDF, first, restart your kernel and run all cells. Then, go to “File &gt; Print Preview”. Then, save a print preview of the webpage as a PDF. There are other ways to save a notebook as a PDF but they may require that you have additional packages installed on your computer, so this is likely the most straightforward. | It’s fine if your plotly graphs don’t render in the PDF output of your notebook. However, make sure none of the code is cut off in your notebook’s PDF. You will lose 5% of the points available on this homework if your code is cut off. | This notebook asks you to include a link to your website; make sure to do so. | . | By submitting a link to your website to the “Portfolio Homework Website Link (All Datasets)” assignment on Gradescope. | We will use the links provided on Gradescope to create a “showcase site” with links to everyone’s websites so that the rest of the class can see your work! | Here, you’ll also need to provide your group member name(s), email(s), and the title of your website. | . | . To both submissions, make sure to tag your partner. You don’t need to submit your actual .ipynb file anywhere. While your website must be public and you should share it with others, you should not make your code for this homework available publicly. Remember that you can’t use slip days on any part of this homework – not on the checkpoint, not on the final PDF submission, and not on the final website link submission. There are a lot of moving parts to this assignment – don’t wait until the last minute to try and submit! . Rubric . Your homework will be graded out of 100 points. The rough rubric is shown below. If you satisfy these requirements as described above, you will receive full credit. Note that the rubric is intentionally vague when it comes to Steps 3-5. This is because an exact rubric would specify exactly what you need to do to build a model, but figuring out what to do is a large part of Steps 3-5. | Component | Weight | . | Checkpoint | 15 points | . | Step 1: Introduction | 5 points | . | Step 2: Data Cleaning and Exploratory Data Analysis • Cleaned data (5 points)• Performed univariate analyses (5 points)• Performed bivariate analyses and aggregations (5 points)• Commented on imputation strategies (5 points) | 20 points | . | Step 3: Framing a Prediction Problem | 5 points | . | Step 4: Baseline Model | 20 points | . | Step 5: Final Model | 25 points | . | Overall: Included all necessary components on the website | 10 points | . | Total | 100 points | . ",
    "url": "/portfolio/#submission-and-rubric",
    
    "relUrl": "/portfolio/#submission-and-rubric"
  },"10": {
    "doc": "Portfolio Homework 📊",
    "title": "Partner Guidelines",
    "content": "Working with a partner? Keep the following points in mind: . | You are both required to actively contribute to all parts of the project. You must both be working on the assignment at the same time together, either physically or virtually on a Zoom call. You are encouraged to follow the pair programming model, in which you work on just a single computer and alternate who writes the code and who thinks about the problems at a high level. In particular, you cannot split up the project and each work on separate parts independently. So, you cannot say Partner 1 is responsible for the analysis and Partner 2 is responsible for the website. | You should decide on your partnership as soon as possible, but certainly before the checkpoint deadline of November 25th (which you’re required to submit on your own). | Ultimately, you will submit three deliverables for this homework to three separate Gradescope assignments – the checkpoint, a PDF of your notebook, and a link to your website. Make sure to have just one partner submit these deliverables, and have them tag the other partner! That is, don’t make duplicate submissions of the same work. | . ",
    "url": "/portfolio/#partner-guidelines",
    
    "relUrl": "/portfolio/#partner-guidelines"
  },"11": {
    "doc": "Portfolio Homework 📊",
    "title": "Portfolio Homework 📊",
    "content": " ",
    "url": "/portfolio/",
    
    "relUrl": "/portfolio/"
  },"12": {
    "doc": "🏡 Home",
    "title": "Practical Data Science 🛠️",
    "content": "EECS 398, Winter 2025 at the University of Michigan . Suraj Rampure he/him . rampure@umich.edu Lecture: MW 3-4:30PM, 1670 BBB . Welcome to Practical Data Science in Winter 2025! This site is currently under construction. Until this disclaimer is removed, all information here is subject to change. See you on January 8th! . ",
    "url": "/#practical-data-science-%EF%B8%8F",
    
    "relUrl": "/#practical-data-science-️"
  },"13": {
    "doc": "🏡 Home",
    "title": "🏡 Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"14": {
    "doc": "🫧 Polynomial Interpolation",
    "title": "🫧 Polynomial Interpolation",
    "content": " ",
    "url": "/interpolation/#-polynomial-interpolation",
    
    "relUrl": "/interpolation/#-polynomial-interpolation"
  },"15": {
    "doc": "🫧 Polynomial Interpolation",
    "title": "Table of contents",
    "content": ". | Definition | Point Representation | Interpolation | Lagrange Interpolation | . ",
    "url": "/interpolation/#table-of-contents",
    
    "relUrl": "/interpolation/#table-of-contents"
  },"16": {
    "doc": "🫧 Polynomial Interpolation",
    "title": "Definition",
    "content": "A polynomial is a function that consists solely of terms of the form \\(ax^k\\), where \\(k\\) is a non-negative integer, and \\(a\\) is a real number. As a function, it takes real numbers as an input and returns real numbers as outputs (\\(f \\colon \\mathbb{R} \\rightarrow \\mathbb{R}\\)). More formally, we define an \\(n\\)th degree polynomial in the following way: . $$\\boxed{p(x) = \\sum_{k = 0}^{n} a_kx^k = a_0 + a_1x + a_2x^2 + ... + a_nx^n}$$ We say a polynomial has degree \\(n\\) if \\(x^{n}\\) is the largest power of \\(x\\) that has a non-zero coefficent, i.e. $$\\textbf{deg}\\left( p(x) \\right) = \\max_{k} \\left(a_k \\neq 0\\right)$$ Here are examples of polynomials, along with their degrees: . | Polynomial | Degree | . | \\(-5 + x\\) | \\(1\\) | . | \\(3 - 4x + 13x^2\\) | \\(2\\) | . | \\(x^{4} + x^{13} - x^{14}\\) | \\(14\\) | . | \\(4\\) | \\(0\\) | . The examples above are all written in standard form – that is, the form defined in the box at the start of this page. We will look at different representations of polynomials shortly. ",
    "url": "/interpolation/#definition",
    
    "relUrl": "/interpolation/#definition"
  },"17": {
    "doc": "🫧 Polynomial Interpolation",
    "title": "Point Representation",
    "content": "The first key feature is that a polynomial of degree \\(n\\) is uniquely determined by a set of \\(n+1\\) points. This is a concept we’ve seen before, with regards to a line – in the \\(xy\\)-plane, there is exactly one line (i.e. a polynomial of degree 1) that passes through these points. It’s impossible to draw a different line that also passes through these two points. However, given just one point in the plane, there are an infinite number of lines that pass through it. This applies to polynomials of all degrees. Any three points uniquely determine a parabola (a polynomial of degree 2); however, given any two or fewer points, an infinite number of parabolas pass through them. Here, we see the only line that passes through the points \\((1, 4)\\) and \\((3, 10)\\): . However, there are infinitely many parabolas that pass through these two points. Three of them are shown below: . Consider the set of points \\(S = \\{(1, 3), (3, 19), (4, 33)\\}\\). These three points uniquely determine a degree 2 polynomial - you should verify on your own that this polynomial is \\(p(x) = 1 + 2x^2\\). Since \\(1 + 2x^2\\) is the only polynomial of degree 2 that passes through these points exactly, the set of points \\(S\\) is an equivalent way of representing \\(p(x)\\). We call this the point representation of polynomials. Note that the point representation of a polynomial is not unique, unlike the standard form representation, which is unique. For example, the set \\(T = \\{(-1, 3), (5, 51), (0, 1)\\}\\) also represents \\(p(x) = 1 + 2x^2\\). We can easily convert between standard form and the point representation - given some degree \\(n\\) polynomial, we can plug in \\(n+1\\) points into it and record the \\(n+1\\) pairs \\((x_i, y_i)\\) and call this our point representation. The question you may be asking, though, is how can we do the opposite – how can we find the standard form of a polynomial given the point representation, without repeated guessing and checking? How can we find that \\(p(x) = 1 + 2x^2\\) is the polynomial defined by \\(S\\) (or \\(T\\))? This process is called interpolation, and we will cover it now. ",
    "url": "/interpolation/#point-representation",
    
    "relUrl": "/interpolation/#point-representation"
  },"18": {
    "doc": "🫧 Polynomial Interpolation",
    "title": "Interpolation",
    "content": "The problem we’re trying to solve is, given a set of \\(n+1\\) points, \\((x_1, y_1), (x_2, y_2), ..., (x_{n+1}, y_{n+1})\\), what is the equation of the degree \\(n\\) polynomial that passes through all \\(n+1\\) points? . Note that it is possible that a set of \\(n + 1\\) points uniquely determine a polynomial that has a degree less than \\(n\\). For example, \\((1, 2)\\), \\((2, 4)\\), and \\((3, 6)\\) all lie on the polynomial \\(p(x) = 2x\\) – here, we were given 3 points, but we didn’t need to find a degree 2 polynomial in order to find a polynomial that passed through them all, since a degree 1 polynomial passed through them all. Fortunately, we won’t have to consider this edge case; the polynomial that the process we’re about to learn about works in general, as long as there are no duplicate \\(x_i\\) values in your dataset. You’ve seen the degree 1 case of interpolation before – and in fact, Question 4.2 of Homework 8 requires you to use it. Given two points \\((x_1, y_1)\\) and \\((x_2, y_2)\\), there are techniques from high school algebra that help us find the intercept and slope of the line that passes through \\((x_1, y_1)\\) and \\((x_2, y_2)\\). We need something more general that would work for cases with more than 2 points, though, and there’s no natural extension to this process. Suppose we’re given five points, \\((0, -1), (1, 0), (2, -11), (3, 2)\\) and \\((4, 99)\\). Since we know that we’re searching for a degree 4 polynomial, our desired polynomial will be of the form \\(p(x) = a + bx + cx^2 + dx^3 + ex^4\\). Substituting each of the five points into \\(p(x)\\) would give us a solvable system of 5 equations and 5 unknowns. These equations would be as follows: . $$\\begin{align*} a + b(0) + c(0)^2 + d(0)^3 + e(0)^4 &amp;= -1 \\\\\\ a + b(1) + c(1)^2 + d(1)^3 + e(1)^4 &amp;= 0 \\\\\\ a + b(2) + c(2)^2 + d(2)^3 + e(2)^4 &amp;= -11 \\\\\\ a + b(3) + c(3)^2 + d(3)^3 + e(3)^4 &amp;= 2 \\\\\\ a + b(4) + c(4)^2 + d(4)^3 + e(4)^4 &amp;= 99\\end{align*}$$ However, solving this system of equations and unknowns would take quite some time. Luckily, there exists a more intuitive way to find \\(p(x)\\). Since there is only one such degree 4 polynomial that passes through these five points, both methods should (and do) result in the same \\(p(x)\\). ",
    "url": "/interpolation/#interpolation",
    
    "relUrl": "/interpolation/#interpolation"
  },"19": {
    "doc": "🫧 Polynomial Interpolation",
    "title": "Lagrange Interpolation",
    "content": "Let’s start by creating five smaller polynomials, formally called Lagrange basis polynomials, that we can then sum to find \\(p(x)\\). For each provided point \\((x_i, y_i)\\), \\((x_1, y_1)\\) being the first point we were given, we will define a basis polynomial \\(p_{i}(x)\\) with the following properties: . \\[p_i(x_i) = 1\\] \\[p_i(x_j) = 0, \\forall \\: j \\neq i\\] (The \\(\\forall\\) symbol means “for all” – that is, for every value of \\(j\\) that is not equal to \\(i\\).) . In other words, basis polynomial \\(p_i(x)\\) should evaluate to 1 if \\(x_i\\) is passed in, and to 0 if any of the other \\(x_j\\)s are passed in (we will see why this structure is important very soon). Intuitively, \\(p_i(x)\\) “turns on” when \\(x_i\\) is passed in and returns 1, and “turns off” when some other \\(x_j \\neq x_i\\) is passed in and returns 0. (When a value other than one of the \\(x\\)s in the dataset is passed in, \\(p_i(x)\\) will return some other, unimportant number.) . We can create such a basis polynomial, for each \\(i\\), as follows: . \\[p_{i}(x) = \\frac{\\Pi_{j \\neq i} (x - x_j)}{\\Pi_{j \\neq i} (x_i - x_j)}\\] (\\(\\Pi\\) represents product notation, the same way \\(\\sum\\) represents summation notation.) . Remember, the five points we were given were \\((0, -1), (1, 0), (2, -11), (3, 2)\\) and \\((4, 99)\\). For clarity, let’s calculate \\(p_1(x)\\) and \\(p_3(x)\\) (corresponding to \\((0, -1)\\) and \\((2, -11)\\), respectively, as these were the first and third points given to us). $$ p_1(x) = \\frac{\\Pi_{j \\neq 1}(x - x_j)}{\\Pi_{j \\neq 1}(0 - x_j)} = \\frac{(x-1)(x-2)(x-3)(x-4)}{(0-1)(0-2)(0-3)(0-4)} = \\frac{1}{24}(x-1)(x-2)(x-3)(x-4) $$ $$ p_3(x) = \\frac{\\Pi_{j \\neq 3}(x - x_j)}{\\Pi_{j \\neq 3}(2 - x_j)} = \\frac{(x-0)(x-1)(x-3)(x-4)}{(2-0)(2-1)(2-3)(2-4)} = \\frac{1}{4}(x)(x-1)(x-3)(x-4) $$ The second-to-last step of the above expansions best illustrate why we’ve chosen to craft our basis polynomials in this way. If we were to evaluate \\(p_1(0)\\), the numerator and denominator would be exactly the same, so \\(p_1(0) = 1\\). If we were to instead evaluate \\(p_1(1), p_1(2), p_1(3)\\) or \\(p_1(4)\\), since \\(x-1, x-2, x-3, x-4\\) are all factors of the numerator, the result would be 0. Similarly, \\(p_3(x_3) = p_3(2) = 1\\), but \\(p_3(x_j)\\) for any other \\(x_j\\) – that is, \\(p_3(0)\\), \\(p_3(1)\\), \\(p_3(3)\\), or \\(p_3(4)\\) – is 0. We’re almost done. We can now say that our final polynomial \\(p(x)\\) is constructed as follows: . $$ p(x) = \\sum_{i = 1}^{n+1} y_i p_i(x) $$ . This is where the \\(y\\) values of each of the given points come into play. Looking at our example dataset of \\((0, -1), (1, 0), (2, -11), (3, 2)\\) and \\((4, 99)\\) more closely, we have: . $$ p(x) = -p_1(x) + 0p_2(x) -11p_3(x) + 2p_4(x) + 99p_5(x) $$ . From the way each \\(p_i(x)\\) was constructed: . \\[\\begin{align*} p(0) &amp;= \\boxed{(-1) \\cdot 1} + 0 \\cdot 0 + (-11) \\cdot 0 + 2 \\cdot 0 + 99 \\cdot 0 = -1 \\\\\\ p(1) &amp;= (-1) \\cdot 0 + \\boxed{0 \\cdot 1} + (-11) \\cdot 0 + 2 \\cdot 0 + 99 \\cdot 0 = 0 \\\\\\ p(2) &amp;= (-1) \\cdot 0 + 0 \\cdot 0 + \\boxed{(-11) \\cdot 1} + 2 \\cdot 0 + 99 \\cdot 0 = -11 \\\\\\ p(3) &amp;= (-1) \\cdot 0 + 0 \\cdot 0 + (-11) \\cdot 0 + \\boxed{2 \\cdot 1} + 99 \\cdot 0 = 2 \\\\\\ p(4) &amp;= (-1) \\cdot 0 + 0 \\cdot 0 + (-11) \\cdot 0 + 2 \\cdot 0 + \\boxed{99 \\cdot 1} = 99 \\end{align*}\\] Since \\(p(x)\\) passes through all 5 of our points, and theory tells that \\(p(x)\\) is unique, we’ve found the polynomial we’re looking for! All that’s left is that we need to expand the right-hand side of the equation \\(p(x) = -p_1(x) + 0p_2(x) -11p_3(x) + 2p_4(x) + 99p_5(x)\\). Yes, this is slightly annoying to work out by hand, but it finds us \\(p(x) = -1 + 13x - 13x^2 + x^4\\). This entire process is known as Lagrange Interpolation. It is named after Lagrange, a famous French mathematician. Lagrange Interpolation is still tedious, though not nearly as tedious as the initial approach from the start of this section. You should practice this process on your own; create your own polynomial of degree \\(n \\leq 5\\), pick \\(n+1\\) points on it, and see if you can correctly reconstruct your polynomial. It should be noted though, that mastering the arithmetic, while important, isn’t the main goal of learning this material. This material is presented so that you can add it to your mathematical toolbox. ",
    "url": "/interpolation/#lagrange-interpolation",
    
    "relUrl": "/interpolation/#lagrange-interpolation"
  },"20": {
    "doc": "🫧 Polynomial Interpolation",
    "title": "🫧 Polynomial Interpolation",
    "content": " ",
    "url": "/interpolation/",
    
    "relUrl": "/interpolation/"
  },"21": {
    "doc": "League of Legends ⌨️",
    "title": "League of Legends ⌨️",
    "content": " ",
    "url": "/portfolio/league-of-legends/#league-of-legends-%EF%B8%8F",
    
    "relUrl": "/portfolio/league-of-legends/#league-of-legends-️"
  },"22": {
    "doc": "League of Legends ⌨️",
    "title": "Table of Contents",
    "content": ". | Getting the Data | Example Questions and Prediction Problems | Make sure to justify what information you would know at the “time of prediction” and to only train your model using those features. | Special Considerations . | Step 2: Data Cleaning and Exploratory Data Analysis | . | . Welcome to Summoner’s Rift! This dataset contains information of players and teams from over 10,000 League of Legends competitive matches. You’ll probably want to be at least a little bit familiar with League of Legends and its terminology to use this dataset. If not, one of the other datasets may be more interesting to you. ",
    "url": "/portfolio/league-of-legends/#table-of-contents",
    
    "relUrl": "/portfolio/league-of-legends/#table-of-contents"
  },"23": {
    "doc": "League of Legends ⌨️",
    "title": "Getting the Data",
    "content": "The data can be found on the website Oracle’s Elixir at the provided Google Drive link. We’ve verified that it’s possible to satisfy the requirements of the homework using match data from 2022. You’re welcome to use newer or older datasets if you wish, but keep in mind that League of Legends changes significantly between years; this can make it difficult to combine or make comparisons between datasets from different years. ",
    "url": "/portfolio/league-of-legends/#getting-the-data",
    
    "relUrl": "/portfolio/league-of-legends/#getting-the-data"
  },"24": {
    "doc": "League of Legends ⌨️",
    "title": "Example Questions and Prediction Problems",
    "content": "Feel free to base your exploration into the dataset in Steps 1-2 around one of these questions, or come up with a question of your own. | Looking at tier-one professional leagues, which league has the most “action-packed” games? Is the amount of “action” in this league significantly different than in other leagues? Note that you’ll have to come up with a way of quantifying “action”. | Which competitive region has the highest win rate against teams outside their region? Note you will have to find and merge region data for this question as the dataset does not have it. | Which role “carries” (does the best) in their team more often: ADCs (Bot lanes) or Mid laners? | Is Talon (a former TA’s favorite champion) more likely to win or lose any given match? | . Feel free to use one of the prompts below to build your predictive model in Steps 3-5, or come up with a prediction task of your own. | Predict if a team will win or lose a game. | Predict which role (top-lane, jungle, support, etc.) a player played given their post-game data. | Predict how long a game will take before it happens. | Predict which team will get the first Baron. | . ",
    "url": "/portfolio/league-of-legends/#example-questions-and-prediction-problems",
    
    "relUrl": "/portfolio/league-of-legends/#example-questions-and-prediction-problems"
  },"25": {
    "doc": "League of Legends ⌨️",
    "title": "Make sure to justify what information you would know at the “time of prediction” and to only train your model using those features.",
    "content": " ",
    "url": "/portfolio/league-of-legends/#make-sure-to-justify-what-information-you-would-know-at-the-time-of-prediction-and-to-only-train-your-model-using-those-features",
    
    "relUrl": "/portfolio/league-of-legends/#make-sure-to-justify-what-information-you-would-know-at-the-time-of-prediction-and-to-only-train-your-model-using-those-features"
  },"26": {
    "doc": "League of Legends ⌨️",
    "title": "Special Considerations",
    "content": "Step 2: Data Cleaning and Exploratory Data Analysis . | Each 'gameid' corresponds to up to 12 rows – one for each of the 5 players on both teams and 2 containing summary data for the two teams (try to find out what distinguishes those rows). After selecting your line of inquiry, make sure to remove either the player rows or the team rows so as not to have issues later in your analysis. | Many columns should be of type bool but are not. | . ",
    "url": "/portfolio/league-of-legends/#special-considerations",
    
    "relUrl": "/portfolio/league-of-legends/#special-considerations"
  },"27": {
    "doc": "League of Legends ⌨️",
    "title": "League of Legends ⌨️",
    "content": "go back to the Portfolio Homework spec . ",
    "url": "/portfolio/league-of-legends/",
    
    "relUrl": "/portfolio/league-of-legends/"
  },"28": {
    "doc": "🧮 Linear Algebra Review",
    "title": "Linear Algebra Review",
    "content": "Welcome to Practical Data Science in Winter 2025! This site is currently under construction. Until this disclaimer is removed, all information here is subject to change. See you on January 8th! . ",
    "url": "/lin-alg/#linear-algebra-review",
    
    "relUrl": "/lin-alg/#linear-algebra-review"
  },"29": {
    "doc": "🧮 Linear Algebra Review",
    "title": "🧮 Linear Algebra Review",
    "content": " ",
    "url": "/lin-alg/",
    
    "relUrl": "/lin-alg/"
  },"30": {
    "doc": "♾️ Maximum Likelihood Estimation",
    "title": "♾️ Maximum Likelihood Estimation",
    "content": " ",
    "url": "/mle/#%EF%B8%8F-maximum-likelihood-estimation",
    
    "relUrl": "/mle/#️-maximum-likelihood-estimation"
  },"31": {
    "doc": "♾️ Maximum Likelihood Estimation",
    "title": "Table of contents",
    "content": ". | Overview | Problem Setup | The Likelihood Function | Maximizing Likelihood | Maximizing Log Likelihood | Summary | . ",
    "url": "/mle/#table-of-contents",
    
    "relUrl": "/mle/#table-of-contents"
  },"32": {
    "doc": "♾️ Maximum Likelihood Estimation",
    "title": "Overview",
    "content": "In Lecture 14, we discussed the relationship between probability and statistics: . | In probability questions, we’re given some model of how the universe works, and it’s our job to determine how various samples could turn out.Example: If we have 5 blue marbles and 3 green marbles and pick 2 at random, what are the chances we see one marble of each? | In statistics questions, we’re given information about a sample, and it’s our job to figure out how the universe – or data generating process works.Example: Repeatedly, I picked 2 marbles at random from a bag with replacement. I don’t know what’s inside the bag. One time, I saw 2 blue marbles, then next time I saw 1 of each, the next time I saw 2 red marbles, and so on. What marbles are inside the bag? | . In this note, we’ll gain a deeper understanding of this relationship, through the lens of your probability knowledge from EECS 203. After reading this note, you’ll be well-equipped to tackle Question 4 on Homework 7 (and also have better context for machine learning, more generally). ",
    "url": "/mle/#overview",
    
    "relUrl": "/mle/#overview"
  },"33": {
    "doc": "♾️ Maximum Likelihood Estimation",
    "title": "Problem Setup",
    "content": "Let’s work with the example mentioned in Lecture 14, Slide 13. Suppose we find a coin on the ground, and we’re unsure of whether the coin is fair. We decide to flip the coin repeatedly to estimate its bias, \\(\\theta\\), which is the probability of flipping heads on any particular flip. (The probability of flipping tails on any particular flip, then, is \\(1 - \\theta\\)). Suppose we flip the coin 100 times and see 65 heads. Assuming that each flip is independent, this is a possible result, no matter what the value of \\(\\theta\\) is, as long as \\(0 &lt; \\theta &lt; 1\\). But, some values of \\(\\theta\\) are more believable than others: . | For example, if \\(\\theta = 0.5\\), the chances of seeing 65 heads and 35 tails is: | . \\[\\mathbb{P}(65 \\text{ heads} \\: | \\: \\theta = 0.5) = {\\binom{100}{65}} 0.5^{65} 0.5^{35} \\approx 0.00086\\] . | If \\(\\theta = 0.7\\), the chances of seeing 65 heads and 35 tails is: | . \\[\\mathbb{P}(65 \\text{ heads} \\: | \\: \\theta = 0.7) = {\\binom{100}{65}} 0.7^{65} 0.3^{35} \\approx 0.04678\\] Again, the true bias, \\(\\theta\\), could be anything, and we don’t truly know what it is, since we just found this coin on the ground. But, as we see above, some values of \\(\\theta\\) are more likely than others – for instance, it seems that \\(\\theta = 0.7\\) is more likely than \\(\\theta = 0.5\\), because the probability of our observation is higher if we assume \\(\\theta = 0.7\\) than if we assume \\(\\theta = 0.5\\). \\(\\theta = 0.5\\) and \\(\\theta = 0.7\\) were arbitrarily chosen values of \\(\\theta\\), just for illustration. The question is, what is the most likely value of \\(\\theta\\), among all possible \\(\\theta\\)’s? . ",
    "url": "/mle/#problem-setup",
    
    "relUrl": "/mle/#problem-setup"
  },"34": {
    "doc": "♾️ Maximum Likelihood Estimation",
    "title": "The Likelihood Function",
    "content": "To answer this question, we’ll define what’s known as the likelihood function of \\(\\theta\\), denoted \\(L(\\theta)\\): . \\[L(\\theta) = \\mathbb{P}(65 \\text{ heads} \\: | \\: \\theta) = {\\binom{100}{65}} \\theta^{65} (1-\\theta)^{35}\\] We’ve used the binomial distribution, which you saw in EECS 203, to calculate the probability of seeing 65 heads and 35 tails, given a bias of \\(\\theta\\). The function \\(L(\\theta)\\) is given the special name of “likelihood” because it helps us measure how likely a particular value of \\(\\theta\\) is. It emphasizes that \\(\\theta\\) is unknown, whereas in most classical probability examples you’ve dealt with, \\(\\theta\\) was known, but the number of heads (for example) was unknown. \\(\\theta\\) is referred to as a parameter of the binomial distribution, and our goal is to estimate \\(\\theta\\) as best as we can, given our data. The word parameter here means the same as it did in Lecture 14 – a parameter defines the relationship between the inputs and outputs of a model, and we’re using the data we’re given to find optimal parameters. Here, the model is a binomial one, which takes in a number of heads and outputs the probability of seeing that many heads. Let’s look at a plot of \\(L(\\theta)\\) for various values of \\(\\theta\\). You should notice that \\(L(\\theta)\\) peaks at 0.65, which is the empirical proportion of heads – remember that we saw 65 heads in 100 flips of this coin! . ",
    "url": "/mle/#the-likelihood-function",
    
    "relUrl": "/mle/#the-likelihood-function"
  },"35": {
    "doc": "♾️ Maximum Likelihood Estimation",
    "title": "Maximizing Likelihood",
    "content": "Let’s see if we can prove that this is always the case – that is, let’s prove that the most likely bias of a coin, \\(\\theta\\), when we flip it many times, is \\(\\frac{\\text{number of heads}}{\\text{total number of flips}}\\). First, let’s pose the problem more generally. If we have a coin that flips heads with probability \\(\\theta\\), the probability of seeing \\(k\\) heads in \\(n\\) independent flips of the coin is: . \\[L(\\theta) = \\mathbb{P}(k \\text{ heads} \\: | \\: \\theta) = {n \\choose k} \\theta^k (1-\\theta)^{n-k}\\] The question at hand is, which value of \\(\\theta\\) maximizes \\(L(\\theta)\\)? This resembles a question we dealt with in Lecture 14 – which value of \\(h\\) minimizes \\(R_\\text{sq}(h)\\)? – the only difference being that there, we minimized, and here, we’re maximizing. \\(L(\\theta)\\) is a function of a single variable. To find the value of \\(\\theta\\) that maximizes it, we can follow the same process from Lecture 14, where we take its derivative with respect to \\(\\theta\\), and solve for the value of \\(\\theta\\) that makes the derivative 0. Let’s do it. To find the derivative of \\(L(\\theta)\\) with respect to \\(\\theta\\), we’ll need to use the power, product, and chain rules from calculus. Here we go! . \\[\\begin{align*} L(\\theta) &amp;= { n \\choose k } \\theta^k (1-\\theta)^{n-k} \\\\ \\frac{d}{d\\theta}L(\\theta) &amp;= { n \\choose k } \\big( k\\theta^{k-1} (1-\\theta)^{n-k} + \\theta^k (n-k)(1-\\theta)^{n-k-1}(-1) \\big) \\end{align*}\\] Now, we’ll set this to 0 and solve for the \\(\\theta\\) that makes this happen. We’re going to call the resulting value \\(\\theta^*\\), to emphasize that it’s the “best” \\(\\theta\\) in some sense. \\[\\begin{align*} { n \\choose k }\\cdot \\left( k\\theta^{k-1} (1-\\theta)^{n-k} + \\theta^k (n-k)(1-\\theta)^{n-k-1}(-1) \\right) &amp;= 0 \\\\ k\\theta^{k-1} (1-\\theta)^{n-k} - \\theta^k (n-k)(1-\\theta)^{n-k-1} &amp;= 0 \\\\ k\\theta^{k-1} (1-\\theta)^{n-k} &amp;= \\theta^k (n-k)(1-\\theta)^{n-k-1} \\\\ k (1-\\theta) &amp;= \\theta (n-k) \\\\ k - \\theta k &amp;= \\theta n - \\theta k \\\\ k &amp;= \\theta n \\\\ \\theta^* &amp;= \\boxed{\\frac{k}{n}} \\end{align*}\\] Since \\(\\theta^* = \\frac{k}{n}\\) is the input to \\(L(\\theta)\\) that maximizes the likelihood function, \\(L(\\theta)\\), we call \\(\\theta^*\\) the maximum likelihood estimate of \\(\\theta\\). In our example, \\(k = 65\\) and \\(n = 100\\), which means the maximum likelihood estimate of the bias of the coin we found on the ground is \\(\\frac{65}{100} = 0.65\\)! This matches what we saw in the graph earlier, and also shouldn’t be surprising. In our 100 flips of this random coin off the ground, we saw 65 heads and 35 tails, and so while the bias of the coin could be anything, the most likely bias of the coin is \\(\\theta = 0.65\\). We’ve now walked through a full example of the method of maximum likelihood estimation. ",
    "url": "/mle/#maximizing-likelihood",
    
    "relUrl": "/mle/#maximizing-likelihood"
  },"36": {
    "doc": "♾️ Maximum Likelihood Estimation",
    "title": "Maximizing Log Likelihood",
    "content": "The process of solving for \\(\\theta^*\\) was messy, because taking the derivative of \\(L(p)\\) was messy, since it involved using the power, product, and chain rules! . It turns out that there’s a simplification we can use here to make our lives easier, that is often used in machine learning and statistics. Fact: If \\(x^*\\) maximizes \\(f(x)\\), then \\(x^*\\) also maximizes \\(\\log f(x)\\).Related, if \\(x^*\\) minimizes \\(f(x)\\), then \\(x^*\\) also minimizes \\(\\log f(x)\\). To see why this is true, let’s plot a graph of \\(\\log L(\\theta)\\) vs. \\(\\theta\\): . While the graph of \\(\\log L(\\theta)\\) looks very different than the graph of \\(L(\\theta)\\), they are both maximized at the same position, \\(\\theta^* = 0.65\\). The reason for this is that \\(\\log\\) is monotonically increasing, meaning that if \\(a &gt; b\\), then \\(\\log(a) &gt; \\log(b)\\). The consequence of this is that if \\(L(0.65)\\) is bigger than \\(L(\\theta)\\) for any other \\(\\theta\\), then \\(\\log L(0.65)\\) is bigger than \\(\\log L(\\theta)\\) for any other \\(\\theta\\), too. You may be wondering, and rightfully so: . Suraj, why did you randomly bring out the \\(\\log\\) function – isn’t this explanation already long and mathematical enough? . It turns out that maximizing \\(\\log L(\\theta)\\) is way easier than maximizing \\(L(\\theta)\\)! Here, let’s work through it. First, let’s simplify \\(\\log L(\\theta)\\). Note that we could use any base on our logarithm, but the calculations are simplest if we use the natural logarithm (which we’ll just denote with \\(\\log\\)). \\[\\begin{align*} L(\\theta) &amp;= {n \\choose k} \\theta^k (1-\\theta)^{n-k} \\\\ \\log L(\\theta) &amp;= \\log \\left( {n \\choose k} \\theta^k (1-\\theta)^{n-k} \\right) \\\\ &amp;= \\log {n \\choose k} + \\log \\left(\\theta^k \\right) + \\log \\left((1 - \\theta)^{n-k}\\right) \\\\ &amp;= \\log {n \\choose k} + k \\log \\theta + (n - k) \\log (1 - \\theta) \\end{align*}\\] Now, let’s take the derivative of \\(\\log L(\\theta)\\) and set it to 0: . \\[\\begin{align*} \\log L(\\theta) &amp;= \\log {n \\choose k} + k \\log \\theta + (n - k) \\log (1 - \\theta) \\\\ \\frac{d}{d\\theta} \\log L(\\theta) &amp;= 0 + k \\cdot \\frac{1}{\\theta} + (n - k) \\cdot\\frac{1}{1 - \\theta}(-1) = \\frac{k}{\\theta} - \\frac{n - k}{1 - \\theta} \\\\ 0 &amp;= \\frac{k}{\\theta} - \\frac{n - k}{1 - \\theta} \\\\ \\frac{k}{\\theta} &amp;= \\frac{n - k}{1 - \\theta} \\\\ k - \\theta k &amp;= \\theta n - \\theta k \\\\ k &amp;= \\theta n \\\\ \\theta^* &amp;= \\boxed{\\frac{k}{n}} \\end{align*}\\] By taking the log of \\(L(\\theta)\\), we were able to find the maximum likelihood estimate, \\(\\theta^*\\), without needing to find the derivative of \\(L(\\theta)\\), which involved working with expressions like \\(k\\theta^{k-1} (1-\\theta)^{n-k} + \\theta^k (n-k)(1-\\theta)^{n-k-1}(-1)\\). The benefit of the log function is that it turns products into sums – that is, \\(\\log(a \\cdot b) = \\log(a) + \\log(b)\\) – which allows us to bypass using the messy product rule. ",
    "url": "/mle/#maximizing-log-likelihood",
    
    "relUrl": "/mle/#maximizing-log-likelihood"
  },"37": {
    "doc": "♾️ Maximum Likelihood Estimation",
    "title": "Summary",
    "content": ". | We found a coin on the ground, which flips heads with an unknown probability, \\(\\theta\\). | We flipped the coin 100 times and saw 65 heads. | To estimate the bias of the coin, we decided to use maximum likelihood estimation, which requires us to define the likelihood function for \\(\\theta\\): . \\[L(\\theta) = \\mathbb{P}(k \\text{ heads} \\: | \\: \\theta) = {n \\choose k} \\theta^k (1-\\theta)^{n-k}\\] in the example stated above, \\(k = 65\\) and \\(n = 100\\). | The most likely value of \\(\\theta\\) is the one that maximizes \\(L(\\theta)\\). | To make the math simpler, instead of maximizing \\(L(\\theta)\\) directly, we maximized \\(\\log L(\\theta)\\). This found us the same maximum likelihood estimate, \\(\\theta^* = \\frac{k}{n}\\). | . ",
    "url": "/mle/#summary",
    
    "relUrl": "/mle/#summary"
  },"38": {
    "doc": "♾️ Maximum Likelihood Estimation",
    "title": "♾️ Maximum Likelihood Estimation",
    "content": " ",
    "url": "/mle/",
    
    "relUrl": "/mle/"
  },"39": {
    "doc": "Winter 2025 Info: Practical Data Science 🛠️",
    "title": "Winter 2025 Info: Practical Data Science 🛠️",
    "content": "EECS 398-002/003, Winter 2025 at the University of Michigan . 4 credits • Open to all majors • ULCS for Computer Science majors, Advanced Technical Elective or Application Elective for Data Science majors, Flexible Technical Elective for Electrical Engineering majors . Welcome! 👋 If you’re reading this, you’re likely interested in enrolling in Practical Data Science in Winter 2025. This page will give an overview of the course and provide logistical information you’ll need before you enroll.But, the best way to see what the course is about is to browse the resources (slides, code, recordings, homeworks, exams, etc.) on our Fall 2024 course homepage!If you have any questions at all, or want to set up a meeting to discuss whether the course makes sense for you, reach out to Suraj at rampure@umich.edu. As a third year data science major, this has been my favorite course I’ve taken throughout my time in college. It’s the first course that’s felt tailored to my major and relevant to the industry. I’ve loved the combination of learning core Python skills and ML techniques, and have gained a breadth of understanding in just a semester. – current student . Read what other students have said about the course and its workload in the Testimonials and Workload section below! . | Content | Prerequisites | Enrollment Logistics | Testimonials | Frequently Asked Questions | . Content . Skills and tools for building practical data science projects, along with their theoretical underpinnings. pandas, numpy, scikit-learn, BeautifulSoup, and Jupyter Notebooks, and also the math behind loss functions, gradient descent, linear and logistic regression, and other key ideas in machine learning. This course will train students to use industry-standard tools to solve real-world problems, while giving them an understanding of how these tools work under the hood. After taking this course, students will be prepared to build data science portfolios, participate in research across campus, succeed in data science internships, and reason about abstract mathematical problems about the foundations of machine learning models. The course will roughly be split into two halves. Data Wrangling Python and Jupyter Notebooks numpy arrays Tabular Data Manipulation in pandas Exploratory Data Analysis and Data Visualization Web Scraping and APIs SQL Regular Expressions and Text Processing Applied Machine Learning Linear Regression through Linear Algebra Feature Engineering in scikit-learn Regularization and Cross-Validation Gradient Descent Logistic Regression Decision Trees and Random Forests Unsupervised Learning Students will be expected to complete weekly homework assignments, which will mostly comprise of programming assignments in Python and Jupyter Notebooks, with theoretical questions sprinkled throughout (and the occasional purely theoretical homework). The course will have in-person, on-paper midterm and final exams that involve a mix of multiple choice, short answer, fill in the blank code, math, and English problems (for context, see the Fall 2024 Midterm Exam and solutions here). The exams are scheduled for the following times: . | Midterm Exam: Tuesday, February 25th, 7-9PM (tentative and subject to change) | Final Exam: Monday, April 28th, 10:30AM-12:30PM | . Other materials from the Fall 2024 offering can be found linked from the course homepage. Prerequisites . The course is open to students from all majors. The enforced prerequisites are discrete math (EECS 203), programming (EECS 280), calculus I, calculus II, and linear algebra. A probability and statistics course is an advisory prerequisite. Options include DATASCI 101, STATS 206, STATS 250, STATS 280, STATS 412, IOE 265, or ECON 451. If you’re interested in the class but don’t meet one of the prerequisites, email me and we can chat about your background and a potential override. I encourage students of all backgrounds who are curious about data science to reach out! . All students, especially those who haven’t taken a linear algebra course and receive an override, should work through LARDS: Linear Algebra Review for Data Science before the semester starts. Enrollment Logistics . If you’d like to enroll in the course, sign up for these EECS 398 sections: . | lecture LEC 002 or LEC 003, and | discussion DIS 021, 022, 023, or 024. | . There is only one live, in-person lecture, Monday and Wednesday 3-4:30PM in 1670 BBB (as is listed for LEC 002), and lecture attendance is not required. | All students, regardless of section, will have access to lecture recordings. | All students, regardless of enrolled section, can also attend the in-person lectures, space permitting on the day of the attendance. Usually, enough students consume material remotely that we don’t anticipate in-person space to be a bottleneck for anyone wishing to attend the class in-person. | . In-person discussion attendance will be required or incentivized: make sure you can attend your enrolled discussion section. Office hours will largely be in-person, but there will be some remote options as well. Testimonials . The following quotes are by students who are currently in the course. They were submitted anonymously and have not been edited. I couldn’t recommend this class enough! I’m a second year CS-LSA student and I’m taking this class as my first ULCS class with EECS 281. Despite not having the strongest foundation in linear algebra coming in, I’ve found the material very accessible with a bit of extra effort. The lectures, led by Suraj, are both engaging and informative; he has a talent in presenting complex ideas in a clear and interesting way that keeps you interested. The homework assignments are particularly cool—they push you to think creatively while applying what you’ve learned in class. The course is manageable with consistent effort, and I’ve found the resources provided by the staff to be helpful for filling in any gaps. If you’re interested in diving deeper into data science and machine learning concepts, I’d definitely recommend taking this course. As a third year data science major, this has been my favorite course I’ve taken throughout my time in college. It’s the first course that’s felt tailored to my major and relevant to the industry. I’ve loved the combination of learning core Python skills and ML techniques, and have gained a breadth of understanding in just a semester. While the homework can be time-consuming and challenging, it’s so worth it to gain a better understanding of class content. Not only that, but the concepts taught have helped me excel while interviewing for data science roles, and even land multiple offers. Also, the course staff are amazing – they’re incredibly intelligent and friendly, I’ve always had a great experience in office hours. Would highly recommend this course to anyone interested in data science! . This is one of my favorite classes I’ve taken at umich. I have enjoyed the content of almost every lecture, and I wish I had taken this class before my past internship because it would’ve helped a ton. I really appreciate the structure of this course, with weekly homeworks and a good amount of late days, but not enough to fall behind. I actually don’t have a single complaint about how this class is run. The professor gives out a grade report after the midterm, so we can see how many points we have in the class so far, what our current grade is, and how many late days we have remaining. I took linear algebra at a community college and learned pretty much nothing, but I still felt as though I could keep up because of the provided linear algebra review. I would recommend this course to anyone! . As a CS senior, this is one of the best courses I’ve taken at Michigan! The course is incredibly well organized: each week’s homework and discussion worksheet aligns well with the content covered in lecture, staff is very open to feedback, and the workload is very reasonable with consistent effort and doable alongside other difficult courses. Suraj is also such an engaging lecturer, and his notes/presentations are a really great mix of content, interactive demos, and practice problems. I would definitely recommend this course! . Coming into this class, I hadn’t had too much experience with Python and Data Science in general, so I was a bit unsure of how it would go. However, the result was completely unexpected: I now find myself really enjoying the course content not only because it is taught in an engaging way but also because I can see the value in the practical applications that come out of these concepts. Every week, even if I was unsure about a certain topic, I knew I could either come to office hours and get the help I needed, or reinforce those concepts through the homework problems, which really helped me build confidence in my understanding of the material. Most importantly, the course can really help you realize an interest for this content that you may have never known to have, as it did with me: I now am pursuing future courses like Machine Learning because the content in this class was taught so well and gave me a good foundation for future ULCS classes. Overall, the course is meant to challenge you, but it is quite rewarding in the end, and it is an enjoyable experience! . I think this course should be required for any DS (or frankly even CS major). There’s a lot to learn in the class. Although we spend a rather brief amount of time covering some of the topics, this class gives a great idea of lot’s of things out there. It may be called practical data science, but the second half goes more into depth for a decent mathematical understanding of basic, but crucial, ML components. There’s lot’s of skills you pick up through it (SQL, Pandas, Latex, Numpy, SciKit Learn, Plotly, Regex, etc.). I also believe that the course is what you make of it. Suraj, and the staff, put in a lot of time to gather resources to you can dwell deeper into areas of interest. My main cons of the course are that it’s not were related to depth, regarding some APIs such as Pandas. If you’re more into lower level programming (computer architecture, systems, DSA in C/C++), then this may not be the course for you. Although if you want a chill class which teaches you a thing or two, I’d say check it out. Bare through the initial few weeks and the content get’s pretty sweet! Believe me, I went through it myself. Make sure you know your linear algebra if you really want to appreciate the content in the second half of the course! . I think the ideal time to take the course is right after 203 and 280, probably alongside 281. Not only are you locking yourself into some internship experience (especially in today’s ML heavy workforce) but also get to branch into some other areas. Frequently Asked Questions . If I plan to take, or have already taken, a dedicated machine learning course (such as EECS 445), should I still take this course? . Yes! The first half of this course introduces students to several tools and skills that aren’t typically covered in other machine learning courses, like using more sophisticated features in pandas, scraping data from the internet, finding patterns in text data, etc. While the second half of the class does overlap a bit with more traditional machine learning courses, this course covers the content from a more introductory and practical perspective. Students who have already seen machine learning will reinforce their understanding of the relevant concepts through hands-on, real-world examples (e.g. hyperparameter tuning in sklearn). Students who haven’t already seen machine learning will develop an intuition for how various machine learning algorithms work from the ground up, both practically and theoretically, giving them a strong foundation upon which further machine learning courses can build off of. What specific topics from linear algebra will the course use? . In addition to matrix-vector multiplication, we will expect students to be familiar with the ideas of linear independence, spans, projections, and orthogonality. We will review these ideas when necessary, but it will help to have seen them already. ",
    "url": "/next/#winter-2025-info-practical-data-science-%EF%B8%8F",
    
    "relUrl": "/next/#winter-2025-info-practical-data-science-️"
  },"40": {
    "doc": "Winter 2025 Info: Practical Data Science 🛠️",
    "title": "Winter 2025 Info: Practical Data Science 🛠️",
    "content": " ",
    "url": "/next/",
    
    "relUrl": "/next/"
  },"41": {
    "doc": "Power Outages 🔋",
    "title": "Power Outages 🔋",
    "content": " ",
    "url": "/portfolio/power-outages/#power-outages-",
    
    "relUrl": "/portfolio/power-outages/#power-outages-"
  },"42": {
    "doc": "Power Outages 🔋",
    "title": "Table of Contents",
    "content": ". | Getting the Data | Example Questions and Prediction Problems | Special Considerations . | Step 2: Data Cleaning and Exploratory Data Analysis | . | . This dataset has major power outage data in the continental U.S. from January 2000 to July 2016. ",
    "url": "/portfolio/power-outages/#table-of-contents",
    
    "relUrl": "/portfolio/power-outages/#table-of-contents"
  },"43": {
    "doc": "Power Outages 🔋",
    "title": "Getting the Data",
    "content": "The data is downloadable here. Note: If you are having a hard time with the “This dataset” link, hold shift and click the link to open it into a new tab and then refresh that new tab. A data dictionary is available at this article under Table 1. Variable descriptions. ",
    "url": "/portfolio/power-outages/#getting-the-data",
    
    "relUrl": "/portfolio/power-outages/#getting-the-data"
  },"44": {
    "doc": "Power Outages 🔋",
    "title": "Example Questions and Prediction Problems",
    "content": "Feel free to base your exploration into the dataset in Steps 1-2 around one of these questions, or come up with a question of your own. | Where and when do major power outages tend to occur? | What are the characteristics of major power outages with higher severity? Variables to consider include location, time, climate, land-use characteristics, electricity consumption patterns, economic characteristics, etc. What risk factors may an energy company want to look into when predicting the location and severity of its next major power outage? | What characteristics are associated with each category of cause? | How have characteristics of major power outages changed over time? Is there a clear trend? | . Feel free to use one of the prompts below to build your predictive model in Steps 3-5, or come up with a prediction task of your own. | Predict the severity (in terms of number of customers, duration, or demand loss) of a major power outage. | Predict the cause of a major power outage. | Predict the number and/or severity of major power outages in the year 2022. | Predict the electricity consumption of an area. | . Make sure to justify what information you would know at the “time of prediction” and to only train your model using those features. ",
    "url": "/portfolio/power-outages/#example-questions-and-prediction-problems",
    
    "relUrl": "/portfolio/power-outages/#example-questions-and-prediction-problems"
  },"45": {
    "doc": "Power Outages 🔋",
    "title": "Special Considerations",
    "content": "Step 2: Data Cleaning and Exploratory Data Analysis . | The data is given as an Excel file rather than a CSV. Open the data in Google Sheets or another spreadsheet application and determine which rows and columns of the sheet should be ignored when loading the data in pandas. | Note that pandas can load multiple filetypes: pd.read_csv, pd.read_excel, pd.read_html, pd.read_json, etc. | . | The power outage start date and time is given by 'OUTAGE.START.DATE' and 'OUTAGE.START.TIME'. It would be preferable if these two columns were combined into one pd.Timestamp column. Combine 'OUTAGE.START.DATE' and 'OUTAGE.START.TIME' into a new pd.Timestamp column called 'OUTAGE.START'. Similarly, combine 'OUTAGE.RESTORATION.DATE' and 'OUTAGE.RESTORATION.TIME' into a new pd.Timestamp column called 'OUTAGE.RESTORATION'. | pd.to_datetime and pd.to_timedelta will be useful here. | . | To visualize geospatial data, consider Folium or another geospatial plotting library. You can even embed Folium maps in your website! If fig is a folium.folium.Map object, then fig._repr_html_() evaluates to a string containing your plot as HTML; use open and write to write this string to an .html file. | . ",
    "url": "/portfolio/power-outages/#special-considerations",
    
    "relUrl": "/portfolio/power-outages/#special-considerations"
  },"46": {
    "doc": "Power Outages 🔋",
    "title": "Power Outages 🔋",
    "content": "go back to the Portfolio Homework spec . ",
    "url": "/portfolio/power-outages/",
    
    "relUrl": "/portfolio/power-outages/"
  },"47": {
    "doc": "Practical Data Science Lecture Questions 🤔",
    "title": "Practical Data Science Lecture Questions 🤔",
    "content": " ",
    "url": "/q/",
    
    "relUrl": "/q/"
  },"48": {
    "doc": "Recipes and Ratings 🍽️",
    "title": "Recipes and Ratings 🍽️",
    "content": " ",
    "url": "/portfolio/recipes-and-ratings/#recipes-and-ratings-%EF%B8%8F",
    
    "relUrl": "/portfolio/recipes-and-ratings/#recipes-and-ratings-️"
  },"49": {
    "doc": "Recipes and Ratings 🍽️",
    "title": "Table of Contents",
    "content": ". | Getting the Data . | Recipes | Ratings | . | Example Questions and Prediction Problems | Special Considerations . | Step 2: Data Cleaning and Exploratory Data Analysis | . | . This dataset contains recipes and ratings from food.com. It was originally scraped and used by the authors of this recommender systems paper. ",
    "url": "/portfolio/recipes-and-ratings/#table-of-contents",
    
    "relUrl": "/portfolio/recipes-and-ratings/#table-of-contents"
  },"50": {
    "doc": "Recipes and Ratings 🍽️",
    "title": "Getting the Data",
    "content": "Download the data here. You’ll download two CSV files: . | RAW_recipes.csv contains recipes. | RAW_interactions.csv contains reviews and ratings submitted for the recipes in RAW_recipes.csv. | . We’ve provided you with a subset of the raw data used in the original report, containing only the recipes and reviews posted since 2008, since the original data is quite large. A description of each column in both datasets is given below. Recipes . For context, you may want to look at an example recipe directly on food.com. | Column | Description | . | 'name' | Recipe name | . | 'id' | Recipe ID | . | 'minutes' | Minutes to prepare recipe | . | 'contributor_id' | User ID who submitted this recipe | . | 'submitted' | Date recipe was submitted | . | 'tags' | Food.com tags for recipe | . | 'nutrition' | Nutrition information in the form [calories (#), total fat (PDV), sugar (PDV), sodium (PDV), protein (PDV), saturated fat (PDV), carbohydrates (PDV)]; PDV stands for “percentage of daily value” | . | 'n_steps' | Number of steps in recipe | . | 'steps' | Text for recipe steps, in order | . | 'description' | User-provided description | . Ratings . | Column | Description | . | 'user_id' | User ID | . | 'recipe_id' | Recipe ID | . | 'date' | Date of interaction | . | 'rating' | Rating given | . | 'review' | Review text | . After downloading the datasets, you must follow the following steps to merge the two datasets and create a column containing the average rating per recipe: . | Left merge the recipes and interactions datasets together. | In the merged dataset, fill all ratings of 0 with np.nan. (Think about why this is a reasonable step, and include your justification in your website.) | Find the average rating per recipe, as a Series. | Add this Series containing the average rating per recipe back to the recipes dataset however you’d like (e.g., by merging). Use the resulting dataset for all of your analysis. | . ",
    "url": "/portfolio/recipes-and-ratings/#getting-the-data",
    
    "relUrl": "/portfolio/recipes-and-ratings/#getting-the-data"
  },"51": {
    "doc": "Recipes and Ratings 🍽️",
    "title": "Example Questions and Prediction Problems",
    "content": "Feel free to base your exploration into the dataset in Steps 1-2 around one of these questions, or come up with a question of your own. | What types of recipes tend to have the most calories? | What types of recipes tend to have higher average ratings? | What types of recipes tend to be healthier (i.e. more protein, fewer carbs)? | What is the relationship between the cooking time and average rating of recipes? | . Feel free to use one of the prompts below to build your predictive model in Steps 3-5, or come up with a prediction task of your own. | Predict ratings of recipes. | Predict the number of minutes to prepare recipes. | Predict the number of steps in recipes. | Predict calories of recipes. | . Make sure to justify what information you would know at the “time of prediction” and to only train your model using those features. ",
    "url": "/portfolio/recipes-and-ratings/#example-questions-and-prediction-problems",
    
    "relUrl": "/portfolio/recipes-and-ratings/#example-questions-and-prediction-problems"
  },"52": {
    "doc": "Recipes and Ratings 🍽️",
    "title": "Special Considerations",
    "content": "Step 2: Data Cleaning and Exploratory Data Analysis . Some columns, like 'nutrition', contain values that look like lists, but are actually strings that look like lists. You may want to turn the strings into actual lists, or create columns for every unique value in those lists. For instance, per the data dictionary, each value in the 'nutrition' column contains information in the form \"[calories (#), total fat (PDV), sugar (PDV), sodium (PDV), protein (PDV), saturated fat (PDV), and carbohydrates (PDV)]\"; you could create individual columns in your dataset titled 'calories', 'total fat', etc. ",
    "url": "/portfolio/recipes-and-ratings/#special-considerations",
    
    "relUrl": "/portfolio/recipes-and-ratings/#special-considerations"
  },"53": {
    "doc": "Recipes and Ratings 🍽️",
    "title": "Recipes and Ratings 🍽️",
    "content": "go back to the Portfolio Homework spec . ",
    "url": "/portfolio/recipes-and-ratings/",
    
    "relUrl": "/portfolio/recipes-and-ratings/"
  },"54": {
    "doc": "📚 Resources",
    "title": "📚 Resources",
    "content": "Welcome to Practical Data Science in Winter 2025! This site is currently under construction. Until this disclaimer is removed, all information here is subject to change. See you on January 8th! . ",
    "url": "/resources/",
    
    "relUrl": "/resources/"
  },"55": {
    "doc": "👩‍🏫 Staff",
    "title": "👩‍🏫 Staff",
    "content": "Welcome to Practical Data Science in Winter 2025! This site is currently under construction. Until this disclaimer is removed, all information here is subject to change. See you on January 8th! . ",
    "url": "/staff/",
    
    "relUrl": "/staff/"
  },"56": {
    "doc": "👩‍🏫 Staff",
    "title": "Instructor",
    "content": "Suraj Rampure he/him . rampure@umich.edu Lecture: MW 3-4:30PM, 1670 BBB . 🎓 Lecturer III, Computer Science and Engineering . 🏠 Windsor, Ontario, Canada 🇨🇦 . 🙋 credit card points, traveling, and planes; my dog Junior, (trying to) gym, this . 🍜 Kitab Cafe, Bash Izakaya, Zingerman's, Tomukun, Buffalo Wild Wings, Antonino's . ",
    "url": "/staff/#instructor",
    
    "relUrl": "/staff/#instructor"
  },"57": {
    "doc": "📖 Syllabus",
    "title": "📖 Syllabus",
    "content": "Welcome to Practical Data Science in Winter 2025! This site is currently under construction. Until this disclaimer is removed, all information here is subject to change. See you on January 8th! . ",
    "url": "/syllabus/",
    
    "relUrl": "/syllabus/"
  },"58": {
    "doc": "WI25 Redirect",
    "title": "WI25 Redirect",
    "content": " ",
    "url": "/wi25/",
    
    "relUrl": "/wi25/"
  }
}
