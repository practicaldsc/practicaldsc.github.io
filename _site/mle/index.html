<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"><link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"><style id="jtd-nav-activation"> .site-nav ul li a { background-image: none; }</style><script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico" type="image/x-icon"><title>♾️ Maximum Likelihood Estimation | EECS 398</title><meta name="generator" content="Jekyll v3.9.5" /><meta property="og:title" content="♾️ Maximum Likelihood Estimation" /><meta name="author" content="Practical Data Science course staff" /><meta property="og:locale" content="en_US" /><meta name="description" content="The current semester’s course website for EECS 398: Practical Data Science." /><meta property="og:description" content="The current semester’s course website for EECS 398: Practical Data Science." /><link rel="canonical" href="http://localhost:4000/mle/" /><meta property="og:url" content="http://localhost:4000/mle/" /><meta property="og:site_name" content="EECS 398" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="♾️ Maximum Likelihood Estimation" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Practical Data Science course staff"},"description":"The current semester’s course website for EECS 398: Practical Data Science.","headline":"♾️ Maximum Likelihood Estimation","url":"http://localhost:4000/mle/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> EECS 398 </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">🏡 Home</a><li class="nav-list-item"><a href="/syllabus/" class="nav-list-link">📖 Syllabus</a><li class="nav-list-item"><a href="/calendar/" class="nav-list-link">📆 Calendar</a><li class="nav-list-item"><a href="/env-setup/" class="nav-list-link">⚙️ Environment Setup</a><li class="nav-list-item"><a href="/resources/" class="nav-list-link">📚 Resources</a><li class="nav-list-item"><a href="/staff/" class="nav-list-link">👩‍🏫 Staff</a><li class="nav-list-item"><a href="/faqs/" class="nav-list-link">🙋 FAQs</a><li class="nav-list-item"><a href="/lin-alg/" class="nav-list-link">🧮 Linear Algebra Review</a></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search EECS 398" aria-label="Search EECS 398" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div></div><div class="main-content-wrap"><div id="main-content" class="main-content"><main><h1 class="no_toc" id="️-maximum-likelihood-estimation"> <a href="#️-maximum-likelihood-estimation" class="anchor-heading" aria-labelledby="️-maximum-likelihood-estimation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> ♾️ Maximum Likelihood Estimation</h1><h2 class="no_toc text-delta" id="table-of-contents"> <a href="#table-of-contents" class="anchor-heading" aria-labelledby="table-of-contents"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Table of contents</h2><ol id="markdown-toc"><li><a href="#overview" id="markdown-toc-overview">Overview</a><li><a href="#problem-setup" id="markdown-toc-problem-setup">Problem Setup</a><li><a href="#the-likelihood-function" id="markdown-toc-the-likelihood-function">The Likelihood Function</a><li><a href="#maximizing-likelihood" id="markdown-toc-maximizing-likelihood">Maximizing Likelihood</a><li><a href="#maximizing-log-likelihood" id="markdown-toc-maximizing-log-likelihood">Maximizing Log Likelihood</a><li><a href="#summary" id="markdown-toc-summary">Summary</a></ol><script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> </script><h2 id="overview"> <a href="#overview" class="anchor-heading" aria-labelledby="overview"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Overview</h2><p>In Lecture 14, we discussed the relationship between probability and statistics:</p><ul><li>In probability questions, we’re given some model of how the universe works, and it’s our job to determine how various samples could turn out.<br /><small>Example: If we have 5 blue marbles and 3 green marbles and pick 2 at random, what are the chances we see one marble of each?</small><li>In statistics questions, we’re given information about a sample, and it’s our job to figure out how the universe – or <strong>data generating process</strong> works.<br /><small>Example: Repeatedly, I picked 2 marbles at random from a bag with replacement. I don’t know what’s inside the bag. One time, I saw 2 blue marbles, then next time I saw 1 of each, the next time I saw 2 red marbles, and so on. What marbles are inside the bag?</small></ul><center> <img src="../assets/mle/prob-stat.png" width="55%" /> </center><p>In this note, we’ll gain a deeper understanding of this relationship, through the lens of your probability knowledge from EECS 203. After reading this note, you’ll be well-equipped to tackle Question 4 on Homework 7 (and also have better context for machine learning, more generally).</p><hr /><h2 id="problem-setup"> <a href="#problem-setup" class="anchor-heading" aria-labelledby="problem-setup"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Problem Setup</h2><p>Let’s work with the example mentioned in <a href="https://practicaldsc.org/resources/lectures/lec14/lec14-filled.pdf#page=13">Lecture 14, Slide 13</a>. Suppose we find a coin on the ground, and we’re unsure of whether the coin is <strong>fair</strong>. We decide to flip the coin repeatedly to estimate its <strong>bias</strong>, \(\theta\), which is the probability of flipping heads on any particular flip. (The probability of flipping tails on any particular flip, then, is \(1 - \theta\)).</p><p>Suppose we flip the coin 100 times and see 65 heads. Assuming that each flip is independent, this is a <em>possible</em> result, no matter what the value of \(\theta\) is, as long as \(0 &lt; \theta &lt; 1\). But, some values of \(\theta\) are more believable than others:</p><ul><li>For example, if \(\theta = 0.5\), the chances of seeing 65 heads and 35 tails is:</ul>\[\mathbb{P}(65 \text{ heads} \: | \: \theta = 0.5) = {\binom{100}{65}} 0.5^{65} 0.5^{35} \approx 0.00086\]<ul><li>If \(\theta = 0.7\), the chances of seeing 65 heads and 35 tails is:</ul>\[\mathbb{P}(65 \text{ heads} \: | \: \theta = 0.7) = {\binom{100}{65}} 0.7^{65} 0.3^{35} \approx 0.04678\]<p>Again, the true bias, \(\theta\), could be anything, and we don’t <strong>truly</strong> know what it is, since we just found this coin on the ground. But, as we see above, some values of \(\theta\) are more <strong>likely</strong> than others – for instance, it seems that \(\theta = 0.7\) is more likely than \(\theta = 0.5\), because the probability of our observation is higher if we assume \(\theta = 0.7\) than if we assume \(\theta = 0.5\).</p><p>\(\theta = 0.5\) and \(\theta = 0.7\) were arbitrarily chosen values of \(\theta\), just for illustration. The question is, <strong>what is the <em>most</em> likely value of \(\theta\), among all possible \(\theta\)’s</strong>?</p><hr /><h2 id="the-likelihood-function"> <a href="#the-likelihood-function" class="anchor-heading" aria-labelledby="the-likelihood-function"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Likelihood Function</h2><p>To answer this question, we’ll define what’s known as the <strong>likelihood</strong> function of \(\theta\), denoted \(L(\theta)\):</p>\[L(\theta) = \mathbb{P}(65 \text{ heads} \: | \: \theta) = {\binom{100}{65}} \theta^{65} (1-\theta)^{35}\]<p>We’ve used the binomial distribution, which you saw in EECS 203, to calculate the probability of seeing 65 heads and 35 tails, given a bias of \(\theta\). The function \(L(\theta)\) is given the special name of “likelihood” because it helps us measure how <strong>likely</strong> a particular value of \(\theta\) is. It emphasizes that \(\theta\) is <strong>unknown</strong>, whereas in most classical probability examples you’ve dealt with, \(\theta\) was known, but the number of heads (for example) was unknown.</p><p>\(\theta\) is referred to as a <strong>parameter</strong> of the binomial distribution, and our goal is to <strong>estimate</strong> \(\theta\) as best as we can, given our data. The word parameter here means the same as it did in Lecture 14 – a parameter defines the relationship between the inputs and outputs of a model, and we’re using the data we’re given to find optimal parameters. Here, the model is a binomial one, which takes in a number of heads and outputs the probability of seeing that many heads.</p><p>Let’s look at a plot of \(L(\theta)\) for various values of \(\theta\).</p><center> <iframe src="../assets/mle/mle-1.html" width="650" height="400" frameborder="0" scrolling="no"></iframe> </center><p>You should notice that \(L(\theta)\) peaks at 0.65, which is the empirical proportion of heads – remember that we saw 65 heads in 100 flips of this coin!</p><hr /><h2 id="maximizing-likelihood"> <a href="#maximizing-likelihood" class="anchor-heading" aria-labelledby="maximizing-likelihood"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Maximizing Likelihood</h2><p>Let’s see if we can prove that this is always the case – that is, let’s prove that the most likely bias of a coin, \(\theta\), when we flip it many times, is \(\frac{\text{number of heads}}{\text{total number of flips}}\).</p><p>First, let’s pose the problem more generally. If we have a coin that flips heads with probability \(\theta\), the probability of seeing \(k\) heads in \(n\) independent flips of the coin is:</p>\[L(\theta) = \mathbb{P}(k \text{ heads} \: | \: \theta) = {n \choose k} \theta^k (1-\theta)^{n-k}\]<p>The question at hand is, which value of \(\theta\) <strong>maximizes</strong> \(L(\theta)\)? This resembles a question we dealt with in Lecture 14 – which value of \(h\) minimizes \(R_\text{sq}(h)\)? – the only difference being that there, we minimized, and here, we’re maximizing.</p><p>\(L(\theta)\) is a function of a single variable. To find the value of \(\theta\) that maximizes it, we can follow the same process from Lecture 14, where we take its derivative with respect to \(\theta\), and solve for the value of \(\theta\) that makes the derivative 0.</p><p>Let’s do it. To find the derivative of \(L(\theta)\) with respect to \(\theta\), we’ll need to use the power, product, and chain rules from calculus. Here we go!</p>\[\begin{align*} L(\theta) &amp;= { n \choose k } \theta^k (1-\theta)^{n-k} \\ \frac{d}{d\theta}L(\theta) &amp;= { n \choose k } \big( k\theta^{k-1} (1-\theta)^{n-k} + \theta^k (n-k)(1-\theta)^{n-k-1}(-1) \big) \end{align*}\]<p>Now, we’ll set this to 0 and solve for the \(\theta\) that makes this happen. We’re going to call the resulting value \(\theta^*\), to emphasize that it’s the “best” \(\theta\) in some sense.</p>\[\begin{align*} { n \choose k }\cdot \left( k\theta^{k-1} (1-\theta)^{n-k} + \theta^k (n-k)(1-\theta)^{n-k-1}(-1) \right) &amp;= 0 \\ k\theta^{k-1} (1-\theta)^{n-k} - \theta^k (n-k)(1-\theta)^{n-k-1} &amp;= 0 \\ k\theta^{k-1} (1-\theta)^{n-k} &amp;= \theta^k (n-k)(1-\theta)^{n-k-1} \\ k (1-\theta) &amp;= \theta (n-k) \\ k - \theta k &amp;= \theta n - \theta k \\ k &amp;= \theta n \\ \theta^* &amp;= \boxed{\frac{k}{n}} \end{align*}\]<p>Since \(\theta^* = \frac{k}{n}\) is the input to \(L(\theta)\) that <strong>maximizes</strong> the likelihood function, \(L(\theta)\), we call \(\theta^*\) the <strong>maximum likelihood estimate</strong> of \(\theta\).</p><p>In our example, \(k = 65\) and \(n = 100\), which means the maximum likelihood estimate of the bias of the coin we found on the ground is \(\frac{65}{100} = 0.65\)! This matches what we saw in the graph earlier, and also shouldn’t be surprising. In our 100 flips of this random coin off the ground, we saw 65 heads and 35 tails, and so while the bias of the coin could be anything, the <strong>most likely</strong> bias of the coin is \(\theta = 0.65\).</p><p>We’ve now walked through a full example of the method of maximum likelihood estimation.</p><hr /><h2 id="maximizing-log-likelihood"> <a href="#maximizing-log-likelihood" class="anchor-heading" aria-labelledby="maximizing-log-likelihood"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Maximizing Log Likelihood</h2><p>The process of solving for \(\theta^*\) was messy, because taking the derivative of \(L(p)\) was messy, since it involved using the power, product, and chain rules!</p><p>It turns out that there’s a simplification we can use here to make our lives easier, that is often used in machine learning and statistics.</p><blockquote><p><strong>Fact</strong>: If \(x^*\) maximizes \(f(x)\), then \(x^*\) also maximizes \(\log f(x)\).<br />Related, if \(x^*\) minimizes \(f(x)\), then \(x^*\) also minimizes \(\log f(x)\).</p></blockquote><p>To see why this is true, let’s plot a graph of \(\log L(\theta)\) vs. \(\theta\):</p><center> <iframe src="../assets/mle/mle-2.html" width="650" height="400" frameborder="0" scrolling="no"></iframe> </center><p>While the graph of \(\log L(\theta)\) looks very different than the graph of \(L(\theta)\), they are both maximized at the same position, \(\theta^* = 0.65\). The reason for this is that \(\log\) is <strong>monotonically increasing</strong>, meaning that if \(a &gt; b\), then \(\log(a) &gt; \log(b)\). The consequence of this is that if \(L(0.65)\) is bigger than \(L(\theta)\) for any other \(\theta\), then \(\log L(0.65)\) is bigger than \(\log L(\theta)\) for any other \(\theta\), too.</p><p>You may be wondering, and rightfully so:</p><blockquote><p>Suraj, why did you randomly bring out the \(\log\) function – isn’t this explanation already long and mathematical enough?</p></blockquote><p>It turns out that maximizing \(\log L(\theta)\) is <strong>way</strong> easier than maximizing \(L(\theta)\)! Here, let’s work through it. First, let’s simplify \(\log L(\theta)\). Note that we could use any base on our logarithm, but the calculations are simplest if we use the natural logarithm (which we’ll just denote with \(\log\)).</p>\[\begin{align*} L(\theta) &amp;= {n \choose k} \theta^k (1-\theta)^{n-k} \\ \log L(\theta) &amp;= \log \left( {n \choose k} \theta^k (1-\theta)^{n-k} \right) \\ &amp;= \log {n \choose k} + \log \left(\theta^k \right) + \log \left((1 - \theta)^{n-k}\right) \\ &amp;= \log {n \choose k} + k \log \theta + (n - k) \log (1 - \theta) \end{align*}\]<p>Now, let’s take the derivative of \(\log L(\theta)\) and set it to 0:</p>\[\begin{align*} \log L(\theta) &amp;= \log {n \choose k} + k \log \theta + (n - k) \log (1 - \theta) \\ \frac{d}{d\theta} \log L(\theta) &amp;= 0 + k \cdot \frac{1}{\theta} + (n - k) \cdot\frac{1}{1 - \theta}(-1) = \frac{k}{\theta} - \frac{n - k}{1 - \theta} \\ 0 &amp;= \frac{k}{\theta} - \frac{n - k}{1 - \theta} \\ \frac{k}{\theta} &amp;= \frac{n - k}{1 - \theta} \\ k - \theta k &amp;= \theta n - \theta k \\ k &amp;= \theta n \\ \theta^* &amp;= \boxed{\frac{k}{n}} \end{align*}\]<p>By taking the log of \(L(\theta)\), we were able to find the maximum likelihood estimate, \(\theta^*\), without needing to find the derivative of \(L(\theta)\), which involved working with expressions like \(k\theta^{k-1} (1-\theta)^{n-k} + \theta^k (n-k)(1-\theta)^{n-k-1}(-1)\).</p><p>The benefit of the log function is that it turns <strong>products</strong> into <strong>sums</strong> – that is, \(\log(a \cdot b) = \log(a) + \log(b)\) – which allows us to bypass using the messy product rule.</p><hr /><h2 id="summary"> <a href="#summary" class="anchor-heading" aria-labelledby="summary"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Summary</h2><ol><li>We found a coin on the ground, which flips heads with an unknown probability, \(\theta\).<li>We flipped the coin 100 times and saw 65 heads.<li><p>To estimate the bias of the coin, we decided to use <strong>maximum likelihood estimation</strong>, which requires us to define the <strong>likelihood function</strong> for \(\theta\):</p>\[L(\theta) = \mathbb{P}(k \text{ heads} \: | \: \theta) = {n \choose k} \theta^k (1-\theta)^{n-k}\]<p>in the example stated above, \(k = 65\) and \(n = 100\).</p><li>The <strong>most likely</strong> value of \(\theta\) is the one that <strong>maximizes</strong> \(L(\theta)\).<li>To make the math simpler, instead of maximizing \(L(\theta)\) directly, we maximized \(\log L(\theta)\). This found us the same maximum likelihood estimate, \(\theta^* = \frac{k}{n}\).</ol></main></div></div><div class="search-overlay"></div></div>
