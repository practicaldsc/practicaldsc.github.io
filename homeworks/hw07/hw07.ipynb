{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e0b501",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\">\n",
    "\n",
    "#### Homework 7\n",
    "\n",
    "# Loss Functions and Linear Algebra\n",
    "\n",
    "### EECS 398-003: Practical Data Science, Fall 2024\n",
    "\n",
    "#### Due Thursday, October 24th at 11:59PM\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738d1e3",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Welcome to Homework 7! In this homework, you'll gain a strong understanding of a key concept in machine learning: loss functions. Along the way, you'll practice working with summation notation, derivatives, limits, and linear algebra, all concepts that are crucial to machine learning. See the [Readings section of the Resources tab on the course website](https://practicaldsc.org/resources/#readings) for supplemental resources.\n",
    "\n",
    "You are given six slip days throughout the semester to extend deadlines. See the [Syllabus](https://practicaldsc.org/syllabus) for more details. With the exception of using slip days, late work will not be accepted unless you have made special arrangements with your instructor.\n",
    "\n",
    "To access this notebook, you'll need to clone our [public GitHub repository](https://github.com/practicaldsc/fa24/). The [‚öôÔ∏è Environment Setup](https://practicaldsc.org/env-setup) page on the course website walks you through the necessary steps.\n",
    "<div class=\"alert alert-warning\" markdown=\"1\">\n",
    "    \n",
    "Unlike other homeworks, you **are not** going to submit this notebook! Instead, you will write **all** of your answers to the questions in this homework in a separate PDF. You can create this PDF either digitally, using your tablet or using [Overleaf + LaTeX](https://overleaf.com) (or some other sort of digital document), or by writing your answers on a piece of paper and scanning them in. \n",
    "\n",
    "**Make sure to show your work for all questions! Answers without work shown may not receive full credit.**\n",
    "</div>\n",
    "    \n",
    "This homework is worth a total of **56 points**, all of which come from **manual grading**. The number of points each question is worth is listed at the start of each question. **All questions in the assignment are independent, so feel free to move around if you get stuck**. Tip: if you're using Jupyter Lab, you can see a Table of Contents for the notebook by going to View > Table of Contents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ae98d",
   "metadata": {},
   "source": [
    "To get started, run the cell below. There's no need to `import otter` at the top, since there are no autograder tests. But, you will still run some code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Preferred styles\n",
    "pio.templates[\"pds\"] = go.layout.Template(\n",
    "    layout=dict(\n",
    "        margin=dict(l=30, r=30, t=30, b=30),\n",
    "        autosize=True,\n",
    "        width=600,\n",
    "        height=400,\n",
    "        xaxis=dict(showgrid=True),\n",
    "        yaxis=dict(showgrid=True),\n",
    "        title=dict(x=0.5, xanchor=\"center\"),\n",
    "    )\n",
    ")\n",
    "pio.templates.default = \"simple_white+pds\"\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98133a4a",
   "metadata": {},
   "source": [
    "## Question 1: Imputation Returns üìè\n",
    "\n",
    "---\n",
    "\n",
    "Earlier in the semester, we learned about mean imputation, a technique for handling missing values in a dataset. Specifically, with mean imputation, we fill in all of the missing values in a column with the mean of the observed values in that column. (Here, we'll just consider _unconditional_ mean imputation.)\n",
    "\n",
    "One of the observations we made in [Lecture 8](https://practicaldsc.org/resources/lectures/lec08/lec08-filled.html#Idea:-Mean-imputation) is that when performing mean imputation:\n",
    "\n",
    "- the mean of the imputed column is **the same as** the mean of the observed values, pre-imputation, and\n",
    "- the standard deviation of the imputed column is **less than** the standard deviation of the observed values, pre-imputation.\n",
    "\n",
    "For clarity, let's illustrate that fact once more. Run the cell below to load in the same `heights` DataFrame we used in Lecture 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4778e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = pd.read_csv('data/heights-missing-2.csv')\n",
    "heights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f73ee0",
   "metadata": {},
   "source": [
    "The `'child'` column in `heights` has many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 169 missing values, 765 present values.\n",
    "original_heights = heights['child']\n",
    "original_heights.isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062927f",
   "metadata": {},
   "source": [
    "When dropping all missing values, the mean and standard deviation of `heights` is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb78d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_heights = heights['child']\n",
    "original_heights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll talk about what ddof=0 does later in the question.\n",
    "# For now, just interpret this result as the standard deviation.\n",
    "original_heights.std(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883593e",
   "metadata": {},
   "source": [
    "After filling in missing `'child'` heights with the mean of the observed heights, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_heights = original_heights.fillna(original_heights.mean())\n",
    "\n",
    "# The same as original_heights.mean()!\n",
    "imputed_heights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller than original_heights.std(ddof=0)!\n",
    "imputed_heights.std(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f01cd",
   "metadata": {},
   "source": [
    "Again, we see that the mean pre-imputation and post-imputation is the same, but the standard deviations are different. But is the mean always guaranteed to be the same after performing mean imputation? And what is the relationship between the standard deviation pre-imputation, $\\approx 3.52047$, and the standard deviation post-imputation, $\\approx 3.18609$?\n",
    "\n",
    "In this question, we will mathematically **prove** the relationships we're seeing above in more generality, to better understand the properties of mean imputation. This will also give us practice with manipulating equations involving summations and squares, which we'll need to do a lot to understand the machinery behind machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391764cd",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1.1 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Consider a column of $n$ numbers, $y_1, y_2, ..., y_n$ with mean $M$ and standard deviation $S$, where the standard deviation is defined as follows:\n",
    "\n",
    "$$S = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n (y_i - M)^2}$$\n",
    "\n",
    "Suppose we introduce $k$ new values to the dataset, $y_{n+1}, y_{n+2}, ... , y_{n+k}$, all of which are equal to $M$. (This is like mean imputation, if the full dataset had $n + k$ values, with $n$ observed/not missing and $k$ missing, and we imputed the $k$ missing values with the mean of the observed, $M$.)\n",
    "\n",
    "Let the new mean and standard deviation of all $n+k$ values be $M'$ and $S'$, respectively.\n",
    "\n",
    "**Prove that $M' = M$.**\n",
    "\n",
    "Some guidance: It's not sufficient to provide a verbal argument. Instead, start with the definition of $M' = \\frac{1}{n+k} \\sum_{i = 1}^{n + k} y_i$, and manipulate the sum to show that it's equal to $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab5768",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1.2 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "\n",
    "Using the same definitions as in Question 1.1, find $S'$ in terms of $M$, $n$, $k$, and $S$. Show your work.\n",
    "\n",
    "Some guidance: \n",
    "- You may not need to use all of these variables in your answer.\n",
    "- To verify your answer is correct, plug in $M = 67.10340$, $n = 765$, $k = 169$, and $S = 3.52047$. The answer you get should be approximately $3.18609$, as we saw at the start of the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570f9e8",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 2: Relative Squared Loss üßë‚Äçüßë‚Äçüßí‚Äçüßí\n",
    "\n",
    "---\n",
    "\n",
    "In [Lecture 14](https://practicaldsc.org/resources/lectures/lec14/lec14-filled.pdf), we introduced the \"modeling recipe\" for making predictions:\n",
    "\n",
    "1. Choose a model.\n",
    "1. Choose a loss function.\n",
    "1. Minimize average loss to find optimal model parameters.\n",
    "\n",
    "The first instance of this recipe saw us choose:\n",
    "1. The constant model, $H(x) = h$.\n",
    "1. The squared loss function: $L_\\text{sq}(y_i, h) = (y_i - h)^2$.\n",
    "1. The average squared loss across our entire dataset, then, was:\n",
    "$$R_\\text{sq}(h) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - h)^2$$\n",
    "which, using calculus, we showed is minimized when: $$h^* = \\text{Mean}(y_1, y_2, ..., y_n)$$\n",
    "This means that using the squared loss function, the **best** constant prediction is $h^* = \\text{Mean}(y_1, y_2, ..., y_n)$.\n",
    "\n",
    "In this question, you will find the best constant prediction when using a different loss function. In particular, here, we'll explore the **relative squared loss** function, $L_{\\text{rsq}}(y_i, h)$:\n",
    "\n",
    "$$L_{\\text{rsq}}(y_i, h) = \\frac{(y_i - h)^2}{y_i}$$\n",
    "\n",
    "Throughout this question, assume that each of $y_1, y_2, ..., y_n$ is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55f7f1",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.1 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Determine $\\frac{d}{d h} L_{\\text{rsq}}(h)$, the derivative of the relative squared loss function with respect to $h$.\n",
    "\n",
    "(Technically, this is a **partial** derivative, since there are other variables in the definition of $L_\\text{rsq}(h)$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe707f4a",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.2 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "\n",
    "What value of $h$ minimizes average loss when using the relative squared loss function ‚Äì that is, what is $h^*$? Your answer should only be in terms of the variables $n, y_1, y_2, ..., y_n$, and any constants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f190fb",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.3 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "\n",
    "Let $C(y_1, y_2, ..., y_n)$ be your minimizer $h^*$ from Question 2.2. That is, for a particular dataset $y_1, y_2, ..., y_n$, $C(y_1, y_2, ..., y_n)$ is the value of $h$ that minimizes empirical risk for relative squared loss on that dataset.\n",
    "\n",
    "What is the value of $\\displaystyle\\lim_{y_4 \\rightarrow \\infty} C(1, 3, 5, y_4)$ in terms of $C(1, 3, 5)$? Your answer should involve the function $C$ and/or one or more constants.\n",
    "\n",
    "Some guidance: To notice the pattern, evaluate $C(1, 3, 5, 100)$, $C(1, 3, 5, 10000)$, and $C(1, 3, 5, 1000000)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870acab",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.4 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "What is the value of $\\displaystyle\\lim_{y_4 \\rightarrow 0} C(1, 3, 5, y_4)$? Again, your answer should involve the function $C$ and/or one or more constants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ce513",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.5 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Based on the results of Questions 2.3 and 2.4, when is the prediction $C(y_1, y_2, ..., y_n)$ robust to outliers? When is it not robust to outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd985f",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 3: Bye, Calculus üëã\n",
    "\n",
    "---\n",
    "\n",
    "As we discussed in the previous question, in Lecture 2, we found that $h^* = \\text{Mean}(y_1, y_2, ..., y_n)$ is the constant prediction that minimizes mean squared error:\n",
    "\n",
    "$$R_\\text{sq}(h) = \\frac{1}{n} \\sum_{i = 1}^n (y_i-h)^2$$\n",
    "\n",
    "To arrive at this result, we used calculus: we took the derivative of $R_\\text{sq}(h)$ with respect to $h$, set it equal to 0, and solved for the resulting value of $h$, which we called $h^*$.\n",
    "\n",
    "In this question, we will minimize $R_\\text{sq}(h)$ in a way that **doesn't** use calculus. The general idea is this: if $f(x) = (x - c)^2 + k$, then we know that $f$ is a quadratic function that opens upwards with a vertex at $(c, k)$, meaning that $x = c$ minimizes $f$. As we saw in class (see [Lecture 14, Slide 35](https://practicaldsc.org/resources/lectures/lec14/lec14-filled.pdf#page=35)), $R_\\text{sq}(h)$ is a quadratic function of $h$!\n",
    "\n",
    "Throughout this problem, let $y_1, y_2, ..., y_n$ be an arbitrary dataset, and let $\\bar{y} = \\frac{1}{n} \\sum_{i = 1}^n y_i$ be the mean of the $y$'s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06480557",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3.1 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "What is the value of $\\sum_{i = 1}^n (y_i - \\bar{y})$? Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6196ff8",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3.2 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Show that:\n",
    "\n",
    "$$R_\\text{sq}(h) = \\frac{1}{n} \\sum_{i = 1}^n \\left( (y_i - \\bar{y})^2 + 2(y_i - \\bar{y})(\\bar{y} - h) + (\\bar{y} - h)^2 \\right)$$\n",
    "\n",
    "Some guidance:\n",
    "- To proceed, start by rewriting $y_i - h$ in the definition of $R_\\text{sq}(h)$ as $(y_i - \\bar{y}) + (\\bar{y} - h)$. Why is this a valid step?\n",
    "- Make sure not to expand unnecessarily. Your work should only take ~3 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25844a",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3.3 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Show that:\n",
    "\n",
    "$$R_\\text{sq}(h) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - \\bar{y})^2 + (\\bar{y} - h)^2$$\n",
    "\n",
    "This is called the **bias-variance decomposition** of $R_\\text{sq}(h)$, which is an idea we'll revisit in the coming weeks.\n",
    "\n",
    "Some guidance: At some point, you will need to use your result from Question 3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3246244",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3.4 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">1 Point</div>\n",
    "\n",
    "Why does the result in Question 3.3 prove that $h^* = \\text{Mean}(y_1, y_2, ..., y_n)$ minimizes $R_\\text{sq}(h)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29b485",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3.5 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">1 Point</div>\n",
    "\n",
    "In Question 3.3, you showed that:\n",
    "\n",
    "$$R_\\text{sq}(h) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - \\bar{y})^2 + (\\bar{y} - h)^2$$\n",
    "\n",
    "Take a close look at the equation above, then fill in the blank below with **a single word**:\n",
    "\n",
    "> The value of $R_\\text{sq}(h^*)$, when $h^* = \\text{Mean}(y_1, y_2, ..., y_n)$, is equal to the ____ of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1a0d5",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4: Probability ü§ù Statistics\n",
    "\n",
    "---\n",
    "\n",
    "In Lecture 14, we discussed the relationship between probability and statistics:\n",
    "- In probability questions, we're given some model of how the universe works, and it's our job to determine how various samples could turn out.<br><small>Example: If we have 5 blue marbles and 3 green marbles and pick 2 at random, what are the chances we see one marble of each?</small>\n",
    "- In statistics questions, we're given information about a sample, and it's our job to figure out how the universe ‚Äì or **data generating process** works.<br><small>Example: Repeatedly, I picked 2 marbles at random from a bag with replacement. I don't know what's inside the bag. One time, I saw 2 blue marbles, then next time I saw 1 of each, the next time I saw 2 red marbles, and so on. What marbles are inside the bag?</small>\n",
    "\n",
    "In this question, we'll gain a deeper understanding of this relationship, through the lens of your probability knowledge from EECS 203. To do so, we'll introduce you to a key idea in machine learning and statistics, called **maximum likelihood estimation**.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<h4>Click <a href=\"https://practicaldsc.org/mle\"><b>here</b></a> to read a lecture note (written by us) that introduces you to maximum likelihood estimation!</h4>You <i>can</i> attempt the question without reading the note, but it'll be significantly more difficult.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c194a",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4.1 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "When you step on campus, each person you see has a $0.1$ chance of saying \"Go Blue!\" to you, independent of all other people.\n",
    "\n",
    "Tomorrow, what's the probability that the first person to say \"Go Blue!\" to you is the **6th** person you see?\n",
    "\n",
    "Leave your answer in unsimplified form. This question should not take very long; think back to the probability distributions you learned in EECS 203 (other than the binomial distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3d6669",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4.2 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Again, assume that the probability that each person you see has a $0.1$ chance of saying \"Go Blue!\" to you, independent of all other people. \n",
    "\n",
    "What's the probability that:\n",
    "- the first person to say \"Go Blue!\" to you tomorrow is the **6th** person you see, **and**\n",
    "- the first person to say \"Go Blue!\" to you the day after tomorrow is the **10th** person you see, **and**\n",
    "- the first person to say \"Go Blue!\" to you the day after that is the **2nd** person you see?\n",
    "\n",
    "Again, leave your answer in unsimplified form. Note that we're asking for a single probability, not three separate probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8350d3b",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4.3 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "\n",
    "Now, suppose that the probability that each person you see says \"Go Blue!\" to you is some **unknown parameter**, $\\pi$. (That is, $0.1$ will not appear in the rest of this question.)\n",
    "\n",
    "Suppose you go to campus on $n$ straight days, and you collect a dataset $x_1, x_2, ..., x_n$, where:\n",
    "- On Day 1, the first person to say \"Go Blue!\" to you is the $x_1$th person you saw (or \"person $x_1$\"), **and**\n",
    "- On Day 2, the first person to say \"Go Blue!\" to you is person $x_2$, **and**,\n",
    "- On Day 3, the first person to say \"Go Blue!\" to you is person $x_3$, **and** so on.\n",
    "- In general, for $i = 1, 2, ..., n$, on Day $i$, the first person to say \"Go Blue!\" to you is person $x_i$.\n",
    "\n",
    "For example, the dataset $x_1 = 5, x_2 = 10, x_3 = 2$ would mean that on Day 1, person 5 was the first to say \"Go Blue!\"; on Day 2, person 10 was the first to say \"Go Blue!\"; and on Day 3, person 2 was the first to say \"Go Blue!\".\n",
    "\n",
    "**Prove** that $\\log L(\\pi)$, the log of the likelihood function for $\\pi$, is:\n",
    "\n",
    "$$\\log L(\\pi) = \\log(1 - \\pi) \\sum_{i = 1}^n (x_i - 1) + n \\log \\pi$$\n",
    "\n",
    "Some guidance: Try and generalize the calculation you made in Question 4.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2641b",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4.4 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "\n",
    "Using the result to Question 4.3, find $\\pi^*$, the maximum likelihood estimate of $\\pi$ given the dataset $x_1, x_2, ..., x_n$. Once you've done that, give a brief English explanation of why the value of $\\pi^*$ makes intuitive sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae35b5",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 5: More and More Losses üÖª\n",
    "\n",
    "---\n",
    "\n",
    "As we mentioned in Questions 2 and 3, $h^* = \\text{Mean}(y_1, y_2, ..., y_n)$ is the constant prediction that minimizes mean squared error, i.e. average squared loss:\n",
    "\n",
    "$$R_\\text{sq}(h) = \\frac{1}{n} \\sum_{i = 1}^n (y_i-h)^2$$\n",
    "\n",
    "Related, $h^* = \\text{Median}(y_1, y_2, ..., y_n)$ is the constant prediction that minimizes mean absolute error, i.e. average absolute loss:\n",
    "\n",
    "$$R_{\\text{abs}}(h) = \\frac{1}{n} \\sum_{i=1}^n \\left|y_i - h\\right|$$\n",
    "\n",
    "You may notice that the formulas for $R_\\text{sq}(h)$ and $R_\\text{abs}(h)$ look awfully similar ‚Äì they're nearly identical besides the exponent. More generally, for any positive integer $p$, define the $L_p$ loss as follows:\n",
    "\n",
    "$$L_p(y_i, h) = |y_i - h|^p$$\n",
    "\n",
    "With this definition, $L_2$ loss is the same as squared loss and $L_1$ loss is the same as absolute loss. The corresponding average loss, for any value of $p$, is then:\n",
    "\n",
    "$$ R_{p}(h) = \\frac{1}{n} \\sum_{i=1}^n \\left|y_i - h\\right| ^ p $$\n",
    "\n",
    "Written in terms of $R_p(h)$, we know ‚Äì from the top of this question ‚Äì that:\n",
    "\n",
    "- The minimizer of $R_1(h)$ is $\\text{Median}(y_1, y_2, ..., y_n)$:\n",
    "\n",
    "$$\\text{Median}(y_1, y_2, ..., y_n) = \\underset{h}{\\mathrm{argmin}} \\: R_1(h)$$\n",
    "\n",
    "- The minimizer of $R_2(h)$ is $\\text{Mean}(y_1, y_2, ..., y_n)$:\n",
    "\n",
    "$$\\text{Mean}(y_1, y_2, ..., y_n) = \\underset{h}{\\mathrm{argmin}} \\: R_2(h)$$\n",
    "\n",
    "But what constant prediction $h^*$ minimizes $R_3(h)$, or $R_{10}(h)$, or $R_{10000}(h)$? In this question, we'll explore this idea ‚Äì more specifically, we'll study how $h^*$ changes as $p$ (the exponent on $|y_i - h|$) increases.\n",
    "\n",
    "[Lecture 14](https://practicaldsc.org/resources/lectures/lec14/lec14-filled.pdf#page=38) worked through how to solve for constant prediction $h^*$ that minimized average squared loss (i.e. minimized $R_2(h)$), and we linked to a [video](https://youtu.be/0s7M8OsnBNA?si=lHm6eN3rns7PzPOW) that works through a similar derivation for average absolute loss (i.e. $R_1(h)$). Unfortunately, $p = 1$ and $p = 2$ are the only cases in which we can solve for the minimizer to $R_p(h)$ by hand. \n",
    "\n",
    "For all other values of $p$, there is no closed-form solution (i.e. no \"formula\" for the best constant prediction), and so we need to approximate the solution using the computer. Later in the class, we'll learn how to minimize functions using code we write ourselves (the idea is called gradient descent if you're curious), but for now, we're going to use `scipy.optimize.minimize`, which does the hard work for us.\n",
    "\n",
    "The `minimize` function is a versatile tool from the `scipy` library that can help us find the input that minimizes the output of a function. Let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf53ef",
   "metadata": {},
   "source": [
    "Below, we've defined and plotted a quadratic function. We can see üëÄ that it's minimized when $x = -4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x + 4) ** 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2710fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-20, 20)\n",
    "ys = f(xs)\n",
    "px.line(x=xs, y=ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b440d",
   "metadata": {},
   "source": [
    "But Python doesn't have eyes, so it can't see that the graph is minimized at $x = -4$. `minimize`, though, magically **can** do this minimization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4820e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To call minimize, we have to provide an array of initial \"guesses\"\n",
    "# as to where the minimizing input might be.\n",
    "# For our purposes, using 0 as an initial guess will work fine.\n",
    "minimize(f, x0=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d492fd8",
   "metadata": {},
   "source": [
    "Above, the `x` attribute of the output tells us that the minimizing input to `f` is `-4.0000`, which is what we were able to see ourselves! Cool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a422f",
   "metadata": {},
   "source": [
    "In this question, we'll deal with the following example array of values, `vals`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81773f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 4, 5, 10, 10, 39])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48935fd5",
   "metadata": {},
   "source": [
    "For context, let's see what the distribution of `vals` looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(vals).hist(nbins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3268c746",
   "metadata": {},
   "source": [
    "To reiterate, the constant prediction $h^*$ that minimizes $R_1(h)$ for `vals` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f80027",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe5c82b",
   "metadata": {},
   "source": [
    "And the constant prediction $h^*$ that minimizes $R_2(h)$ for `vals` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38533e27",
   "metadata": {},
   "source": [
    "### Question 5.1 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">0 Points</div>\n",
    "\n",
    "Complete the implementation of the function `h_star`, which takes in a positive integer `p` and an array `vals` and returns the value of the constant prediction $h^*$ that minimizes average $L_p$ loss for `vals`, i.e. the value of $h^*$ that minimizes $R_p(h)$ for `vals`. Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> h_star(1, vals)\n",
    "2.0\n",
    "\n",
    ">>> h_star(2, vals)\n",
    "5.714285345730987\n",
    "```\n",
    "\n",
    "Some guidance:\n",
    "- Your solution should use `minimize`, and will likely involve defining a helper function inside.\n",
    "- It's okay if your example values are slightly different than those above, but they should be roughly the same. (So, it's fine if `h_star(1, vals)` gives you `1.9999999920558864` or something similar.)\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**We're not autograding Question 5.1, and it's not worth any points.** But, you need to do it in order to answer Question 5.2, which is worth points (and which you will answer on paper!).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9d267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def h_star(deg, vals):\n",
    "    ...\n",
    "\n",
    "# Feel free to change this input to make sure your function works correctly.\n",
    "h_star(2, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c446a1",
   "metadata": {},
   "source": [
    "Before proceeding, make sure that the following cells both say `True`, otherwise you did something incorrectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b836805",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(h_star(1, vals), np.median(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b31054",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(h_star(2, vals), np.mean(vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55a20d",
   "metadata": {},
   "source": [
    "Once you have a working implementation of `h_star`, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac3578",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = np.arange(1, 91)\n",
    "hs = [h_star(p, vals) for p in ps]\n",
    "px.line(x=ps, y=hs).update_layout(xaxis_title=r'$p$', yaxis_title=r'$h^* = \\text{minimizer of } R_p(h)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148aa9d3",
   "metadata": {},
   "source": [
    "It seems like as $p$ increases, the value of $h^*$ that minimizes $R_p(h)$ approaches some fixed value. But what is that value? For context, look at `vals` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd4c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eefe67",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5.2 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "\n",
    "Use the plot above to answer the following prompts:\n",
    "\n",
    "1. In the `vals` dataset, as $p$ increases, what does the value of $h^*$ that minimizes $R_p(h)$ approach?\n",
    "1. In any general dataset of values $y_1, y_2, ..., y_n$, as $p$ increases, what does the value of $h^*$ that minimizes $R_p(h)$ approach? Why?\n",
    "\n",
    "Put another way, we're asking you to evaluate the following limit, but using your plot, not calculus (you're welcome üòä):\n",
    "\n",
    "$$\\lim_{p \\rightarrow \\infty} \\left( \\underset{h}{\\mathrm{argmin}} \\frac{1}{n} \\sum_{i = 1}^n |y_i - h|^p \\right)$$\n",
    "\n",
    "Some guidance:\n",
    "- To answer the second prompt, try calling `h_star` with different arrays that you create. Try and see if you can find a pattern in the values that `h_star` returns when `p` is very large.\n",
    "- If you experiment in the way we're suggesting above, you may run into _overflow_ errors, where the numbers you're dealing with are too big for Python to compute. (e.g., something like $|100 - 90|^{2000}$ is far too big to be represented)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f1218",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 6: Algebra, Too üìê\n",
    "\n",
    "---\n",
    "\n",
    "In the coming lectures, we'll start formulating the problem of making predictions about future data given past data in terms of matrices and vectors. Why? The answer is simple: doing so will allow us to build models that use multiple input variables (i.e. features) in order to make predictions.\n",
    "\n",
    "This question serves to review the key linear algebra knowledge you'll need to be familiar with as we start using matrices and vectors in lecture. If any of this feels foreign ‚Äì and it's totally fine if it does! ‚Äì review [LARDS: Linear Algebra Review for Data Science](https://practicaldsc.org/lin-alg/). We'll link to specific sections in LARDS for each part of this question.\n",
    "\n",
    "Throughout this question, consider the following vectors in $\\mathbb{R}^3$, where $\\beta \\in \\mathbb{R}$ is a scalar:\n",
    "\n",
    "$$\n",
    "\\vec{v}_1 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \n",
    "\\vec{v}_2 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\quad \n",
    "\\vec{v}_3 = \\begin{bmatrix} \\beta \\\\ 1 \\\\ 2 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21628b36",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6.1 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "For what value(s) of $\\beta$ are $\\vec{v}_1, \\vec{v}_2,$ and $\\vec{v}_3$ linearly **in**dependent?\n",
    "\n",
    "<small><small>üìï To review, read LARDS [Section 5](https://practicaldsc.org/lin-alg/#linear-independence).</small></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9a65d",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6.2 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">1 Point</div>\n",
    "\n",
    "For what value(s) of $\\beta$ are $\\vec{v}_1$ and $\\vec{v}_3$ orthogonal?\n",
    "\n",
    "<small><small>üìï To review, watch LARDS [Section 2](https://practicaldsc.org/lin-alg/#the-dot-product-angles-and-orthogonality).</small></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cab53",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6.3 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">1 Point</div>\n",
    "\n",
    "For what value(s) of $\\beta$ are $\\vec{v}_2$ and $\\vec{v}_3$ orthogonal?\n",
    "\n",
    "<small><small>üìï To review, watch LARDS [Section 2](https://practicaldsc.org/lin-alg/#the-dot-product-angles-and-orthogonality).</small></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d2211",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6.4 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Regardless of your answers to the previous three parts, in this part, let $\\beta = 3$.\n",
    "\n",
    "Is the vector $\\begin{bmatrix}\n",
    "3 \\\\\n",
    "5 \\\\\n",
    "8\n",
    "\\end{bmatrix}$ in $\\text{span}(\\vec{v}_1, \\vec{v}_2, \\vec{v}_3)$? Why or why not?\n",
    "\n",
    "<small><small>üìï To review, watch LARDS [Section 4](https://practicaldsc.org/lin-alg/#linear-combinations-and-span) and read [Section 5](https://practicaldsc.org/lin-alg/#linear-independence).</small></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9cb54b",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6.5 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "\n",
    "What is the projection of the vector $\\begin{bmatrix}\n",
    "3 \\\\\n",
    "15 \\\\\n",
    "21\n",
    "\\end{bmatrix}$ onto $\\vec{v}_1$?  Give your answer in the form of a vector.\n",
    "\n",
    "<small><small>üìï To review, watch LARDS [Section 6](https://practicaldsc.org/lin-alg/#projecting-onto-a-single-vector).</small></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d14fa7",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 6.6 <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "\n",
    "What is the orthogonal projection of the vector $\\begin{bmatrix}\n",
    "3 \\\\\n",
    "15 \\\\\n",
    "21\n",
    "\\end{bmatrix}$ \n",
    "onto $\\text{span}(\\vec{v}_1, \\vec{v}_2)$?\n",
    "\n",
    "The answer is a vector, $\\vec{z}$, which can be written in the form:\n",
    "\n",
    "$$\\vec{z} = \\lambda_1 \\vec v_1 + \\lambda_2 \\vec v_2$$\n",
    "\n",
    "**Your job** is to find the values of scalars $\\lambda_1$ and $\\lambda_2$, and then, the vector $\\vec z$. As done in LARDS [Section 8](https://practicaldsc.org/lin-alg/#projecting-onto-the-span-of-multiple-vectors-again), one of the intermediate steps in answering this question involves defining a particular matrix $X$ and computing $(X^T X) ^{-1}X^T$.\n",
    "\n",
    "<small><small>üìï To review, watch LARDS [Sections 6-8](https://practicaldsc.org/lin-alg/#projecting-onto-the-span-of-multiple-vectors-again).</small></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78144885",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Finish Line üèÅ\n",
    "\n",
    "Congratulations! You're ready to submit Homework 7.\n",
    "\n",
    "Remember, you'll submit Homework 7 as a **PDF** to the **Homework 7 (PDF)** assignment on Gradescope. You won't submit this notebook anywhere; Homework 7 will be entirely manually graded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
