{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88395c47",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw09.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53f5c3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\">\n",
    "\n",
    "#### Homework 9\n",
    "\n",
    "# Multiple Linear Regression and Feature Engineering\n",
    "\n",
    "### EECS 398-003: Practical Data Science, Fall 2024\n",
    "\n",
    "#### Due Monday, November 11th at 11:59PM (note the later deadline!)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cfd796",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Instructions\n",
    "\n",
    "Welcome to Homework 9! In this homework, you'll gain a better understanding of how the normal equations and multiple regression work, and learn how to create new features for model building, both using `pandas` and `sklearn`. Only content through Lecture 18 is necessary, though parts of Question 3 touch on ideas from Lecture 19. See the [Readings section of the Resources tab on the course website](https://practicaldsc.org/resources/#readings) for supplemental resources.\n",
    "\n",
    "You are given **eight** slip days throughout the semester to extend deadlines. See the [Syllabus](https://practicaldsc.org/syllabus) for more details. With the exception of using slip days, late work will not be accepted unless you have made special arrangements with your instructor.\n",
    "\n",
    "To access this notebook, you'll need to clone our [public GitHub repository](https://github.com/practicaldsc/fa24/). The [‚öôÔ∏è Environment Setup](https://practicaldsc.org/env-setup) page on the course website walks you through the necessary steps.\n",
    "<div class=\"alert alert-warning\" markdown=\"1\">\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "This homework features a mix of autograded programming questions and manually-graded questions.\n",
    "    \n",
    "- Questions 1, 2, and 3.4 are **manually graded**, like in Homework 8, and say **[Written ‚úèÔ∏è]** in the title. For these questions, **do not write your answers in this notebook**! Instead, like in Homework 8, write **all** of your answers to the written questions in this homework in a separate PDF. You can create this PDF either digitally, using your tablet or using [Overleaf + LaTeX](https://overleaf.com) (or some other sort of digital document), or by writing your answers on a piece of paper and scanning them in. Submit this separate PDF to the **Homework 9 (Questions 1, 2, and 3.4; written problems)** assignment on Gradescope, and **make sure to correctly select the pages associated with each question**!\n",
    "\n",
    "- Questions 3, 4, and 5 (except 3.4) are **fully autograded**, and say **[Autograded üíª]** in the title. For these questions, all you need to is write your code in this notebook, run the local `grader.check` tests, and submit to the **Homework 9 (Questions 3-5; autograder problems)** assignment on Gradescope to have your code graded by the hidden autograder. This is the same workflow you followed in Homeworks 1-5 and Homework 8.\n",
    "\n",
    "Your Homework 9 submission time will be the **later** of your two individual submissions.\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "**Make sure to show your work for all written questions! Answers without work shown may not receive full credit.**\n",
    "\n",
    "    \n",
    "This homework is worth a total of **62 points**, 26 of which are manually graded and 36 of which come from the autograder. The number of points each question is worth is listed at the start of each question. **All questions in the assignment are independent, so feel free to move around if you get stuck**, but keep in mind that you'll need to submit this homework twice ‚Äì one submission for your written problems, and one submission for your autograded problems. Tip: if you're using Jupyter Lab, you can see a Table of Contents for the notebook by going to View > Table of Contents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccc3ff",
   "metadata": {},
   "source": [
    "To get started, run the cell below, plus the cell at the top of the notebook that imports and initializes `otter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f06f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# Preferred styles\n",
    "pio.templates[\"pds\"] = go.layout.Template(\n",
    "    layout=dict(\n",
    "        margin=dict(l=30, r=30, t=30, b=30),\n",
    "        autosize=True,\n",
    "        width=600,\n",
    "        height=400,\n",
    "        xaxis=dict(showgrid=True),\n",
    "        yaxis=dict(showgrid=True),\n",
    "        title=dict(x=0.5, xanchor=\"center\"),\n",
    "    )\n",
    ")\n",
    "pio.templates.default = \"simple_white+pds\"\n",
    "\n",
    "# Use plotly as default plotting engine\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f266e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question 1: Sums of Residuals ü§î\n",
    "\n",
    "---\n",
    "\n",
    "In this problem, we will prove that the sum of the residuals of a fit regression model is 0.\n",
    "\n",
    "We define the $i$th **residual** to be the difference between the actual and predicted values for individual $i$ in our dataset, when the predictions are made using a regression model whose coefficients $w_0^*$ and $w_1^*$ (or, for multiple linear regression models, $w_0^*$, $w_1^*$, $w_2^*$, ..., $w_d^*$) are all optimal.\n",
    "\n",
    "In other words, the $i$th residual $e_i$ is: $$e_i = y_i - H^*(x_i)$$\n",
    "\n",
    "We use the letter $e$ for residuals because residuals are also known as errors.\n",
    "\n",
    "We'll get to the proof soon, but first, a warmup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d9ae7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1.1 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "Suppose $\\vec{1} \\in \\mathbb{R}^n$ is a vector containing the value 1 for each element, i.e. $\\vec{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ ... \\\\ 1 \\end{bmatrix}$.\n",
    "\n",
    "For any other vector $\\vec{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ ... \\\\ b_n \\end{bmatrix}$, what is the value of $\\vec{1}^T \\vec{b}$, i.e. what is the dot product of $\\vec{1}$ and $\\vec{b}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675289cb",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1.2 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "Back to the main problem at hand.\n",
    "\n",
    "Consider the typical multiple regression scenario where our hypothesis function has an intercept term:\n",
    "$$H(\\vec{x}) = w_0 + w_1 x^{(1)} + w_2 x^{(2)} + ... + w_d x^{(d)}$$\n",
    "\n",
    "Note that another way of writing the $i$th residual, $e_i = y_i - H^*(x_i)$, is:\n",
    "\n",
    "$$e_i = (\\vec{y} - X \\vec{w}^*)_i$$\n",
    "\n",
    "Here, $X$ is a $n \\times (d + 1)$ design matrix, $\\vec{y} \\in \\mathbb{R}^n$ is an observation vector, and $\\vec{w} \\in \\mathbb{R}^{(d+1)}$ is the parameter vector. We'll use $\\vec{w}^*$ to denote the optimal parameter vector, or the one that satisfies the normal equations. $(\\vec{y} - X \\vec{w}^*)_i$ is referring to element $i$ of the vector $\\vec{y} - X \\vec{w}^*$.\n",
    "\n",
    "Using facts about $\\vec{w}^*$ we learned in Lectures 16 and 17, prove that for multiple linear regression models with an intercept term, the sum of the residuals is 0. That is, prove that:$$\\sum_{i = 1}^n e_i = 0$$\n",
    "\n",
    "*Hint: Refer to the derivation of $\\vec{w*^*}$ in Lecture 16. How did we define $X$? Your proof should not be very long.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e168e4",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1.3 [Written ‚úèÔ∏è]  <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "Now suppose our hypothesis function does not have an intercept term, but is otherwise linear with multiple features: $$H(\\vec{x}) = w_1 x^{(1)} + w_2 x^{(2)} + ... + w_d x^{(d)}$$\n",
    "\n",
    "- Is it still guaranteed that $\\displaystyle\\sum_{i = 1}^n e_i = 0$? Why or why not?\n",
    "- Is it still possible that $\\displaystyle\\sum_{i = 1}^n e_i = 0$? If you believe the answer is yes, come up with a simple example where a linear hypothesis function without an intercept has residuals that sum to 0. If you believe the answer is no, state why not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3be436",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 2: Real Estate üè°\n",
    "\n",
    "---\n",
    "\n",
    "You are given a dataset containing information on recently sold houses in Ann Arbor, including:\n",
    "\n",
    "- square footage\n",
    "- number of bedrooms\n",
    "- number of bathrooms\n",
    "- year the house was built\n",
    "- asking price, or how much the house was originally listed for, before negotiations\n",
    "- sale price, or how much the house actually sold for, after negotiations\n",
    "\n",
    "The table below shows the first few rows of the data set. Note that since you don't have the full dataset, you cannot answer the questions that follow based on calculations; you must answer conceptually.\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>House</th>\n",
    "      <th>Square Feet</th>\n",
    "      <th>Bedrooms</th>\n",
    "      <th>Bathrooms</th>\n",
    "      <th>Year</th>\n",
    "      <th>Asking Price</th>\n",
    "      <th>Sale Price</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>1247</td>\n",
    "      <td>3</td>\n",
    "      <td>3</td>\n",
    "      <td>2005</td>\n",
    "      <td>500,000</td>\n",
    "      <td>494,000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>2</td>\n",
    "      <td>1670</td>\n",
    "      <td>3</td>\n",
    "      <td>2</td>\n",
    "      <td>1927</td>\n",
    "      <td>1,000,000</td>\n",
    "      <td>985,000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>3</td>\n",
    "      <td>716</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "      <td>1993</td>\n",
    "      <td>335,000</td>\n",
    "      <td>333,850</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>4</td>\n",
    "      <td>1600</td>\n",
    "      <td>4</td>\n",
    "      <td>2</td>\n",
    "      <td>1962</td>\n",
    "      <td>830,000</td>\n",
    "      <td>815,000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>5</td>\n",
    "      <td>2635</td>\n",
    "      <td>4</td>\n",
    "      <td>3</td>\n",
    "      <td>1993</td>\n",
    "      <td>1,250,000</td>\n",
    "      <td>1,250,000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>&#8943;</td> <!-- ellipsis -->\n",
    "      <td>&#8943;</td>\n",
    "      <td>&#8943;</td>\n",
    "      <td>&#8943;</td>\n",
    "      <td>&#8943;</td>\n",
    "      <td>&#8943;</td>\n",
    "      <td>&#8943;</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75f90d",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.1 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "First, suppose we fit a multiple linear regression model to predict the sale price of a house given all five of the other variables.  Which feature would you expect to have the largest magnitude weight? Why? (Remember that the weight of a feature is the value of $w^*$ for that feature.)\n",
    "\n",
    "Then, suppose we standardize each variable separately. (Recall, to standardize a column $x_1, x_2, ..., x_n$, we replace each value $x_i$ with $\\frac{x_i - \\bar{x}}{\\sigma_x}$.) Suppose we fit another multiple linear regression model to predict the sale price of a house given all five of the other standardized variables. Now, which feature would you expect to have the largest magnitude weight? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b66cfb6",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.2 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "\n",
    "Suppose we fit a multiple linear regression model to predict the sale price of a house given all five of the other variables in their original, unstandardized form. Suppose the weight for the Year feature is $\\alpha$.\n",
    "\n",
    "Now, suppose we replace Year with a new feature, Age, which is 0 if the house was built in 2024, 1 if the house was built in 2023, 2 if the house was built in 2022, and so on. If we fit a new multiple linear regression model on all five variables, but using Age instead of Year, what will the weight for the Age feature be, in terms of $\\alpha$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab05f84",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.3 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "\n",
    "Now, suppose we fit a multiple linear regression model to predict the sale price of a house given all five of the other features, plus a new sixth feature named $\\text{Rooms}$, which is the total number of bedrooms and bathrooms in the house. Will our new regression model with an added sixth feature make better predictions than the models we fit in Questions 2.1 or 2.2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae80ab1",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.4 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "\n",
    "Now, suppose we fit two multiple linear regression models to predict the sale price of a house.\n",
    "\n",
    "The first uses the features $\\text{Square Feet}$ and $\\text{Bathrooms}$:\n",
    "\n",
    "$$H_\\gamma(\\vec x) = \\gamma_0 + \\gamma_1 \\cdot \\text{Square Feet} +\\gamma_2 \\cdot \\text{Bathrooms}$$\n",
    "\n",
    "The second model uses the  features $\\text{Square Feet}$ and $\\text{Bathrooms}$ and a new seventh feature named $\\text{Length of Street Name}$, which is the number of letters in the name of the street that the house is on:\n",
    "\n",
    "$$H_\\lambda(\\vec x) = \\lambda_0 + \\lambda_1 \\cdot \\text{Square Feet} +\\lambda_2 \\cdot \\text{Bathrooms} + \\lambda_3 \\cdot \\text{Length of Street Name}$$\n",
    "\n",
    "Let $\\text{TMSE}$ refer to the \"training\" mean squared error, that is, the mean squared error of a hypothesis function **on the same dataset we used to fit it**. (Through Lecture 18, we just referred to this idea as MSE.)\n",
    "\n",
    "Argue why $\\text{TMSE}(H_\\lambda) \\leq \\text{TMSE}(H_\\gamma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3554e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 3: Play Ball ‚öæÔ∏è\n",
    "\n",
    "---\n",
    "\n",
    "In this question, you'll get a feel for the process of creating new features from existing ones and how to _think_ about model generalizability, an idea we'll see more in Lecture 19.\n",
    "\n",
    "<br>\n",
    "\n",
    "As we discussed in Lecture 18, a numerical-to-numerical transformation results from taking the values in some numerical column $x_1, x_2, ..., x_n$ and applying some function $f$ to each value, to produce a new set of numbers $f(x_1), f(x_2), ..., f(x_n)$. These **transformed** values, $f(x_1), f(x_2), ..., f(x_n)$, can then either be used as a feature, or as the target ($y$) variable.\n",
    "\n",
    "A common goal of applying a numerical-to-numerical transformation is to modify the data from a complicated, non-linear relationship into a **linear** relationship. Linear relationships are easy to understand and are well-described using linear models.\n",
    "\n",
    "However, non-linear growth is common in real-world datasets. Sometimes this growth is by a **fixed power** and sometimes it is **exponential**. The transformation functions, $f$, that turn these types of growth linear are **root** and **log** transformations respectively. (Generally, it is more difficult to determine which transformation is appropriate for a given dataset, though the [Tukey-Mosteller bulge diagram](https://freakonometrics.hypotheses.org/files/2014/06/Selection_005.png) from Lectures 17 and 18 is useful.)\n",
    "\n",
    "Let's start by looking at some examples of transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48dec5b",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Run the cell below to generate a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e546d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting a seed, we guarantee that we will see the same results each time we run this cell.\n",
    "np.random.seed(23)\n",
    "\n",
    "# Generates a random scatter plot\n",
    "x = np.arange(1, 101) + np.random.normal(0, 0.5, 100)\n",
    "y = 2 * ((x + np.random.normal(0, 1, 100)) ** 2) + np.abs(x) * np.random.normal(0, 30, 100)\n",
    "df_1 = pd.DataFrame().assign(x=x, y=y)\n",
    "\n",
    "px.scatter(df_1, x='x', y='y', trendline=\"ols\", trendline_color_override=\"#ff7f0e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115a96d",
   "metadata": {},
   "source": [
    "It doesn't appear to be the case that `'x'` and `'y'` are linearly associated here, and they aren't ‚Äì there is a **quadratic** relationship between them. \n",
    "\n",
    "One way we may be able to notice this is a **residual plot**, where we visualize the residuals (or errors), $e = y_i - H^*(x_i)$, as defined in Question 1. Note that if we were to create a **residual plot** based on the data above, there would be a pattern ‚Äì the residuals for smaller `'x'` would mostly be positive, and the residuals for larger `'x'` would mostly be negative. Patterns in a residual plot imply that the relationship between the two variables is non-linear.\n",
    "\n",
    "Let's take a look at the residual plot, using a helper function defined below. This function fits a `LinearRegression` model to `'x'` and `'y'`, adds a `'residuals'` column to the `df`, and plots that against the predictions `'pred'`. Note that it's equally valid to plot the residuals against `'x'`: to do that, change `x = 'pred'` to `x = x` in the call to `px.scatter` below. You'll see the trend is the same, but the x-axis will have different numbers. That's because `'pred'` is just a linear transformation of `'x'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to use this function directly to help you answer Question 1.\n",
    "def create_residual_plot(df, x, y):\n",
    "    df = df.copy()\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    model = LinearRegression()\n",
    "    model.fit(df[[x]], df[y])\n",
    "    df['pred'] = model.predict(df[[x]])\n",
    "    df[f'{y} residuals'] = df[y] - model.predict(df[[x]])\n",
    "    return px.scatter(df, x='pred', y=f'{y} residuals', trendline='ols', trendline_color_override='red')\n",
    "\n",
    "create_residual_plot(df_1, 'x', 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfa6d6",
   "metadata": {},
   "source": [
    "To linearize the relationship, we can take the square root of each `'y'` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b20f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['root y'] = np.sqrt(df_1['y'])\n",
    "\n",
    "px.scatter(df_1, x='x', y='root y', trendline=\"ols\", trendline_color_override=\"#ff7f0e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0c681",
   "metadata": {},
   "source": [
    "That looks much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b9fbb0",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Run the cell below to generate another scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db1914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting a seed, we guarantee that we will see the same results each time we run this cell\n",
    "np.random.seed(32)\n",
    "\n",
    "# Generates a different random scatter plot\n",
    "x = np.linspace(2, 5, 100)\n",
    "y = 10 * (np.e ** x) + np.abs(x) * np.random.normal(0, 5, 100) + np.random.normal(0, 30, 100)\n",
    "df_2 = pd.DataFrame().assign(x=x, y=y)\n",
    "\n",
    "px.scatter(df_2, x='x', y='y', trendline=\"ols\", trendline_color_override=\"#ff7f0e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0160004",
   "metadata": {},
   "source": [
    "Again, the relationship between `'x'` and `'y'` is not quite linear. Let's try the square root transformation we tried in Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d4c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['root y'] = np.sqrt(df_2['y'])\n",
    "\n",
    "px.scatter(df_2, x='x', y='root y', trendline=\"ols\", trendline_color_override=\"#ff7f0e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed68b7",
   "metadata": {},
   "source": [
    "Hmm... the relationship certainly looks _more_ linear than before, but still not quite linear. Let's look at the residual plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_residual_plot(df_2, 'x', 'root y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed636c1f",
   "metadata": {},
   "source": [
    "There is clearly a pattern in the residual plot. Let's instead try another transformation for the `'y'` values ‚Äì $\\log$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['log y'] = np.log(df_2['y'])\n",
    "\n",
    "px.scatter(df_2, x='x', y='log y', trendline=\"ols\", trendline_color_override=\"#ff7f0e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343eb3bf",
   "metadata": {},
   "source": [
    "That looks much better! We can verify that the residual plot has no \"patterns\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3524e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_residual_plot(df_2, 'x', 'log y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc65841",
   "metadata": {},
   "source": [
    "Note ‚Äì there is still evidence of **heteroscedasticity**, or \"uneven spread\", in this scatter plot, but the relationship is as close to linear as we'll get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94254929",
   "metadata": {},
   "source": [
    "Now that we've learned how to perform transformations with example datasets, it's your job to apply these ideas to a real dataset. Below, we load in a dataset that describes the [number of home runs in the MLB per year](https://www.mlb.com/glossary/standard-stats/home-run). The relationship between the two variables, `'Year'` and `'Homeruns'`, is not linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeruns = pd.read_csv('data/homeruns.csv')\n",
    "homeruns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c68f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeruns.plot(kind='scatter', x='Year', y='Homeruns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898ed2c",
   "metadata": {},
   "source": [
    "**Throughout this entire question**, suppose we're modeling `'Homeruns'` as a function of `'Year'`, i.e. `'Homeruns'` is the $y$ variable and `'Year'` is the $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c766f67d",
   "metadata": {},
   "source": [
    "### Question 3.1 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "\n",
    "**Your first job is to determine what the appropriate transformation to apply to the `'Homeruns'` column is, in order to linearize the relationship.** Specifically, try out the transformations below, and then draw and examine residual plots to identify which numerical-to-numerical transformation is best.\n",
    "\n",
    "While you'll have to write a bunch of code, this is a multiple-choice question. Assign `best_transformation` to either 1, 2, 3, or 4, with the value corresponding to one of the following choices:\n",
    "\n",
    "1. Square root transformation.\n",
    "2. Log transformation.\n",
    "3. Both work the same.\n",
    "4. Neither gives a transformation revealing a linear relationship.\n",
    "\n",
    "If you find that both residual plots have some sort of pattern, choose the residual plot in which the vertical spread is constant. There is one clearly correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95460c34",
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "best_transformation = ...\n",
    "best_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b728b3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q03_01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9b38d",
   "metadata": {},
   "source": [
    "### Question 3.2 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">5 Points</div>\n",
    "\n",
    "Recall, our goal in this question is to model `'Homeruns'` as a function of `'Year'`. In the previous part, we had you apply a numerical-to-numerical transformation to `'Homeruns'`, which is the $y$ variable.\n",
    "\n",
    "In this part, you'll be required to engineer new quantitative features **of your own choosing**, all based on transformations of the $x$ variable, `'Year'`.\n",
    "\n",
    "Complete the implementation of the function `fit_model_and_return_predictions`, which takes in:\n",
    "- `X`, a DataFrame with a single column of `'Year'` values from `homeruns`, and\n",
    "- `y`, an array or Series with a sequence of `'Homerun'` values from `homeruns`.\n",
    "\n",
    "`fit_model_and_return_predictions` should:\n",
    "- Create new numerical features by applying various transformations to the values in `X['Year']` (look at the \"Polynomial regression\" example from Lecture 18 for inspiration ‚Äì you don't need to use the `apply` method to apply a transformation),\n",
    "- Fit a `sklearn` LinearRegression object using your custom design matrix as the `X` argument and our passed-in `y` as the `y` argument, and\n",
    "- **Return an array of predictions** that result from calling the `predict` method on the fit model, using your custom design matrix as the `X` argument.\n",
    "\n",
    "For example, suppose our `fit_model_and_return_predictions` function creates polynomial features of degrees 2 through 10, and adds no other new features. Example behavior of `fit_model_and_return_predictions` may then be as follows:\n",
    "\n",
    "```python\n",
    ">>> fit_model_and_return_predictions(homeruns[['Year']], homeruns['Homeruns'])[:5]\n",
    "array([165.07808666, 300.52105073, 174.28363771, 288.87689757, 395.065479  ])\n",
    "```\n",
    "\n",
    "A plot of the predictions returned by `fit_model_and_return_predictions` might then look like:\n",
    "\n",
    "<center><img src=\"imgs/fit-model.png\" width=500></center>\n",
    "\n",
    "Is this a \"good\" model? Sure, it has a low training MSE, but it's not likely to generalize well to unseen $x$-values ‚Äì in this case, future `'Year'`.\n",
    "\n",
    "**You can create your features however you'd like!** Don't just use our example of using polynomial features of degrees 2 to 10. Try, intuitively, to come up with a fit hypothesis function that _you think_ is likely to generalize well to future `'Year'`s for whom we don't know the number of `'Homeruns'`. We will formalize how to develop models that generalize well in the coming lectures.\n",
    "\n",
    "All we can autograde in this question are your resulting predictions ‚Äì practically, we have no way of knowing how you come up with them. Other than what's described above, here are the only added requirements of your function:\n",
    "\n",
    "- It should be able to take in a **subset** of the rows in `homeruns`, and should do all calculations (feature creation, fitting, predicting) using that subset. So, this should work too:\n",
    "    ```python\n",
    "        >>> fit_model_and_return_predictions(homeruns.head()[['Year']], homeruns.head()['Homeruns'])\n",
    "        \n",
    "    ```\n",
    "    Note that in `fit_model_and_return_predictions`, the `X` data used to fit the model is always the same as the data used to make predictions. In other cases, this is not necessarily how it works ‚Äì after all, we typically build models using historical data and use them to make predictions about future data ‚Äì but this is how we'll use and test `fit_model_and_return_predictions`.\n",
    "\n",
    "- The array that `fit_model_and_return_predictions` returns should be **deterministic**. That is, if `fit_model_and_return_predictions` is called twice with the exact same inputs `X` and `y`, the output should not change.\n",
    "- The mean squared error of the predictions, when called on `X=homeruns[['Year']]` and `y=homeruns['Homeruns']`, should be **between 100,000 and 200,000**. Yes, it's possible to achieve a mean squared error of less than 100,000, but such a model is likely **overfitting** significantly to the data. (In fact, in Homework 8, you learned how to build models with 0 MSE, using Lagrange Interpolation! **Don't do that here ‚Äì try and build more general-purpose models.**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d3271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def fit_model_and_return_predictions(X, y):\n",
    "    X = X.copy()\n",
    "    # Below, create your features and design matrix,\n",
    "    # instantiate a LinearRegression object,\n",
    "    # fit it, and then call model.predict on it.\n",
    "    ...\n",
    "\n",
    "# Feel free to change this input to make sure your function works correctly.\n",
    "preds = fit_model_and_return_predictions(homeruns[['Year']], homeruns['Homeruns'])\n",
    "\n",
    "# Uncomment the code below to see a graph of your\n",
    "# fit hypothesis function's predictions.\n",
    "# fig = homeruns.plot(kind='scatter', x='Year', y='Homeruns')\n",
    "\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=homeruns['Year'],\n",
    "#     y=preds,\n",
    "#     mode='lines',\n",
    "#     line=dict(width=4),\n",
    "#     name='Fit Model'\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae995c4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q03_02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a7f59",
   "metadata": {},
   "source": [
    "### Question 3.3 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">5 Points</div>\n",
    "\n",
    "Above, you had to manually create features that resulted in a hypothesis function that fit the data well (but not too well). You may wonder, is there a way to do this automatically?\n",
    "\n",
    "One _kind-of_ solution is to use **Nearest Neighbors Regression**. In nearest neighbors regression, to evaluate the hypothesis function $H^*$ on the input $x_\\text{new}$:\n",
    "\n",
    "1. First, choose a value of $k$. Sometimes, this is called **$k$-Nearest Neighbors Regression, or $k$-NN Regression**.\n",
    "1. Then:\n",
    "    1. Find the $k$ points in the original dataset whose $x$-values are closest to $x_\\text{new}$ in terms of absolute value (note that since we're essentially dealing with just a single $x$ feature, using squared distance would also work the same way).\n",
    "    1. Return the mean of the $y$-values corresponding to the $k$ points found in the step above.\n",
    "\n",
    "For example, suppose our original dataset is:\n",
    "\n",
    "| x | y |\n",
    "| --- | --- |\n",
    "| 10 | 5 |\n",
    "| 11 | 17 |\n",
    "| 12 | 26 |\n",
    "| 19 | -5 |\n",
    "| 25 | 3 |\n",
    "\n",
    "Suppose we choose $k = 3$, and suppose we want to predict the $y$-value for $x_\\text{new} = 20$. Then:\n",
    "- The $k = 3$ points with the closest $x$-values are $(19, -5)$, $(25, 3)$, and $(12, 26)$.\n",
    "- The mean of the $y$-values of the three points above is $\\frac{-5 + 3 + 26}{3} = 8$.\n",
    "- So, we predict a $y$-value of 8 for input $x_\\text{new} = 20$.\n",
    "\n",
    "This is a regression technique, because it allows us to predict real-valued outputs. However, it is different from linear regression in that it is **non-parametric** ‚Äì there are no **parameters** $w_0^*, w_1^*, ...$ that we're solving for in order to make our predictions.\n",
    "\n",
    "We can choose $k$ to be whatever we want it to be, but some values of $k$ are \"better\" in some sense than others. We'll explore this idea in Question 3.4, when we tie things back into the `homeruns` dataset.\n",
    "\n",
    "**Your job is to** complete the implementation of the function `create_knn_regressor`, which takes in:\n",
    "- `x`, a 1D array/Series of $x$-values, e.g. `homeruns['Year']`,\n",
    "- `y`, a 1D array/Series of $y$-values, e.g. `homeruns['Homeruns']`, and\n",
    "- `k`, a positive integer corresponding to the value of $k$ (where `k <= len(x)`).\n",
    "\n",
    "`create_knn_regressor` should return a **function** that can take in a single number `x_new` and return the predicted $y$-value for the input `x_new`, according to the process outlined above.\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">> regressor = create_knn_regressor(x=np.array([10, 11, 12, 19, 25]),\n",
    "                                    y=np.array([5, 17, 26, -5, 3]),\n",
    "                                    k=3)\n",
    ">>> regressor(20)\n",
    "8.0\n",
    "```\n",
    "\n",
    "Some guidance:\n",
    "- The bulk of the work in this question is in understanding how Nearest Neighbors Regression works. Our implementation is very short (5 lines total).\n",
    "- **You're not allowed to use `sklearn` here**, but don't forget to use what you know about `pandas` DataFrames! You shouldn't use a `for`-loop.\n",
    "- Don't worry about cases in which there are ties in distance (e.g. if $k = 3$ but there are 4 points that are all equidistant from $x_\\text{new}$ such that they are all the closest); our tests are written in a way that will not penalize your handling of this situation if it's different from ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c61ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_knn_regressor(x, y, k):\n",
    "    ...\n",
    "\n",
    "# Feel free to change these inputs to make sure your function works correctly.\n",
    "# It's a good idea to test out create_knn_regressor on the homeruns dataset!\n",
    "regressor = create_knn_regressor(x=np.array([10, 11, 12, 19, 25]),\n",
    "                                 y=np.array([5, 17, 26, -5, 3]),\n",
    "                                 k=3)\n",
    "regressor(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032512a9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q03_03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d376f4",
   "metadata": {},
   "source": [
    "Once you've implemented `create_knn_regressor`, run the cell below to see an **interactive** widget that will allow you to choose different values of $k$ and see the resulting $k$-NN regressor plotted on top of the `homeruns` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def plot_given_k(k):\n",
    "    x = homeruns['Year']\n",
    "    y = homeruns['Homeruns']\n",
    "    regressor = create_knn_regressor(x, y, k)\n",
    "    preds = [regressor(xi) for xi in x]\n",
    "\n",
    "    fig = px.scatter(x=x, y=y).update_layout(xaxis_title='Year', yaxis_title='Homeruns', title=f'Fit kNN Model with k={k}')\n",
    "\n",
    "    return fig.add_trace(go.Scatter(\n",
    "        x=x,\n",
    "        y=preds,\n",
    "        mode='lines',\n",
    "        line=dict(width=4),\n",
    "        name='Fit Model'\n",
    "    ))\n",
    "\n",
    "widgets.interact(plot_given_k, k=widgets.IntSlider(min=1, max=140, step=1, value=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5c6e3",
   "metadata": {},
   "source": [
    "Try different values of $k$ ‚Äì what do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b35959",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3.4 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "\n",
    "Play around with the interactive cell above. Then, comment on the following points **in your PDF writeup, along with your answers to the rest of the written problems in this homework (that is, _not_ in this notebook)**:\n",
    "1. When $k = 1$, what does the resulting fit model look like, and how does it relate to models we've seen in earlier lectures/homeworks?\n",
    "2. When $k = 140$, what does the resulting fit model look like, and how does it relate to models we've seen in earlier lectures/homeworks?\n",
    "3. Which value of $k$ do you _feel_ best captures the trend in the data, and why? (Just give a one sentence intuitive answer ‚Äì no calculations needed.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6aa416",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 4: Diamond Pricing üíé\n",
    "\n",
    "---\n",
    "\n",
    "In this next section, you will pretend you are a jewelry appraiser and predict the prices of diamonds given several standard characteristics of diamonds.\n",
    "\n",
    "You will use linear regression to predict prices, while improving the quality of your predictions using **feature engineering**. Since this question is supposed to help you understand feature engineering, **you will be building these features from scratch, instead of using built-in `sklearn` methods**.\n",
    "\n",
    "The `diamonds` dataset is accessible via `seaborn` (with `sns.load_dataset('diamonds')`), but we've skipped that step and loaded it for you below. The DataFrame has 53940 rows and 10 columns:\n",
    "\n",
    "|column|description|unique values or range|\n",
    "|---|---|---|\n",
    "|`'carat'`|weight of the diamond in carats (each carat is 0.2 grams)| 0.2 - 5.01 |\n",
    "|`'cut'`|quality of the cut | Fair, Good, Very Good, Premium, Ideal |\n",
    "|`'color'`|diamond colour | J (worst, near colorless), I, H, G, F, E, D (best, absolute colorless) |\n",
    "|`'clarity'`|a measurement of how clear the diamond is | I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best) |\n",
    "|`'depth'`|total depth percentage, computed as z / mean(x, y) = 2 * z / (x + y) | 43 - 79 |\n",
    "|`'table'`|width of top of diamond relative to widest point | 43 - 95 |\n",
    "|`'price'`|price in US dollars | \\\\$326 - \\\\$18,823 USD |\n",
    "|`'x'`|length in mm | 0 - 10.74 |\n",
    "|`'y'`|width in mm | 0 - 58.9 | \n",
    "|`'z'`|depth in mm | 0 - 31.8 |\n",
    "\n",
    "If you want to learn more about how diamonds are measured, refer to [this page by the American Gem Society](https://www.americangemsociety.org/4cs-of-diamonds/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds = pd.read_csv('data/diamonds.csv')\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982ecbc",
   "metadata": {},
   "source": [
    "### Question 4.1 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "\n",
    "Every categorical variable in the dataset is an ordinal column, meaning that there is an inherent order that we can use to sort the values in the column. An **ordinal encoding** is a feature transformation that maps the values in an ordinal column to positive integers in a way that preserves the order of the column values. For instance, an ordinal encoding for Freshman, Sophomore, Junior, Senior is 0, 1, 2, 3.\n",
    "\n",
    "Complete the implementation of the function `create_ordinal`, which takes in a DataFrame `df` like `diamonds` and returns a DataFrame of ordinal features only with names of the form `'ordinal_<col>'`, where `'<col>'` is the original categorical column name. For instance, the `'ordinal_color'` column should consist of values from 0 to 6, where 0 refers to `'J'` and 6 refers to `'D'`. **In all cases, start counting from 0.**\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> create_ordinal(diamonds.head(5))\n",
    "```\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>ordinal_cut</th>\n",
    "      <th>ordinal_clarity</th>\n",
    "      <th>ordinal_color</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>4</td>\n",
    "      <td>1</td>\n",
    "      <td>5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>3</td>\n",
    "      <td>2</td>\n",
    "      <td>5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>1</td>\n",
    "      <td>4</td>\n",
    "      <td>5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>3</td>\n",
    "      <td>3</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Some guidance:\n",
    "- Remember, you're only permitted to use `pandas` operations. You might want to create a helper function that takes in a single column and an ordering for that column.\n",
    "- Don't include non-ordinal features in the returned DataFrame. That is, if there are only three columns in `diamonds` that are ordinal, `create_ordinal` should return a DataFrame with three columns.\n",
    "- The orderings for each of the ordinal columns are displayed in the data dictionary above (in the `'unique values or range'` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f8420",
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "def create_ordinal(df):\n",
    "    ...\n",
    "    \n",
    "# Feel free to change this input to make sure your function works correctly.\n",
    "create_ordinal(diamonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6139c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q04_01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d08fd6",
   "metadata": {},
   "source": [
    "### Question 4.2 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "\n",
    "Even though the categorical variables in the dataset are ordinal, we can still treat them as nominal by forgetting their order. To treat the categorical variables in our dataset as nominal, we might **one hot encode** them. \n",
    "\n",
    "Complete the implementation of the function `create_one_hot`, which takes in a DataFrame `df` like `diamonds` and returns a DataFrame of one hot encoded features with names of the form `'one_hot_<col>_<val>'`, where `'<col>'` is the original categorical column name, and `'<val>'` is the value found in the categorical column `'<col>'`. For instance, one of your column names will be `'one_hot_color_J'`.\n",
    "\n",
    "Example behavior is given below, for a subset of features.\n",
    "\n",
    "```python\n",
    ">>> out = create_one_hot(diamonds)\n",
    ">>> out.loc[:5, out.columns.str.contains('cut')]\n",
    "```\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>one_hot_cut_Ideal</th>\n",
    "      <th>one_hot_cut_Premium</th>\n",
    "      <th>one_hot_cut_Good</th>\n",
    "      <th>one_hot_cut_Very Good</th>\n",
    "      <th>one_hot_cut_Fair</th>\n",
    "      <th>one_hot_color_E</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Some guidance:\n",
    "- Only include one-hot-encoded columns in the DataFrame that `create_one_hot` returns.\n",
    "- Create a helper function that creates the one-hot encoding for a single column. **Do not** use `sklearn` or `pd.get_dummies` for this question!\n",
    "- As per usual, write an efficient implementation. You may use a `for`-loop over **columns**, but not over rows. And the order of **columns** does not matter.\n",
    "- In lecture, we discussed the fact that for statistical reasons, we often drop a single one hot encoded column per original categorical feature. **Do not drop** any one hot encoded columns here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2aa1e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_one_hot(df):\n",
    "    ...\n",
    "    \n",
    "# Feel free to change this input to make sure your function works correctly.\n",
    "create_one_hot(diamonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a34a3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q04_02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41ff70",
   "metadata": {},
   "source": [
    "### Question 4.3 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "\n",
    "Similar to the one hot encoding case, you can replace a value in a nominal column with the proportion of times that value appears in the column. For instance, if a column consists of the values `['a', 'b', 'a', 'c']`, then the proportion-encoded column is `[0.5, 0.25, 0.5, 0.25]`.  This might be a reasonable approach to predicting the price of a diamond, as you might expect **rarer attributes to be considered more valuable** than common ones.\n",
    "\n",
    "Complete the implementation of the function `create_proportions`, which takes in a DataFrame `df` like `diamonds` and returns a DataFrame of proportion-encoded features with names of the form `'proportion_<col>'`, where `'<col>'` is the original categorical column name.\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> create_proportions(diamonds).head(5)\n",
    "```\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>proportion_cut</th>\n",
    "      <th>proportion_color</th>\n",
    "      <th>proportion_clarity</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>0.399537</td>\n",
    "      <td>0.181628</td>\n",
    "      <td>0.170449</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>0.255673</td>\n",
    "      <td>0.181628</td>\n",
    "      <td>0.242214</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>0.090953</td>\n",
    "      <td>0.181628</td>\n",
    "      <td>0.151483</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>0.255673</td>\n",
    "      <td>0.100519</td>\n",
    "      <td>0.227253</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>0.090953</td>\n",
    "      <td>0.052058</td>\n",
    "      <td>0.170449</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bfadbb",
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "def create_proportions(df):\n",
    "    ...\n",
    "    \n",
    "# Feel free to change this input to make sure your function works correctly.\n",
    "create_proportions(diamonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcae17",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q04_03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1b535",
   "metadata": {},
   "source": [
    "### Question 4.4 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "\n",
    "As we looked at in-depth in Question 3, linear regression doesn't capture non-linear relationships between variables, but you can create features that encode such dependencies **before** fitting your regression model, and creating polynomial features is one way to do this.\n",
    "\n",
    "For instance, the diamonds dataset contains `'x'`, `'y'`, and `'z'` dimensions for each stone. However, different combinations of size may be more valuable than others: a \"deep and wide\" diamond might be considered more valuable than a shallow, but \"long and wide\" diamond.\n",
    "\n",
    "Complete the implementation of the function `create_quadratics`, which takes in a DataFrame `df` like `diamonds` DataFrame and returns a DataFrame of quadratic features of the form `'<col1> * <col2>'`, where `'<col1>'` and `'<col2>'` are the original quantitative columns.\n",
    "- The output DataFrame should contain a column for every distinct pair of quantitative columns in `df` (aside from `'price'`, which should be left out as it is what we are predicting).\n",
    "- For instance, one of the columns in the returned DataFrame should named either `'carat * x'` or `'x * carat'`; the order of column names is not important.\n",
    "\n",
    "Example behavior is given below. \n",
    "\n",
    "\n",
    "```python\n",
    ">>> out = create_quadratics(diamonds)\n",
    ">>> out.loc[:5, out.columns.str.contains('carat')]\n",
    "```\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>carat * depth</th>\n",
    "      <th>carat * table</th>\n",
    "      <th>carat * x</th>\n",
    "      <th>carat * y</th>\n",
    "      <th>carat * z</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>14.145</td>\n",
    "      <td>12.65</td>\n",
    "      <td>0.9085</td>\n",
    "      <td>0.9154</td>\n",
    "      <td>0.5589</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>12.558</td>\n",
    "      <td>12.81</td>\n",
    "      <td>0.8169</td>\n",
    "      <td>0.8064</td>\n",
    "      <td>0.4851</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>13.087</td>\n",
    "      <td>14.95</td>\n",
    "      <td>0.9315</td>\n",
    "      <td>0.9361</td>\n",
    "      <td>0.5313</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>18.096</td>\n",
    "      <td>16.82</td>\n",
    "      <td>1.2180</td>\n",
    "      <td>1.2267</td>\n",
    "      <td>0.7627</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>19.623</td>\n",
    "      <td>17.98</td>\n",
    "      <td>1.3454</td>\n",
    "      <td>1.3485</td>\n",
    "      <td>0.8525</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>15.072</td>\n",
    "      <td>13.68</td>\n",
    "      <td>0.9456</td>\n",
    "      <td>0.9504</td>\n",
    "      <td>0.5952</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Some guidance:\n",
    "- Again, **do not** use `sklearn` for this question! \n",
    "- Try finding all pairs of quantitative columns efficiently; don't use a nested loop (hint: think back to `SimpleLAD` from Homework 8). Our solution contains just a single `for`-loop, over pairs of columns.\n",
    "- The columns of the resulting DataFrame may be in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e48ccf",
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "def create_quadratics(df):\n",
    "    ...\n",
    "    \n",
    "# Feel free to change this input to make sure your function works correctly.\n",
    "create_quadratics(diamonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972277e1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q04_04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6b438",
   "metadata": {},
   "source": [
    "This homework is already quite long, and the focus of Question 4 was on having you develop the features yourself, not necesssarily use them in prediction tasks. So, we won't rqequire you to _fit_ any models using the features you've created. That said, you **should** try and experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc8f63f",
   "metadata": {},
   "source": [
    "## Question 5: Feature Engineering with `sklearn` üß†\n",
    "\n",
    "---\n",
    "\n",
    "In this final question, you will use `sklearn`'s transformers and estimators for feature engineering. While everything you do with `sklearn` is possible to do with `pandas`, `sklearn` transformers enable you to couple your feature engineering with your modeling. This will allow you to more quickly build and assess your models in `sklearn`.\n",
    "\n",
    "Specifically, you will implement a `TransformDiamonds` class that has the three methods described below. In the starter code, there is a skeleton for `TransformDiamonds`, complete with an initializer.\n",
    "\n",
    "Each of the methods you implement in the `TransformDiamonds` class should take in a DataFrame, initialize a specific `sklearn.Transformer` object (like `Binarizer` or `FunctionTransformer`), and use the transformer to transform columns from the input DataFrame. You should **not** use DataFrame methods like `apply` in this problem.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1. `transform_carat` <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div> [Autograded üíª]\n",
    "\n",
    "We call a diamond **large** if its weight is strictly greater than 1 carat. We want to **binarize** weights, so that they are 1 for large diamonds and 0 for small diamonds. This methd takes in a DataFrame `df` like `diamonds` and returns a binarized **array** of weights. Use a `Binarizer` object as your transformer.\n",
    "\n",
    "Additional guidance:\n",
    "- `transform_carat` should return an array, not a Series, because `sklearn` thinks in terms of `np.ndarray`s, not DataFrames.\n",
    "- The implementation of `transform_carat` should only take two lines.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2. `transform_to_quantile` <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div> [Autograded üíª]\n",
    "\n",
    "Here, transform the `'carat'` column so that each diamond's weight in carats is replaced with the **percentile** amongst all diamonds in which its weight lies. This method takes in a DataFrame `df` like `diamonds` and returns an array containing the percentiles of the weight of each diamond, amongst all diamonds in `self.data`. This array should consist of proportions between 0 and 1; for instance, 0.65 will refer to the 65th percentile. The relevant transformer is `QuantileTransformer`. \n",
    "\n",
    "Additional guidance:\n",
    "\n",
    "- Unlike with `Binarizer`, you need to `fit` your `QuantileTransformer` before calling `transform` on the input DataFrame `data`. \n",
    "    - You should `fit` your transformer on the DataFrame `self.data`, but you should only `transform` the `data` that is passed to `transform_to_quantiles`. \n",
    "    - Note that these two DataFrames, `self.data` and `data`, don't have to be the same! For instance, if we fit a `QuantileTransformer` using just the first 1000 rows of `diamonds`, and then `transform` the entire `diamonds` DataFrame, your `transform_to_quantiles` method should still work.\n",
    "- When initializing your `QuantileTransformer`, use `n_quantiles=100` and `random_state=98`. The `random_state` argument **is necessary** because `QuantileTransformer` is non-deterministic, meaning that it potentially outputs different results each time it's called on the same output. Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html) for more details, and **don't forget this step!**\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3. `transform_to_depth_pct` <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div> [Autograded üíª]\n",
    "\n",
    "Recall from Question 4 that the \"depth percentage\" of a diamond is defined as:\n",
    "$$\\text{Depth Pct.} = 100\\% \\cdot \\frac{2z}{x + y}$$\n",
    "where $x$, $y$, and $z$ come from the `'x'`, `'y'`, and `'z'` columns in `diamonds`.\n",
    "\n",
    "Let's suppose that for some reason we don't have access to the `'depth'` column in `diamonds`, and instead need to recreate it just by looking at the `'x'`, `'y'`, and `'z'` columns. \n",
    "\n",
    "This method takes in a DataFrame `df` like `diamonds` and returns an array consisting of the depth percentages of each diamond. Percentages should be between 0 and 100. The relevant transformer is `FunctionTransformer`.\n",
    "\n",
    "Additional guidance:\n",
    "- To use `FunctionTransformer`, you will need to define your own function that takes in a 2D array and returns a single array.\n",
    "- Ignore `ZeroDivisionError` errors, and leave `np.NaN`s as is.\n",
    "- To verify your work, compare your outputted array to the actual `'depth'` column in `diamonds`. Most ‚Äì but not all ‚Äì of the values should be the same.\n",
    "- It may seem like `FunctionTransformer` is totally unnecessary, since we can compute depth percentages using broadcasting directly. However, as we will see in lecture, transformers can be **pipelined** with other processing steps which greatly simplifies our code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7439c3e",
   "metadata": {},
   "source": [
    "The three test cells at the bottom of this section will test each method independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a72690",
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer, QuantileTransformer, FunctionTransformer\n",
    "\n",
    "class TransformDiamonds(object):\n",
    "    \n",
    "    def __init__(self, diamonds):\n",
    "        self.data = diamonds\n",
    "        \n",
    "    def transform_carat(self, df):\n",
    "        ...\n",
    "    \n",
    "    def transform_to_quantile(self, df):\n",
    "        # Don't forget to use random_state=98!\n",
    "        ...\n",
    "    \n",
    "    def transform_to_depth_pct(self, df):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2ad3e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q05_test_transform_carat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea57509",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q05_transform_to_quantile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39293817",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q05_transform_to_depth_pct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4036ef",
   "metadata": {},
   "source": [
    "## Finish Line üèÅ\n",
    "\n",
    "Congratulations! You're ready to submit Homework 9. **Remember, you need to submit Homework 9 twice**:\n",
    "\n",
    "### To submit the manually graded problems (Questions 1, 2, and 3.4; marked [Written ‚úèÔ∏è])\n",
    "\n",
    "- Make sure your answers **are not** in this notebook, but rather in a separate PDF.\n",
    "    - You can create this PDF either digitally, using your tablet or using [Overleaf + LaTeX](https://overleaf.com) (or some other sort of digital document), or by writing your answers on a piece of paper and scanning them in.\n",
    "- Submit this separate PDF to the **Homework 9 (Questions 1, 2, and 3.4; written problems)** assignment on Gradescope, and **make sure to correctly select the pages associated with each question**!\n",
    "\n",
    "### To submit the autograded problems (Questions 3-5; marked [Autograded üíª])\n",
    "\n",
    "1. Select `Kernel -> Restart & Run All` to ensure that you have executed all cells, including the test cells.\n",
    "2. Read through the notebook to make sure everything is fine and all tests passed.\n",
    "3. Run the cell below to run all tests, and make sure that they all pass.\n",
    "4. Download your notebook using `File -> Download as -> Notebook (.ipynb)`, then upload your notebook to Gradescope under \"Homework 9 (Questions 3-5; autograded problems)\". Make sure your notebook is still named `hw09.ipynb` and the name has not been changed.\n",
    "5. Stick around while the Gradescope autograder grades your work.\n",
    "6. Check that you have a confirmation email from Gradescope and save it as proof of your submission.\n",
    "\n",
    "Your Homework 9 submission time will be the **later** of your two individual submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476782fe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d141a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "otter": {
   "tests": {
    "q03_01": {
     "name": "q03_01",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> best_transformation in [1, 2, 3, 4]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q03_02": {
     "name": "q03_02",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> callable(fit_model_and_return_predictions)\nTrue",
         "failure_message": "fit_model_and_return_predictions is not a function!",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = fit_model_and_return_predictions(homeruns[['Year']], homeruns['Homeruns'])\n>>> isinstance(out, np.ndarray) and out.shape[0] == 120\nTrue",
         "failure_message": "Output of fit_model_and_return_predictions has wrong shape (expecting length 120).",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = fit_model_and_return_predictions(homeruns[['Year']].sample(55), homeruns['Homeruns'].sample(55))\n>>> out.shape[0] == 55\nTrue",
         "failure_message": "Output of fit_model_and_return_predictions has wrong shape (expecting length 55).",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = fit_model_and_return_predictions(homeruns[['Year']], homeruns['Homeruns'])\n>>> out2 = fit_model_and_return_predictions(homeruns[['Year']], homeruns['Homeruns'])\n>>> np.allclose(out, out2)\nTrue",
         "failure_message": "Output of fit_model_and_return_predictions is non-deterministic, i.e. not the same on each function call.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q03_03": {
     "name": "q03_03",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> regressor = create_knn_regressor(x=np.array([10, 11, 12, 19, 25]), y=np.array([5, 17, 26, -5, 3]), k=3)\n>>> np.isclose(regressor(20), 8) and np.isclose(regressor(10), 16)\nTrue",
         "failure_message": "Incorrect outputs for input x, y, and k in the problem description.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> regressor = create_knn_regressor(x=np.array([10, 11, 12, 19, 25]), y=np.array([5, 17, 26, -5, 3]), k=5)\n>>> np.isclose(regressor(100), 9.2)\nTrue",
         "failure_message": "Incorrect outputs for the same x and y as in the problem description, with k=5. Should always return 9.2 since there are only 5 points.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> regressor = create_knn_regressor(x=np.array([10, 11, 12, 19, 25]), y=np.array([5, 17, 26, -5, 3]), k=1)\n>>> np.allclose([regressor(i) for i in range(10, 26)], [5.0, 17.0, 26.0, 26.0, 26.0, 26.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, 3.0, 3.0, 3.0])\nTrue",
         "failure_message": "Incorrect outputs for the same x and y as in the problem description, with k=1.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q04_01": {
     "name": "q04_01",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_ordinal(diamonds)\n>>> set(out.columns) == {'ordinal_cut', 'ordinal_clarity', 'ordinal_color'}\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_ordinal(diamonds)\n>>> np.unique(out['ordinal_cut']).tolist() == [0, 1, 2, 3, 4]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_ordinal(diamonds.head(5))\n>>> out.sum().sum() == 39\nTrue",
         "failure_message": "Wrong output for example provided in problem statement.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q04_02": {
     "name": "q04_02",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_one_hot(diamonds)\n>>> out.shape == (53940, 20)\nTrue",
         "failure_message": "Wrong shape for create_one_hot output.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_one_hot(diamonds)\n>>> out.loc[:5, out.columns.str.contains('cut')].sum().sum() == 6\nTrue",
         "failure_message": "Wrong output for example provided in problem statement.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_one_hot(diamonds)\n>>> out.columns.str.startswith('one_hot').all()\nTrue",
         "failure_message": "All columns must start with \"one_hot\".",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_one_hot(diamonds)\n>>> out.isin([0,1]).all().all()\nTrue",
         "failure_message": "All values are 0s or 1s.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q04_03": {
     "name": "q04_03",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_proportions(diamonds)\n>>> out.shape[1] == 3\nTrue",
         "failure_message": "Incorrect shape for create_proportions.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_proportions(diamonds)\n>>> np.isclose(out.head().sum().sum(), 2.752094920281795)\nTrue",
         "failure_message": "Wrong output for example provided in problem statement.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_proportions(diamonds)\n>>> out.columns.str.startswith('proportion_').all()\nTrue",
         "failure_message": "All column names must start with 'proportion_'.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_proportions(diamonds)\n>>> ((out >= 0) & (out <= 1)).all().all()\nTrue",
         "failure_message": "All values must be between 0 and 1 (inclusive).",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q04_04": {
     "name": "q04_04",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_quadratics(diamonds)\n>>> out.columns.str.contains(' * ').all()\nTrue",
         "failure_message": "All columns should contain '*'.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_quadratics(diamonds)\n>>> np.isclose(out.loc[:5, out.columns.str.contains('carat')].sum().sum(), 197.6061)\nTrue",
         "failure_message": "Wrong output for example provided in problem statement.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_quadratics(diamonds)\n>>> ('x * z' in out.columns) or ('z * x' in out.columns)\nTrue",
         "failure_message": "'x * z' or 'z * x' should be a column name.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> out = create_quadratics(diamonds)\n>>> out.shape[1] == 15\nTrue",
         "failure_message": "DataFrame shape was incorrect.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q05_test_transform_carat": {
     "name": "q05_test_transform_carat",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> transformed = TransformDiamonds(diamonds)\n>>> out = transformed.transform_carat(diamonds)\n>>> isinstance(out, np.ndarray)\nTrue",
         "failure_message": "Output should be array.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> transformed = TransformDiamonds(diamonds)\n>>> out = transformed.transform_carat(diamonds)\n>>> out[172, 0] == 1\nTrue",
         "failure_message": "Incorrect element at [172, 0].",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> transformed = TransformDiamonds(diamonds)\n>>> out = transformed.transform_carat(diamonds)\n>>> out[0, 0] == 0\nTrue",
         "failure_message": "Incorrect element at [0, 0].",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q05_transform_to_depth_pct": {
     "name": "q05_transform_to_depth_pct",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> transformed = TransformDiamonds(diamonds)\n>>> out = transformed.transform_to_depth_pct(diamonds)\n>>> len(out.shape) == 1\nTrue",
         "failure_message": "Shape should be 1-dimensional.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> transformed = TransformDiamonds(diamonds)\n>>> out = transformed.transform_to_depth_pct(diamonds)\n>>> np.isclose(out[0], 61.286, atol=0.0001)\nTrue",
         "failure_message": "Incorrect value of depth_pct first element.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q05_transform_to_quantile": {
     "name": "q05_transform_to_quantile",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> transformed = TransformDiamonds(diamonds)\n>>> out = transformed.transform_to_quantile(diamonds)\n>>> transformed_top_1000 = TransformDiamonds(diamonds[:1000])\n>>> out_top_1000 = transformed_top_1000.transform_to_quantile(diamonds)\n>>> isinstance(out, np.ndarray)\nTrue",
         "failure_message": "Output type should be array.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> transformed = TransformDiamonds(diamonds)\n>>> out = transformed.transform_to_quantile(diamonds)\n>>> np.isclose(out[0, 0], 0.005050505050505059, atol=0.01)\nTrue",
         "failure_message": "Output had wrong value. Make sure you're using random_state=98!",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> diamonds = pd.read_csv('data/diamonds.csv')\n>>> transformed = TransformDiamonds(diamonds)\n>>> out = transformed.transform_to_quantile(diamonds)\n>>> np.isclose(out[1, 0], 0.0, atol=0.01)\nTrue",
         "failure_message": "Output had wrong value. Make sure you're using random_state=98!",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
