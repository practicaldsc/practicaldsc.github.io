{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3afe137",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *\n",
    "import lec19_util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6823b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 19\n",
    "\n",
    "# Pipelines, Generalization\n",
    "\n",
    "### EECS 398-003: Practical Data Science, Fall 2024\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/fa24\">github.com/practicaldsc/fa24</a></small>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    "  MathJax.Hub.Config({\n",
    "    TeX: {\n",
    "      extensions: [\"color.js\"],\n",
    "      packages: {\"[+]\": [\"color\"]},\n",
    "    }\n",
    "  });\n",
    "  </script>\n",
    "  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b023e3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/interpolation.png\" width=700><br>(<a href=\"https://x.com/Luky_BB/status/1852511393297342756\">source)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35564024",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements üì£\n",
    "\n",
    "- Homework 9 is due on **Monday, November 11th**.\n",
    "- The Portfolio Homework will be released by tomorrow.\n",
    "- Homework 8 solutions can be found in [**#282 on Ed**](https://edstem.org/us/courses/61012/discussion/5648287)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678102c2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Come say hi on Thursday!\n",
    "\n",
    "A few other professors and I are hosting a faculty-student panel, where you can learn more about our career (and personal) paths. Come say hi ‚Äì there will be pizza üçï!\n",
    "\n",
    "<center><img src=\"imgs/CSE Panel 11_7.png\" width=400></center>\n",
    "\n",
    "[**RSVP here**](https://docs.google.com/forms/d/e/1FAIpQLSchVg5byJC5cHJrUit8_e8d_Nb8NGEHk_vPKRWR3BBcnsq2gw/viewform)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ab7dc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Pipelines.\n",
    "- Generalization.\n",
    "- Train-test splits.\n",
    "- Hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ccc812",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b1b9d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d85b1f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954356b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll start with our trusty commute times dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe3749",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/commute-times.csv')\n",
    "df['day_of_month'] = pd.to_datetime(df['date']).dt.day\n",
    "df['month'] = pd.to_datetime(df['date']).dt.month_name()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8df3d0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our goal, as always, is to predict commute time in `'minutes'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec4cc9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['minutes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07625c4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The main numerical feature we have is `'departure_hour'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ad7b1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .plot(kind='scatter', x='departure_hour', y='minutes')\n",
    "    .update_layout(xaxis_title='Home Departure Time (AM)', \n",
    "                   yaxis_title='Minutes',\n",
    "                   title='Commuting Time vs. Home Departure Time')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35b3ce",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Last class, we used transformer classes to one hot encode `'day'` and `'month'`. We'll look at how we can easily use these columns ‚Äì and more! ‚Äì as inputs to a linear model that predicts commute times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a23af",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/image_0.png\" width=\"50%\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "So far, we've used transformers (like `OneHotEncoder` and `StandardScaler`) for feature engineering and models (like `LinearRegression`) for prediction. We can combine these steps into a single `Pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c35f07",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pipelines in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceecb4c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- From [`sklearn`'s documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html):\n",
    "\n",
    "> `Pipeline` allows you to sequentially apply a list of transformers to preprocess the data and, **if desired**, conclude the sequence with a final predictor for predictive modeling.<br><br>Intermediate steps of the pipeline must be \"transforms\", that is, they must implement `fit` and `transform` methods. The final estimator only needs to implement `fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdfb770",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- General template: `pl = Pipeline([trans_1, trans_2, ..., model])`.<br><small>Note that the `model` is optional, meaning you can have Pipelines of just transformers.<br>Each element in the list must be a **tuple**; the first item in the tuple should be a \"name\" for the step, and the second item should be a transformer or estimator instance.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c6f6cd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once a Pipeline is instantiated, you can fit **all** steps (transformers and model) using `pl.fit(X, y)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663d103",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To make predictions using **raw, untransformed data**, use `pl.predict(X)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab58ac",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our first Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1a569",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's build a Pipeline that:\n",
    "    1. One hot encodes `'day'` and `'month'`.\n",
    "    2. Fits a regression model on just the one hot encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa9cc7",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9380c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "pl = Pipeline([\n",
    "    ('one-hot', OneHotEncoder(drop='first')),\n",
    "    ('lin-reg', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88572ffc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now that `pl` is instantiated, we `fit` it the same way we would fit the individual steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a22e4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pl.fit(X=df[['day', 'month']], y=df['minutes']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07caf57",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, to make predictions using **raw data**, all we need to do is use `pl.predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a603033",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pl.predict([['Mon', 'November']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783ad99",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `pl` performs **both** feature transformation and prediction with just a single call to `predict`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f7d28",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "    \n",
    "### Pipeline internals\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967bed9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- We can access individual \"steps\" of a `Pipeline` through the `named_steps` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b71ae",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pl.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9049e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pl.named_steps['one-hot'].transform(df[['day', 'month']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6f70c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pl.named_steps['one-hot'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8d790",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pl.named_steps['lin-reg'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae8fa1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- `pl` also has a `score` method, the same way a fit `LinearRegression` instance does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ddd52d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Why is this so low?\n",
    "pl.score(df[['day', 'month']], df['minutes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ce97a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More sophisticated Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ddf50d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the previous example, we one hot encoded every input column, and didn't use any columns that were originally numeric.<br><small>That's not realistic or useful!</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e2938",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Why is this so low?\n",
    "pl.score(df[['day', 'month']], df['minutes']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e787b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What if we want to perform different transformations on different columns, or include some columns without transformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d66af7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Or, what if we want to perform multiple transformations to the same column?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9d027",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are a variety of useful functions/classes we can use:\n",
    "\n",
    "| Name | Functionality |\n",
    "| --- | --- |\n",
    "| `ColumnTransformer` | Allows us to transform different columns with different transformations.<br><small>Instantiate a `ColumnTransformer` using a list of tuples, where:<br>‚Ä¢ The first element is a \"name\" we choose for the transformer.<br>‚Ä¢ The second element is a transformer instance (e.g. `OneHotEncoder()`).<br>‚Ä¢ The third element is a **list of relevant column names**.</small> | \n",
    "| `FunctionTransformer` | Allows us to create a custom transformation (similar to using `.apply` on a DataFrame's columns). |\n",
    "| `make_pipeline` | Helper function for creating a `Pipeline` (slightly less verbose).<br>**Note that you can make a pipeline of just transformations,<br> if you want to use multiple transformations on the same column!** |\n",
    "| `make_column_transformer` | Helper function for creating a `ColumnTransformer`. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a615b223",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import FunctionTransformer, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587a949",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583cdb9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Before writing any code, let's plan out _how_ we want to transform our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c1897",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df[['departure_hour', 'day', 'month', 'day_of_month']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e6900",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `'departure_hour'`: Create degree 2 and degree 3 polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aedf466",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `'day'`: One hot encode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22883a9c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `'month'`: One hot encode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7128a1d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `'day_of_month'`: Separate into five weeks, then one hot encode.<br>\n",
    "<small>Days 1 to 7 are Week 1, Days 8 to 15 are Week 2, and so on.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242317f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- After all of these transformations, we'll fit a `LinearRegression` object ‚Äì i.e., fit a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae61c0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><small>\n",
    "    \n",
    "`'departure_hour'`: Create degree 2 and degree 3 polynomial features.<br>\n",
    "`'day'`: One hot encode.<br>\n",
    "`'month'`: One hot encode.<br>\n",
    "`'day_of_month'`: Separate into five weeks, then one hot encode.<br>\n",
    "    \n",
    "</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87acfb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's start with `'day_of_month'`, since it seems to involve the most complicated transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4250426",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, let's figure out how to extract the week number given the day of the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3283ebb8",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "example_vals = df['day_of_month'].tail()\n",
    "example_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e34a0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Expression to convert from day of month to Week #.\n",
    "'Week ' + ((example_vals - 1) // 7 + 1).astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8d225a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The function that FunctionTransformer takes in\n",
    "# itself takes in a Series/DataFrame, not a single element!\n",
    "# Here, we're having that function return a new Series/DataFrame,\n",
    "# depending on what's passed in to .tranform (experiment on your own).\n",
    "week_converter = FunctionTransformer(lambda s: 'Week ' + ((s - 1) // 7 + 1).astype(str)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3a7c9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "week_converter.transform(df[['day_of_month']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c734284",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We need to apply two consecutive transformations to `'day_of_month'`, which calls for a Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de921492",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "day_of_month_transformer = make_pipeline(week_converter, OneHotEncoder(drop='first')) \n",
    "day_of_month_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb8ba0c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "day_of_month_transformer.fit_transform(df[['day_of_month']]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d003cd7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, `day_of_month_transformer` does everything we need to transform `'day_of_month'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300566d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><small>\n",
    "    \n",
    "`'departure_hour'`: Create degree 2 and degree 3 polynomial features.<br>\n",
    "`'day'`: One hot encode.<br>\n",
    "`'month'`: One hot encode.<br>\n",
    "`'day_of_month'`: Separate into five weeks, then one hot encode. ‚úÖ **Use `day_of_month_transformer`.**<br>\n",
    "    \n",
    "</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d6150",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Every other column only needs a single transformation. We can specify the transformations needed for each column using `make_column_transformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262df1f9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85867778",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessing = make_column_transformer(\n",
    "    (PolynomialFeatures(3), ['departure_hour']),\n",
    "    (OneHotEncoder(drop='first'), ['day', 'month']),\n",
    "    (day_of_month_transformer, ['day_of_month']),\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1297253",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, we're ready for a final Pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953f8bc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(preprocessing, LinearRegression())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b7ac6",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.fit(X=df[['departure_hour', 'day', 'month', 'day_of_month']], y=df['minutes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044601e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once our Pipeline is fit, we can use it to make predictions!<br><small>What's the predicted commute time if I leave at 8:30AM on a Tuesday in November, which happens to be the 5th of the month?</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e97b3",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.predict(pd.DataFrame([{\n",
    "    'departure_hour': 8.5,\n",
    "    'day': 'Tue',\n",
    "    'month': 'November',\n",
    "    'day_of_month': 5\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86646f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Activity</h3>\n",
    "    \n",
    "How many columns does the final design matrix that `model` creates have? If you write code to determine the answer, make sure you can walk through the steps over the past few slides to figure out **why** the answer is what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cbb876",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3e0427",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2610d24b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272d621",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What went wrong with polls in 2016? Can we trust them now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63e036",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Trump's victory in 2016 came as a surprise to many, since most polls in swing states had Clinton ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b66f0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Polls severely underestimated support for Trump; many voters were undecided until the last minute, and many didn't want to share they supported Trump."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a4844",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But a more systematic issue with the polls was that:\n",
    "    1. college-educated voters tended to be more likely to respond to polls.\n",
    "    1. college-educated voters tended to support Clinton over Trump.\n",
    "    1. there are fewer college-educated voters than non-college-educated voters, meaning that the polled support for Trump was **lower** than the true support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21bddd7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Read more [**at this CNBC article**](https://www.cbsnews.com/news/2016-polls-president-trump-clinton-what-went-wrong/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6608922",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10941a9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You and Billy are studying for an upcoming exam. You both decide to test your understanding by taking a **practice exam**.<br><small>Your logic: If you do well on the practice exam, you should do well on the real exam.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b1e5ea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You each take the practice exam once and look at the solutions afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de2cf0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Your strategy**: Memorize the answers to all practice exam questions, e.g. \"Question 1: A; Question 2: C; Question 3: A.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b8774",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Billy's strategy**: Learn high-level concepts from the solutions, e.g. \"the TF-IDF of term $t$ in document $d$ is large when $t$ occurs often in $d$ but rarely overall.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a721e667",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Who will do better on the **practice exam**? Who will probably do better on the **real exam**? üßê"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c1cd0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating the quality of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0258a986",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So far, we've computed the MSE (and $R^2$) of our fit regression models on the **data that we used to fit them**, i.e. the **training data**.<br><small>This mean squared error is called the **training MSE**, or **training error**.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b17164",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We've said that Model A is **better** than Model B if Model A's MSE is **lower** than Model B's MSE.\n",
    "    - Remember, our **training data** is a sample from some population.\n",
    "    - Just because a model fits the training data well doesn't mean it will **generalize** and work well on **similar, unseen samples** from the same population!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dddbc6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6fb810",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's collect two samples $\\{(x_i, y_i)\\}$ from the same population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d42f6e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(23) # For reproducibility.\n",
    "def sample_from_pop(n=100):\n",
    "    x = np.linspace(-2, 3, n)\n",
    "    y = x ** 3 + (np.random.normal(0, 3, size=n))\n",
    "    return pd.DataFrame({'x': x, 'y': y})\n",
    "sample_1 = sample_from_pop()\n",
    "sample_2 = sample_from_pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fc11f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For now, let's just look at Sample 1. The relationship between $x$ and $y$ is roughly **cubic**; that is, $y \\approx x^3$.<br><small>Remember, in reality, you won't get to see the population distribution. If you could, there'd be no need to build a model!</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b55c6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "px.scatter(sample_1, x='x', y='y', title='Sample 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52dde4b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28cd00",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's fit three **polynomial** models on Sample 1: degree 1, degree 3, and degree 25.<br><small>Again, we'll use the `PolynomialFeatures` transformer.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df8b3d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fit_transform fits and transforms the same input.\n",
    "d2 = PolynomialFeatures(3)\n",
    "d2.fit_transform(np.array([1, 2, 3, 4, -2]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a75d49",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Below, we look at our three models' predictions on Sample 1, which they were **trained** on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40153c10",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Look at the definition of train_and_plot in lec19_util.py if you're curious as to how the plotting works.\n",
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_1, degs=[1, 3, 25], data_name='Sample 1')\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efadb0dc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The degree 25 polynomial has the lowest MSE on Sample 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53515fe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- How do the same fit polynomials look on Sample 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf330fe",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_2, degs=[1, 3, 25], data_name='Sample 2')\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c741af",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The degree 3 polynomial has the lowest MSE on Sample 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6330c8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that **we didn't get to see Sample 2 when fitting our models**! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b0b51",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As such, it seems that the degree 3 polynomial **generalizes better** to unseen data than the degree 25 polynomial does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120268e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What if we fit a degree 1, degree 3, and degree 25 polynomial **on Sample 2** as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c06db5",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_multiple_models(sample_1, sample_2, degs=[1, 3, 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf79081",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key idea**: Degree 25 polynomials seem to **vary more when trained on different samples** than degree 3 and 1 polynomials do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d18d8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97373e4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The training data we have access to is a sample from the population. We are concerned with our model's ability to **generalize** and work well on **different datasets** drawn from the same population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b4386",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we **fit** a model $H^*$ (e.g. a degree 3 polynomial) on **several different datasets** from a population. There are three sources of error that arise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da9f472",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. **Bias**: **The expected deviation between a predicted value and an actual value**.<br><small>In other words, **for a given $x_i$, how far is $H^*(x_i)$ from the true $y_i$, on average?**\n",
    "    - Low bias is good! ‚úÖ\n",
    "    - High bias is a sign of **underfitting**, i.e. that our model is too **basic** to capture the relationship between our features and response.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2179e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. **Model variance (\"variance\")**: **The variance of a model's predictions**.<br><small>In other words, **for a given $x_i$, how much does $H^*(x_i)$ vary across all datasets**?\n",
    "    - Low model variance is good! ‚úÖ\n",
    "    - High model variance is a sign of **overfitting**, i.e. that our model is too **complicated** and is prone to fitting to the noise in our training data.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190faa6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. **Observation error**: The error due to the random noise in the process we are trying to model (e.g. measurement error).<br><small>We can't reduce this without collecting more data!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b1e3b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/image_5.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399fc84",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, suppose:\n",
    "    - The <span style='color:#c6283f'><b>red bulls-eye</b></span> represents your **true weight and height** üßç.\n",
    "    - The <span style='color:#080c6f'><b>dark blue darts</b></span> represent **predictions of your weight and height** using different models that were fit using different samples drawn from the same population. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70296a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'd like our models to be in the top left, but in practice that's hard to achieve!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f9153",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Risk vs. empirical risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8189d5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since Lecture 14, we've minimized **empirical risk** to find optimal model parameters $\\vec{w}^*$:\n",
    "\n",
    "$$\n",
    "\\text{choose the $\\vec{w}$ such that } \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - H(x_i) \\right)^2 \\text{ is minimized}$$\n",
    "\n",
    "<center>or, equivalently:</center>\n",
    "\n",
    "$$\\vec{w}^* = \\underset{\\vec{w}}{\\text{argmin}} \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - H(x_i) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72c3b6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key idea**: A model that works well on past data should work well on future data, if future data looks like past data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ca1d9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What we really want is for the **expected loss for a new data point $(x_{\\text{new}}, y_{\\text{new}})$, drawn from the same population as the training set, to be small**. That is, we want\n",
    "    $$\\mathbb{E}[y_{\\text{new}} - H(x_{\\text{new}})]^2$$\n",
    "    to be minimized. The quantity above is called **risk**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeace0b8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What's that fancy $\\mathbb{E}$? It is the **expectation** operator of a random variable: it computes the **average value** of the random variable across its entire distribution.<br><small>For example, if $X \\sim \\text{Binomial}(n, p)$, then $\\mathbb{E}[X] = np$.<br>Here, the expectation is being computed across the entire population distribution of $(x_i, y_i)$ pairs.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aba911",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In general, we don't know the entire population distribution of $x$s and $y$s, so we can't compute risk exactly. That's why we compute **empirical risk**!\n",
    "\n",
    "$$\\mathbb{E}[y_{\\text{new}} - H(x_{\\text{new}})]^2 \\approx \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - H(x_i) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6abc10",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The bias-variance decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0679a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Risk can be decomposed as follows:<br><small>Remember, this expectation $\\mathbb{E}$ is over the entire population of $x$s and $y$s. In real life, we don't know what this population distribution is, so we can't put actual numbers to this.</small>\n",
    "\n",
    "$$\\mathbb{E}[y_{\\text{new}} - H(x_{\\text{new}})]^2 = \\text{model bias}^2 + \\text{model variance} + \\text{observation error}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b183d60b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key takeaway**: If we care about minimizing (empirical) risk, we can equivalently try to minimize both model bias and model variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a047208e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $H$ is too simple to capture the relationship between $x$s and $y$s in the population, $H$ will **underfit** to training sets and have **high bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8a0fd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $H$ is overly complex, $H$ will **overfit** to training sets and have **high variance**, meaning it will change significantly from one training set to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a1aee",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We won't cover the proof of the decomposition here ‚Äì read [**this**](https://learningds.org/ch/17/inf_pred_gen_prob.html#probability-behind-model-selection) for more ‚Äì but note that in Homework 7, you proved a related formula for $R_\\text{sq}(h)$:\n",
    "\n",
    "$$R_\\text{sq}(h) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - h)^2 = \\underbrace{\\frac{1}{n} \\sum_{i = 1}^n (y_i - \\bar{y})^2}_{\\text{variance of } y} + (\\bar{y} - h)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed914ad",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5217290",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As model variance increases, model bias tends to decrease, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0acad0f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The graph below shows, conceptually, this **tradeoff**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c7d212",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"imgs/bv-decomp.svg\" width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e42d6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "\n",
    "### Navigating the bias-variance tradeoff\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9691bc9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$\\mathbb{E}[y_{\\text{new}} - H(x_{\\text{new}})]^2 = \\text{model bias}^2 + \\text{model variance} + \\text{observation error}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59935fcb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- As we collect more data points (i.e. as $n \\uparrow$):\n",
    "    - Model variance decreases.\n",
    "    - If $H$ can exactly model the true population relationship between $x$ and $y$ (e.g. cubic), then model bias also decreases.\n",
    "    - If $H$ can't exactly model the true population relationship between $x$ and $y$, then model bias will remain large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d905ce",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- As we add more features (i.e. as $d \\uparrow$):\n",
    "    - Model variance increases, whether or not the feature was useful.\n",
    "    - Adding a useful feature decreases model bias.\n",
    "    - Adding a useless feature doesn't change model bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0846d9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Example: Suppose the actual relationship between $x$ and $y$ in the population is linear, and we fit $H$ using simple linear regression.\n",
    "    - Model bias = 0.\n",
    "    - Model variance $\\propto \\frac{d}{n}$.\n",
    "        - As $d \\uparrow$, model variance $\\uparrow$.\n",
    "        - As $n \\uparrow$, model variance $\\downarrow$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3df88",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Activity</h3>\n",
    "\n",
    "Determine how each change below affects model bias and variance compared to this model:\n",
    "\n",
    "<center><img src='imgs/hx.png' width=50%></center>\n",
    "\n",
    "For each change, choose all of the following that apply: **increase bias, decrease bias, increase variance, decrease variance.**\n",
    "\n",
    "1. Add degree 3 polynomial features.\n",
    "1. Add a feature of numbers chosen at random between 0 and 1.\n",
    "1. Collect 100 more points for the training set.\n",
    "1. Don‚Äôt use the `'veg'` feature.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874f550",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train-test splits\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3cca7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Avoiding overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b426cfe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We won't know whether our model has **overfit** to our sample (training data) unless we get to see how well it performs on a new sample from the same population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84e8be",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üí°**Idea**: **Split** our sample into a **training set** and **test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638d5195",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use **only** the training set to fit the model (i.e. find $\\vec{w}^*$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae9b95",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use the test set to evaluate the model's error (MSE, $R^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b20f911",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The test set is like a new sample of data from the same population as the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650136ad",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Generally:\n",
    "    - Training error reflects bias, **not variance**.\n",
    "    - Test error reflects **both bias and variance**, so we need to compute it to understand the true error of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f75988",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/train-test.png\" width='50%'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c57bb1a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train-test split üöÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a9a2f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `sklearn.model_selection.train_test_split` implements a train-test split for us! üôèüèº "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7c991",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If `X` is an array/DataFrame of features and `y` is an array/Series of responses,\n",
    "    ```py\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    ```\n",
    "    randomly splits the features and responses into training and test sets, such that the test set contains 0.25 of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eda6bd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3943ace",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the documentation!\n",
    "train_test_split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd803024",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's perform a train/test split on `sample_1`, for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8973b4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e52c4f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = sample_1[['x']] # DataFrame. \n",
    "y = sample_1['y'] # Series. \n",
    "# We don't have to choose 0.25.\n",
    "# We also don't have to set a random_state;\n",
    "# we've done this so that we get the same results in lecture every time.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502ada4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Before proceeding, let's check the sizes of `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d200bf4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Rows in X_train:', X_train.shape[0])\n",
    "display(X_train.head())\n",
    "print('Rows in X_test:', X_test.shape[0])\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917bd2ba",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c76b1d8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, we'll fit a model on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0fc17",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, we'll use a stand-alone `LinearRegression` model without a `Pipeline`, but this process would work the same if we were using a `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd606e0a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b4e9b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0250173a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's check our model's performance on the **training** set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af694a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, pred_train)\n",
    "mse_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40bdbd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- And the **test** set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528499b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, pred_test)\n",
    "mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950e210",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since `mse_train` and `mse_test` are similar, it **doesn't seem like our model is overfitting** to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d03e69",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If `mse_test` was much larger than `mse_train`, it would be evidence that our model is unable to **generalize well**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd23b67b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1597da9b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8258d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We recently looked at an example of **polynomial regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac9746",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_2, degs=[1, 3, 25], data_name='Sample 2')\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb8938",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When building these models:\n",
    "    - We **got to choose** the degree of the polynomials ‚Äì we chose 1, 3, and 25.\n",
    "    - We didn't get to choose the exact formulas for the three polynomials ‚Äì their formulas were **learned from data**.<br><small>No matter what the data looked like, the left-most model **had** to look like a line, because we chose its degree in advance.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44faaf15",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters vs. hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7379758",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **parameter** defines the relationship between variables in a model. **We learn parameters from data**.\n",
    "    - For instance, suppose we fit a degree 3 polynomial to data, and end up with:\n",
    "    \n",
    "    $$H(x) = 1 - 2x + 13x^2 - 4x^3$$\n",
    "    \n",
    "    - 1, -2, 13, and -4 are parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7244c3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **hyperparameter** is a parameter that **we choose _before_ our model is fit to the data**.\n",
    "    - Think of hyperparameters as knobs üéõ ‚Äì **we get to pick and tune them!**\n",
    "    - **Polynomial degree** was a hyperparameter in the previous example, and we tried three different values: 1, 3, and 25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edce0a5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How do we choose the \"right\" hyperparameter(s)?<br><small>Degree 3 was a better choice than degree 25, for example ‚Äì but how do we systematically choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32627297",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training error vs. test error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef3b1a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We know that a model's performance on a **test set** is a good estimate of its ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244d62ef",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We want to find the hyperparameter that leads to the best **test set performance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09609ab8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Idea:\n",
    "    1. Come up with a **list** of hyperparameters to try.\n",
    "    2. For each hyperparameter, train the model on the training set and compute its performance on the test set.\n",
    "    3. Pick the hyperparameter with the best performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86514efd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's try this strategy on Sample 1 from our earlier example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db0113",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll try to fit a polynomial model on the dataset; we'll choose the polynomial's degree from the list [1, 2, ..., 25]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9967a2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial degree vs. train/test error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f0202",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We already performed a train-test split of `sample_1` a few slides ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d1e12",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcaecef",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, we'll create models with degree 1 through degree 25 polynomial features and compute their train and test errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce148a8",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_errs = []\n",
    "test_errs = []\n",
    "for d in range(1, 26):\n",
    "    pl = make_pipeline(PolynomialFeatures(d), LinearRegression())\n",
    "    pl.fit(X_train, y_train)\n",
    "    train_errs.append(mean_squared_error(y_train, pl.predict(X_train)))\n",
    "    test_errs.append(mean_squared_error(y_test, pl.predict(X_test)))\n",
    "errs = pd.DataFrame({'Train Error': train_errs, 'Test Error': test_errs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6e9c5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's look at the plots of training error vs. degree and test error vs. degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f667f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = px.line(errs.iloc[:-1])\n",
    "fig.update_layout(showlegend=True, xaxis_title='Polynomial Degree', yaxis_title='Mean Squared Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f2606",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Training error appears to decrease as polynomial degree increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ace7d1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Test error appears to decrease until a \"valley\", and then increases again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc26bc4c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, we'd choose a degree of 3, since that degree has the **lowest test error**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27054f13",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training error vs. test error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a309e0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The pattern we saw in the previous example is true more generally.\n",
    "\n",
    "<center><img src='imgs/tt-errors.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64efc96",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We pick the hyperparameter(s) at the \"valley\" of test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33264c50",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that training error **tends** to underestimate test error, but it doesn't have to ‚Äì i.e., it is possible for test error to be lower than training error (say, if the test set is \"easier\" to predict than the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916cc913",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The results ‚Äì and the bias-variance tradeoff more generally ‚Äì hold true for \"classic\" machine learning models, like the ones we're studying here. But in deep neural networks, this pattern is often violated; extremely complex models can have low test error as well.<br><small>This phenomenon is known as \"double descent\"; learn more [**here**](https://en.wikipedia.org/wiki/Double_descent).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c268e52",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conducting train-test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8cf4e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall, <span style='color: blue'><b>training data</b></span> is used to fit our model, and <span style='color: orange'><b>test data</b></span> is used to evaluate our model.\n",
    "\n",
    "<center><img src='imgs/train-test-first.png' width=40%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d79f33",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: _How_ should we split?\n",
    "    - `sklearn`'s `train_test_split` splits **randomly**, which usually works well.\n",
    "    - However, if there is some element of **time** in the training data (say, when predicting the future price of a stock), a better split is \"past\" and \"future\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7570cd07",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How _large_ should the split be, e.g. 90%-10% vs. 75%-25%?\n",
    "    - There's a tradeoff ‚Äì a larger training set should lead to a \"better\" model, while a larger test set should lead to a better estimate of our model's ability to generalize.\n",
    "    - There's no \"right\" choice, but we usually choose between 10% to 25% for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3a4ab",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### But wait..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ac0e9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- With our current strategy, we are choosing the hyperparameter that creates the model that **performs best on the test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1166e61",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As such, we are **overfitting to the test set** ‚Äì the best hyperparameter for the test set might not be the best hyperparameter for a totally unseen dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79a906",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems like we need **another** split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7b328",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- On Thursday, we'll cover the more robust solution to the problem of selecting hyperparameters: **cross-validation**."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
