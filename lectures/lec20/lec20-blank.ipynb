{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991217fa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *\n",
    "import lec20_util as util\n",
    "def show_cv_slides():\n",
    "    src = \"https://docs.google.com/presentation/d/e/2PACX-1vTydTrLDr-y4nxQu1OMsaoqO5EnPEISz2VYmM6pd83ke8YnnTBJlp40NfNLI1HMgoaKx6GBKXYE4UcA/embed?start=false&loop=false&delayms=60000&rm=minimal\"\n",
    "    display(IFrame(src, width=900, height=361))\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f629f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 20\n",
    "\n",
    "# Cross-Validation and Regularization\n",
    "\n",
    "### EECS 398-003: Practical Data Science, Fall 2024\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/fa24\">github.com/practicaldsc/fa24</a></small>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    "  MathJax.Hub.Config({\n",
    "    TeX: {\n",
    "      extensions: [\"color.js\"],\n",
    "      packages: {\"[+]\": [\"color\"]},\n",
    "    }\n",
    "  });\n",
    "  </script>\n",
    "  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d26a73",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements üì£\n",
    "\n",
    "- Homework 9 is due on **Monday, November 11th**.<br><small>We're adding more office hours tomorrow and on Monday.</small>\n",
    "- The Portfolio Homework has been released! Read all about it [**here**](https://practicaldsc.org/portfolio). It has two due dates:\n",
    "    - A checkpoint (worth 15 points / 100) is due on **Monday, November 25th** (no slip days!).\n",
    "    - The full homework is due on **Saturday, December 7th** (no slip days!).\n",
    "- Homework 8 solutions can be found in [**#282 on Ed**](https://edstem.org/us/courses/61012/discussion/5648287), and Homework 7 solutions can be found in [**#259 on Ed**](https://edstem.org/us/courses/61012/discussion/5597496).\n",
    "- Please help spread the word about this class by submitting an **anonymous** testimony [**here**](https://docs.google.com/forms/d/e/1FAIpQLSd3YpC1N8T4ocSB0lrTb5-7Gyi2Vl3L-1bSzHag5wKMY_ns9g/viewform) üôè.<br><small>We'll share some of the responses we get on this form at [practicaldsc.org/next](https://practicaldsc.org/next), and in advertisement emails/posts we share with other students.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8599f5ff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Come say hi tonight!\n",
    "\n",
    "A few other professors and I are hosting a faculty-student panel, where you can learn more about our career (and personal) paths. Come say hi ‚Äì there will be pizza üçï!\n",
    "\n",
    "<center><img src=\"imgs/CSE Panel 11_7.png\" width=400></center>\n",
    "\n",
    "[**RSVP here**](https://docs.google.com/forms/d/e/1FAIpQLSchVg5byJC5cHJrUit8_e8d_Nb8NGEHk_vPKRWR3BBcnsq2gw/viewform)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e29ddf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Recap: Generalization, hyperparameters, and train-test splits.\n",
    "- Cross-validation.\n",
    "- Grid search.\n",
    "- Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8432d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Additional resources\n",
    "\n",
    "- Take a look at [mlu-explain.github.io](https://mlu-explain.github.io/), a site with interactive explanations for a lot of core machine learning topics, like:\n",
    "    - [Linear Regression](https://mlu-explain.github.io/linear-regression/).\n",
    "    - [The Bias-Variance Tradeoff](https://mlu-explain.github.io/bias-variance/).\n",
    "    - [Train, Test, and Validation Sets](https://mlu-explain.github.io/train-test-validation/).\n",
    "    - [Cross-Validation](https://mlu-explain.github.io/cross-validation/).\n",
    "    - and other ideas we'll see later in the semester!\n",
    "- We've linked these articles in the [Resources](https://practicaldsc.org/resources) tab of the course website, too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42d4e2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae569a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Generalization, hyperparameters, and train-test splits\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5776a912",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/shot-1.png\" width=900><br>As we increase the <b>complexity</b> (e.g., polynomial degree) of our models, their <b>training error</b> decreases.<br><br>Here, polynomial degree is a <b>hyperparameter</b> ‚Äì something we get to choose <b>before</b> fitting the model to the data. We're trying to determine how to choose the \"best\" hyperparameter.\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d409fc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/shot-2.png\" width=900><br>But, as we increase complexity, our models <b>overfit</b> to their <b>training data</b>, and can fail to generalize to <b>unseen, test data</b>.\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5d4b6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c32afd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We care about how well our models **generalize** to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8446b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The more complex a model is, the more it will **overfit** to the noise in the training data, and have high **model variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d755b5a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The less complex a model is, the more it will **underfit** the training data, and have high **bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf1cd5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To navigate this **bias-variance tradeoff**, we choose model complexity by choosing the model with the lowest error on an **unseen test set**.\n",
    "\n",
    "<center><img src=\"imgs/tt-errors.png\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaccd7a9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conducting train-test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a021cff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall, <span style='color: blue'><b>training data</b></span> is used to fit our model, and <span style='color: orange'><b>test data</b></span> is used to evaluate our model.\n",
    "\n",
    "<center><img src='imgs/train-test-first.png' width=40%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae99a5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: _How_ should we split?\n",
    "    - `sklearn`'s `train_test_split` splits **randomly**, which usually works well.\n",
    "    - However, if there is some element of **time** in the training data (say, when predicting the future price of a stock), a better split is \"past\" and \"future\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8fd780",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How _large_ should the split be, e.g. 90%-10% vs. 75%-25%?\n",
    "    - There's a tradeoff ‚Äì a larger training set should lead to a \"better\" model, while a larger test set should lead to a better estimate of our model's ability to generalize.\n",
    "    - There's no \"right\" choice, but we usually choose between 10% to 25% for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af619bc1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### But wait..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9a946",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- With our current strategy, we are choosing the hyperparameter that creates the model that **performs best on the test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d5618",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As such, we are **overfitting to the test set** ‚Äì the best hyperparameter for the test set might not be the best hyperparameter for a totally unseen dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8370d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems like we need **another** split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdbd830",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Next, we'll cover the more robust solution to the problem of selecting hyperparameters: **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25434b04",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b154d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea: A single validation set\n",
    "\n",
    "<center><img src='imgs/train-test-val.png' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f4d76",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Split the data into three sets: <span style='color: blue'><b>training</b></span>, <span style='color: green'><b>validation</b></span>, and <span style='color: orange'><b>test</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c0911",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. For each hyperparameter choice, <span style='color: blue'><b>train</b></span> the model only on the <span style='color: blue'><b>training set</b></span>, and <span style='color: green'><b>evaluate</b></span> the model's performance on the <span style='color: green'><b>validation set</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b03969",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Find the hyperparameter with the best <span style='color: green'><b>validation</b></span> performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c5940",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. Retrain the final model on the <span style='color: blue'><b>training</b></span> and <span style='color: green'><b>validation</b></span> sets, and report its performance on the <span style='color: orange'><b>test set</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f0100",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Issue**: This strategy is too dependent on the <span style='color: green'><b>validation</b></span> set, which may be small and/or not a representative sample of the data. **We're not going to do this.** ‚ùå"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6030aac",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A better idea: $k$-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5fa9a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead of relying on a single <span style='color: green'><b>validation</b></span> set, we can create $k$ <span style='color: green'><b>validation</b></span> sets, where $k$ is some positive integer (5 in the example below).\n",
    "\n",
    "<center><img src='imgs/k-fold.png' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7726f12f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since each data point is used for <span style='color: blue'><b>training</b></span> $k-1$ times and <span style='color: green'><b>validation</b></span> once, the (averaged) <span style='color: green'><b>validation</b></span> performance should be a good metric of a model's ability to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb09f0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $k$-fold cross-validation (or simply \"cross-validation\") is **the** technique we will use for finding hyperparameters, or more generally, for choosing between different possible models. **It's what you should use in your Portfolio Homework!** ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a412e3d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustrating $k$-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a9eed9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To illustrate $k$-fold cross-validation, let's use the following example dataset with $n = 12$ rows.<br><small>Suppose this dataset represents our **training set**, i.e. suppose we already performed a train-test split.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d97ae",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_data = pd.DataFrame().assign(x=range(0, 120, 10),\n",
    "                                      y=[9, 1, 58, 3, 6, 4, -2, 8, 1, 10, 1.1, -45])        \n",
    "display_df(training_data, rows=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be44734",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we choose $k = 4$. Then, each fold has $\\frac{12}{4} = 3$ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ce628",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "show_cv_slides()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554b594",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation, in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4208d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, **shuffle** the entire training set randomly and **split** it into $k$ disjoint folds, or \"slices\". Then:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbee46e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For each hyperparameter:\n",
    "    - For each slice:\n",
    "        - Let the slice be the \"validation set\", $V$.\n",
    "        - Let the rest of the data be the \"training set\", $T$.\n",
    "        - Train a model using the selected hyperparameter on the training set $T$.\n",
    "        - Evaluate the model on the validation set $V$.\n",
    "    - Compute the **average** validation error (e.g. MSE) for the particular hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e02b4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Choose the hyperparameter with the **lowest** average validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d617fda7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7f1a0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To illustrate, let's continue using the roughly-cubic dataset titled \"Sample 1\" from the past few lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943ea0f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(23) # For reproducibility.\n",
    "def sample_from_pop(n=100):\n",
    "    x = np.linspace(-2, 3, n)\n",
    "    y = x ** 3 + (np.random.normal(0, 3, size=n))\n",
    "    return pd.DataFrame({'x': x, 'y': y})\n",
    "sample_1 = sample_from_pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936fe30",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "px.scatter(sample_1, x='x', y='y', title='Sample 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf65b023",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll look at how to use $k$-fold cross-validation to choose a polynomial degree that best generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cb7d22",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Before** doing cross-validation, we must always perform a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8f6df",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_1[['x']], sample_1['y'], random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9f190",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation in `sklearn`\n",
    "\n",
    "\n",
    "<!-- Specifically, it takes in:\n",
    "- A `Pipeline` or estimator **that has not already been `fit`**.\n",
    "- Training data.\n",
    "- A value of $k$ (through the `cv` argument).\n",
    "- (Optionally) A `scoring` metric.\n",
    "\n",
    "and performs $k$-fold cross-validation, returning the values of the scoring metric on each fold. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82d96b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The `cross_val_score` function in `sklearn` implements $k$-fold cross-validation for us! \n",
    "<br><br>\n",
    "\n",
    "```py\n",
    "        cross_val_score(estimator, X_train, y_train, cv)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d5594",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5103348b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's perform $k$-fold cross validation in order to help us pick a degree for polynomial regression from the list [1, 2, ..., 25]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bafad9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll use $k=5$ since it's a common choice (and the default in `sklearn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b476e6",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01dbd7",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "errs_df = pd.DataFrame()\n",
    "for d in range(1, 26):\n",
    "    pl = make_pipeline(PolynomialFeatures(d), LinearRegression())\n",
    "    # The `scoring` argument is used to specify that we want to compute the MSE; \n",
    "    # the default is R^2. It's called \"neg\" MSE because, \n",
    "    # by default, sklearn likes to \"maximize\" scores, and maximizing -MSE is the same\n",
    "    # as minimizing MSE.\n",
    "    errs = cross_val_score(pl, X_train, y_train, \n",
    "                           cv=5, scoring='neg_mean_squared_error')\n",
    "    errs_df[f'Deg {d}'] = -errs # Negate to turn positive (sklearn computed negative MSE).\n",
    "errs_df.index = [f'Fold {i}' for i in range(1, 6)]\n",
    "errs_df.index.name = 'Validation Fold'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed09f4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4b8ec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that for each choice of degree (our hyperparameter), we have **five** MSEs, one for each \"fold\" of the data. This means that in total, $5 \\cdot 25 = 125$ models were trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c29f4f7",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "errs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c09fd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember, our goal is to choose the **degree** with the **lowest average** validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813392e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "errs_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca77c9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = errs_df.mean(axis=0).iloc[:18].plot(kind='line', title='Average Validation Error')\n",
    "fig.update_layout(xaxis_title='Degree', yaxis_title='Average Validation MSE', showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7fce1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "errs_df.mean(axis=0).idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3380ade6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that if we didn't perform $k$-fold cross-validation, but instead just used a single validation set, we may have ended up with a different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82484d9b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "errs_df.idxmin(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824cae2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "        \n",
    "- Suppose you have a training dataset with 1000 rows.\n",
    "- You want to decide between 20 hyperparameters for a particular model.\n",
    "- To do so, you perform 10-fold cross-validation.\n",
    "- **How many times is the first row in the training dataset (`X.iloc[0]`) used for training a model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d7855",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid search\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3390c2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An easier approach: `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5abbbf6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead of having to `for`-loop over possible hyperparameter values, we can let `sklearn` do the hard work for us, using `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67775e4b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `GridSearchCV` takes in:\n",
    "    - an **un-`fit`** instance of an estimator, and\n",
    "    - a **dictionary** of hyperparameter values to try,\n",
    "    \n",
    "  and performs $k$-fold cross-validation to find the **combination of hyperparameters** with the best average validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92fe706",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61de819",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why do you think it's called \"grid search\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184e1ef",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grid searching for the best polynomial degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ecc92",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's once again aim to fit a polynomial regression model to `X_train` and `y_train`, taken from `sample_1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17210f11",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, we want to try values of degree from 1 through 25, so we'll need to specify these values in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2f6c2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The key names in this dictionary are chosen very carefully.\n",
    "# They need to be of the format pipelinestep__hyperparametername,\n",
    "# where pipelinestep is a lowercase version of the step in the pipeline\n",
    "# that we want to tune, and \n",
    "# hyperparameter name is the formal name of the hyperparameter (see the documentation).\n",
    "hyperparams = {\n",
    "    'polynomialfeatures__degree': range(1, 26)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0316028e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "searcher = GridSearchCV(\n",
    "    make_pipeline(PolynomialFeatures(), LinearRegression()),\n",
    "    param_grid=hyperparams,\n",
    "    cv=5, # k = 5.\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e48a798",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Like any other estimator, `GridSearchCV` instances need to be `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f56ba",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b1f4d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once fit, `searcher` can tell us what it found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997edf1a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930feaf1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "-pd.DataFrame(np.vstack([searcher.cv_results_[f'split{i}_test_score'] for i in range(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8492a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `searcher` is now a fit regression model ‚Äì there's no _need_ to refit it on the entire training set, since it already did that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364c3fd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "searcher.predict([[4], \n",
    "                  [-1], \n",
    "                  [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1dc0a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another example: Commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd2c19b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can also use $k$-fold cross-validation to determine which subset of features to use in a linear model that predicts commute times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c28524",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(23)\n",
    "df = pd.read_csv('data/commute-times.csv')\n",
    "df['day_of_month'] = pd.to_datetime(df['date']).dt.day\n",
    "df['month'] = pd.to_datetime(df['date']).dt.month_name()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318e218",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's make several candidate pipelines. But first, **as always**, a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b9937",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Here, we're letting X_train and X_test keep all of the columns in the DataFrame\n",
    "# OTHER than 'minutes'.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('minutes', axis=1), df['minutes'], random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a80ecc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating many pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da19c4d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b3e09a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "selecter = FunctionTransformer(lambda x: x) # Shortcut to say \"keep just these columns.\"\n",
    "week_converter = FunctionTransformer(lambda s: 'Week ' + ((s - 1) // 7 + 1).astype(str))\n",
    "day_of_month_transformer = make_pipeline(week_converter, OneHotEncoder(drop='first')) # From last class.\n",
    "pipes = {\n",
    "    'departure_hour only': make_pipeline(\n",
    "        make_column_transformer((selecter, ['departure_hour'])),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    'departure_hour + day_of_month': make_pipeline(\n",
    "        make_column_transformer((selecter, ['departure_hour', 'day_of_month'])),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    'departure_hour + day OHE': make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (selecter, ['departure_hour']),\n",
    "            (OneHotEncoder(drop='first', handle_unknown='ignore'), ['day'])\n",
    "        ),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    'departure_hour + day OHE + month OHE': make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (selecter, ['departure_hour']),\n",
    "            (OneHotEncoder(drop='first', handle_unknown='ignore'), ['day', 'month'])\n",
    "        ),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    'departure_hour with poly features + day OHE + month OHE + week': make_pipeline(\n",
    "        make_column_transformer(\n",
    "        (PolynomialFeatures(3), ['departure_hour']),\n",
    "        (OneHotEncoder(drop='first', handle_unknown='ignore'), ['day', 'month']),\n",
    "        (day_of_month_transformer, ['day_of_month']),\n",
    "    ),\n",
    "    LinearRegression())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5c48c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, we won't be able to use `GridSearchCV` directly, because we're choosing between many different pipelines, **not** between hyperparameters for a particular pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009d9b9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Average Training MSE', 'Average Validation MSE'])\n",
    "for pipe in pipes:\n",
    "    fitted = GridSearchCV(\n",
    "        pipes[pipe],\n",
    "        param_grid={}, # No hyperparameters, but we could have them.\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=10, # Change this and see what happens!,\n",
    "        return_train_score=True # So that we can compute training MSEs, too.\n",
    "    )\n",
    "    fitted.fit(X_train, y_train)\n",
    "    results.loc[pipe] = [-fitted.cv_results_['mean_train_score'][0], -fitted.cv_results_['mean_test_score'][0]]\n",
    "commute_models_summarized = (\n",
    "    results\n",
    "    .sort_values('Average Training MSE')\n",
    "    .plot(kind='barh', barmode='group', width=1000)\n",
    "    .update_layout(xaxis_title='Mean Squared Error', yaxis_title='Model')\n",
    ")\n",
    "commute_models_summarized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa67531",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What's the \"right\" combination of features to choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7d6d7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc84f0c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Split the data into two sets: <span style='color: blue'><b>training</b></span> and <span style='color: orange'><b>test</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31afbf19",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Use only the <span style='color: blue'><b>training</b></span> data when designing, training, and tuning the model.\n",
    "    - Use <span style='color: green'><b>$k$-fold cross-validation</b></span> to choose hyperparameters and estimate the model's ability to generalize.\n",
    "    - Do not ‚ùå look at the <span style='color: orange'><b>test</b></span> data in this step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9cd56",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Commit to your final model and train it using the entire <span style='color: blue'><b>training</b></span> set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e0b68",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. Test the data using the <span style='color: orange'><b>test</b></span> data. If the performance (e.g. MSE) is not acceptable, return to step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27193c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5. Finally, train on **all available data** and ship the model to production! üõ≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5126b4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üö® This is the process you should **always** use! üö® "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9b04c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "        \n",
    "What questions do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f937f04f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7687fe1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64fd73f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So far, to us, \"model complexity\" has essentially meant \"number of features.\"<br><small>The main hyperparameter we've tuned is polynomial degree. For instance, a polynomial of degree 5 has 5 features ‚Äì an $x$, $x^2$, $x^3$, $x^4$, and $x^5$ feature.<br>In the more recent example, we <b>manually</b> created several different pipelines, each of which used different combinations of features from the commute times dataset.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1021ff5b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once we've created several different candidate models, we've used cross-validation to choose the one that best generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e4b45",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another approach: **instead of manually choosing which features to include, put some constraint on the optimal parameters, $w_0^*, w_1^*, ..., w_d^*$**.<br><small>This would save us time from having to think of combinations of features that might be relevant.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e00321",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuition: **The bigger the optimal parameters $w_0^*, w_1^*, ..., w_d^*$ are, the more _overfit_ the model is to the training data.**<br><small>Why?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8899830c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial regression returns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c317a14",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once again, let's try fitting a polynomial model to Sample 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d57fe",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sample_1[['x']], sample_1['y'], random_state=23)\n",
    "px.scatter(x=X_train['x'], y=y_train, title=\"Sample 1's Training Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85e1c1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This time, instead of using cross-validation to choose a polynomial degree, let's pick a degree in advance, like 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a84f85",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(25), LinearRegression(fit_intercept=False))\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073d967",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X_train['x'], y=y_train, title=\"Sample 1's Training Data\")\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train['x'].sort_values(),\n",
    "    y=model.predict(X_train.sort_values('x')),\n",
    "    mode='lines',\n",
    "    line=dict(width=4),\n",
    "    name='Fit Polynomial of Degree 25'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e9dde",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This degree 25 polynomial is clearly overfit to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a396e9c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Observe**: `sklearn` assigned **really large** values to many features. For instance, we're seeing that the coefficient on $x^{10}$ is 1712.92. This means that if $x$ changes a little, the output is going to change **a lot**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116566d3",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.Series(model.named_steps['linearregression'].coef_).abs().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3534c2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Penalizing large parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0da3e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Idea**: In addition to just minimizing mean squared error, what if we could **also** try and prevent large parameter values?<br><small>Maybe this would lead to less overfitting!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098c0a3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Previously, we minimized mean squared error to find $\\vec{w}_\\text{OLS}^*$:\n",
    "    $$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2$$\n",
    "    <small>Here, OLS stands for \"ordinary least squares.\"</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943d868",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Idea**: Instead, minimize **regularized** mean squared error:\n",
    "\n",
    "$$R_\\text{sq-reg}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\mathbf{+} \\underbrace{\\lambda \\sum_{j = 1}^d w_j^2}_{\\text{penalty!}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641cf8e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For each $w_j$, we've added a penalty on how large it is! The larger each $w_j$ is, the larger $R_\\text{sq-reg}$ will be.<br>Remember, the goal is to find the $\\vec{w}^*$ that minimizes $R_\\text{sq-reg}$.<br><small>Note that we're not penalizing the intercept term, which doesn't really contribute to overfitting.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f9090",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6604f82",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The specific penalty we've chosen ‚Äì $w_j^2$ for each $w_j$ ‚Äì is called **$L_2$ regularization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d3887",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Minimizing mean squared error, with $L_2$ regularization, is called **ridge regression**. The **objective function** for ridge regression is:\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\mathbf{+} \\underbrace{\\lambda \\sum_{j = 1}^d w_j^2}_{\\text{penalty!}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb8a83",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\lambda$ is a **hyperparameter**, which we choose through cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18789910",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $\\vec{w}_\\text{ridge}^*$ that minimizes $R_\\text{ridge}(\\vec{w})$ is not necessarily the same as $\\vec{w}_\\text{OLS}^*$, which minimizes $R_\\text{sq}(\\vec{w})$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3eb19",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "The objective function we minimize to find $\\vec{w}_\\text{ridge}^*$ in **ridge regression** is:\n",
    "    \n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "    \n",
    "$\\lambda$ is a **hyperparameter**, which we choose through cross-validation.\n",
    "    \n",
    "- What if we pick $\\lambda = 0$ ‚Äì what is $\\vec{w}_\\text{ridge}^*$ then?\n",
    "- What happens to $\\vec{w}_\\text{ridge}^*$ as $\\lambda \\rightarrow \\infty$?\n",
    "- Can $\\lambda$ be negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e0f60",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression, visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5df70",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As $\\lambda$ increases, the penalty on the size of $\\vec{w}_\\text{ridge}^*$ increases, meaning that each $w_j^*$ inches closer to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91c9cdf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An equivalent way of formulating the ridge regression objective function,\n",
    "\n",
    "    $$\\text{minimize} \\:\\:\\:\\: \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "\n",
    "    is as a **constrained** optimization problem:\n",
    "    \n",
    "    $$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d w_j^2 \\leq Q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020f45a6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $Q$ and $\\lambda$ are **inversely related**: the larger $Q$ is, the less of a penalty we're putting on size of $\\vec{w}_\\text{ridge}^*$, so the smaller $\\lambda$ is.<br><small>The exact relationship between $Q$ and $\\lambda$ is outside of the scope of this course, as is the proof of this fact. Take IOE 310!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b6a7b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuitively:\n",
    "\n",
    "    - The **loss surface** for just the mean squared error component is in <span style=\"color:blue\"><b>blue</b></span>.\n",
    "    - The constraint, $\\sum_{j = 1}^d w_j^2 \\leq Q$, is in <span style=\"color:green\"><b>green</b></span>.\n",
    "\n",
    "<center><img src=\"imgs/ball.png\" width=500>(<a href=\"https://ds100.org/course-notes-su23/cv_regularization/cv_reg.html\">source</a>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd529e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The bigger $Q$ is ‚Äì so, the smaller $\\lambda$ is ‚Äì the larger the <span style=\"color:green\"><b>green circle</b></span> is!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d4914",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- The bigger $Q$ is, the larger the range of possible values for $\\vec{w}_\\text{ridge}^*$ is, and the closer $\\vec{w}_\\text{ridge}^*$ gets to $\\vec{w}_\\text{OLS}^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea378a57",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding $\\vec{w}_\\text{ridge}^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd94ecfd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We know that the $\\vec{w}_\\text{OLS}^*$ that minimizes mean squared error,\n",
    "    $$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2$$\n",
    "  is the one that satisfies the normal equations, $X^TX \\vec{w} = X^T \\vec{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1cf87a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sometimes, $\\vec{w}^*_\\text{OLS}$ is unique, and sometimes there are infinitely many possible $\\vec{w}^*_\\text{OLS}$.<br><small>There are infinitely many possible $\\vec{w}^*_\\text{OLS}$ when the design matrix, $X$, is not full rank! All of these infinitely many solutions minimize mean squared error.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f28c7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which vector $\\vec{w}_\\text{ridge}^*$ minimizes the ridge regression objective function?\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945a354",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It turns out there is **always** a unique solution for $\\vec{w}_\\text{ridge}^*$, even if $X$ is not full rank. It is:\n",
    "    $$\\vec{w}_\\text{ridge}^* = (X^TX + n \\lambda I)^{-1} X^T \\vec{y}$$\n",
    "    <br><small>The proof is outside of the scope of the class, and requires vector calculus.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c8bde4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since there is **always** a unique solution, ridge regression is often used in the presence of multicollinearity!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414d2d1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taking a step back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f458a97f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\vec{w}_\\text{ridge}^*$ **doesn't** minimize mean squared error ‚Äì it minimizes a slightly different objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e23a0a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, why would we use ever use ridge regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c44311",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d5aad",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Fortunately, `sklearn` can perform ridge regression for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d68136",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960333c1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Just to experiment, let's set $\\lambda$ to something extremely large and look at the resulting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02120626",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The name of the lambda hyperparameter in sklearn is alpha.\n",
    "model_large_lambda = make_pipeline(PolynomialFeatures(25), Ridge(alpha=1000000000000000000000000000))\n",
    "model_large_lambda.fit(X_train, y_train)\n",
    "fig = px.scatter(x=X_train['x'], y=y_train, title=\"Sample 1's Training Data\")\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train['x'].sort_values(),\n",
    "    y=model_large_lambda.predict(X_train.sort_values('x')),\n",
    "    mode='lines',\n",
    "    line=dict(width=4, color='purple'),\n",
    "    name='Extremely Regularized Polynomial of Degree 25'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04983342",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e703c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d45df",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0ba43",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using `GridSearchCV` to choose $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bf76b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In general, we won't just arbitrarily choose a value of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fec965",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead, we'll perform $k$-fold cross-validation to choose the $\\lambda$ that leads to predictions that work best on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ee333",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'ridge__alpha': 10.0 ** np.arange(-2, 15) # Try 0.01, 0.1, 1, 10, 100, 1000, ... \n",
    "}\n",
    "model_regularized = GridSearchCV(\n",
    "    estimator=make_pipeline(PolynomialFeatures(25), Ridge()),\n",
    "    param_grid=hyperparams,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "model_regularized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a41df54",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's check the optimal $\\lambda$ it found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd5fa4c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_regularized.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d63635",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the resulting predictions look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a1a8c5",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The name of the lambda hyperparameter in sklearn is alpha.\n",
    "fig = px.scatter(x=X_train['x'], y=y_train, title=\"Sample 1's Training Data\")\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train['x'].sort_values(),\n",
    "    y=model.predict(X_train.sort_values('x')),\n",
    "    mode='lines',\n",
    "    line=dict(width=4),\n",
    "    name='Unregularized Polynomial of Degree 25'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train['x'].sort_values(),\n",
    "    y=model_regularized.predict(X_train.sort_values('x')),\n",
    "    mode='lines',\n",
    "    line=dict(width=4, color='green'),\n",
    "    name='Regularized Polynomial of Degree 25'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09446148",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that the regularized polynomial is _less_ overfit to the specific noise in the training data than the unregularized polynomial!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1eeeb3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The largest coefficients are all much smaller now, too.\n",
    "<br><small>The coefficient on $x^{20}$ is 0.000136.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d19e1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.Series(model_regularized.best_estimator_.named_steps['ridge'].coef_).abs().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24664f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another example: Commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8d68a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use ridge regression to find coefficients for the most complicated commute times model we looked at earlier, to see if it can generalize well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b0a7f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('minutes', axis=1), df['minutes'], random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62308507",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll use the model previously labeled `departure_hour with poly features + day OHE + month OHE + week`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0522fc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_models_summarized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1930006",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression for commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b41c139",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's first instantiate a Pipeline for the steps we want to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc8c1f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_pipe = make_pipeline(\n",
    "        make_column_transformer(\n",
    "        (PolynomialFeatures(3), ['departure_hour']),\n",
    "        (OneHotEncoder(drop='first', handle_unknown='ignore'), ['day', 'month']),\n",
    "        (day_of_month_transformer, ['day_of_month']),\n",
    "    ),\n",
    "    Ridge())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ec4a0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Then, as before, we'll fit a `GridSearchCV` instance with a hyperparameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19bdf13",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lambdas = 10.0 ** np.arange(-10, 15) # Try 0.00000000001, ..., 1, 10, 100, 1000, ... \n",
    "hyperparams = {\n",
    "    'ridge__alpha': lambdas \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44d78e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_regularized = GridSearchCV(\n",
    "    commute_pipe,\n",
    "    param_grid = hyperparams,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "commute_model_regularized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a8afa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which $\\lambda$ did it choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a2682",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_regularized.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860e4c8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How did the average validation MSE change with $\\lambda$?<br><small>Here, large values of $\\lambda$ mean **less complex models**, not more complex.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ca475",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pd.Series(-commute_model_regularized.cv_results_['mean_test_score'], \n",
    "              index=np.log(lambdas))\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .plot(kind='line', x='index', y=0)\n",
    "    .update_layout(xaxis_title='$\\log(\\lambda)$', yaxis_title='Average Validation MSE')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33594acc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b247c17",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Could we have chosen a different method of penalizing each $w_j$ other than $w_j^2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aea3ca",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Ridge regression's objective function happened to have a closed-form solution. What if we want to minimize a function that **can't** be minimized by hand?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
