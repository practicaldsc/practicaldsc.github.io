{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39347b24",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *\n",
    "import lec21_util as util\n",
    "np.random.seed(23) # For reproducibility.\n",
    "def sample_from_pop(n=100):\n",
    "    x = np.linspace(-2, 3, n)\n",
    "    y = x ** 3 + (np.random.normal(0, 3, size=n))\n",
    "    return pd.DataFrame({'x': x, 'y': y})\n",
    "sample_1 = sample_from_pop()\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc5d1e0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 21\n",
    "\n",
    "# Regularization, Gradient Descent\n",
    "\n",
    "### EECS 398-003: Practical Data Science, Fall 2024\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/fa24\">github.com/practicaldsc/fa24</a></small>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    "  MathJax.Hub.Config({\n",
    "    TeX: {\n",
    "      extensions: [\"color.js\"],\n",
    "      packages: {\"[+]\": [\"color\"]},\n",
    "    }\n",
    "  });\n",
    "  </script>\n",
    "  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2e3c1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements üì£\n",
    "\n",
    "- The Portfolio Homework has been released! Read all about it [**here**](https://practicaldsc.org/portfolio). It has two due dates:\n",
    "    - A checkpoint (worth 15 points / 100) is due on **Monday, November 25th** (no slip days!).\n",
    "    - The full homework is due on **Saturday, December 7th** (no slip days!).\n",
    "- Homework 10 will be out later this week.\n",
    "- The [**Grade Report**](https://www.gradescope.com/courses/823979/assignments/5191081) now includes scores and slip days through Homework 8.\n",
    "- Please help spread the word about this class by submitting an **anonymous** testimony [**here**](https://docs.google.com/forms/d/e/1FAIpQLSd3YpC1N8T4ocSB0lrTb5-7Gyi2Vl3L-1bSzHag5wKMY_ns9g/viewform) üôè.<br><small>We'll share some of the responses we get on this form at [practicaldsc.org/next](https://practicaldsc.org/next), and in advertisement emails/posts we share with other students.</small>\n",
    "- Interested in research opportunities in data science? See [**#302 on Ed**](https://edstem.org/us/courses/61012/discussion/5690003)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab071e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Recap: Ridge Regression.\n",
    "- LASSO.\n",
    "- Gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbfeb6d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de6aef1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Ridge Regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e33537",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation: Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee37d52",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Last class, we fit a degree 25 polynomial model to Sample 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca39868",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sample_1[['x']], sample_1['y'], random_state=23)\n",
    "px.scatter(x=X_train['x'], y=y_train, title=\"Sample 1's Training Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae89f32",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# include_bias=False makes sure that PolynomialFeatures \n",
    "# doesn't create a new column of all 1s in the design matrix, since\n",
    "# LinearRegression already makes one.\n",
    "model = make_pipeline(PolynomialFeatures(25, include_bias=False), LinearRegression())\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce84675",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This degree 25 polynomial is clearly overfit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca258ad",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X_train['x'], y=y_train, title=\"Sample 1's Training Data\")\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train['x'].sort_values(),\n",
    "    y=model.predict(X_train.sort_values('x')),\n",
    "    mode='lines',\n",
    "    line=dict(width=4),\n",
    "    name='Fit Polynomial of Degree 25'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f138980",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inspecting the fit degree 25 polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37507de",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What are the resulting coefficients of the <b><span style=\"color:#ff7f0f\">fit polynomial</span></b>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b493db",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.Series(model.named_steps['linearregression'].coef_, index=range(1, 26))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84e88f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does the resulting polynomial actually look like, as an equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7a9ce",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# These coefficients are rounded to two decimal places.\n",
    "# The coefficient on x^25 is not 0.00, but is something very small.\n",
    "util.display_features(model.named_steps['linearregression'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c1649",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `sklearn` assigned **really large** values to many features.<br><small>This means that if $x$ changes a little, the output is going to change **a lot**. It seems like some of the terms are trying to \"cancel\" each other out ‚Äì some have large negative coefficients, some have large positive coefficients.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c07cb5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuition: In general, **the bigger the optimal parameters $w_0^*, w_1^*, ..., w_d^*$ are, the more _overfit_ the model is to the training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378df71",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331554d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Idea**: In addition to just minimizing mean squared error, what if we could **also** try and prevent large parameter values?<br><small>Maybe this would lead to less overfitting!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ef710",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Minimizing mean squared error, with $L_2$ regularization, is called **ridge regression**. The **objective function** for ridge regression is:\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\mathbf{+} \\underbrace{\\lambda \\sum_{j = 1}^d w_j^2}_{\\text{penalty!}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2851fb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuition: Instead of just minimizing mean squared error, we balance minimizing mean squared error and a penalty on the size of the fit coefficients, $w_1^*$, $w_2^*$, ..., $w_d^*$.<br><small>We don't regularize the intercept term!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e368ad5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\lambda$ is a **hyperparameter**, which we choose through cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbddcdc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $\\vec{w}_\\text{ridge}^*$ that minimizes $R_\\text{ridge}(\\vec{w})$ is not necessarily the same as $\\vec{w}_\\text{OLS}^*$, which minimizes $R_\\text{sq}(\\vec{w})$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704ff94",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another interpretation of ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1216080",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As $\\lambda$ increases, the penalty on the size of $\\vec{w}_\\text{ridge}^*$ increases, meaning that each $w_j^*$ inches closer to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5d1b78",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An equivalent way of formulating the ridge regression objective function,\n",
    "\n",
    "    $$\\text{minimize} \\:\\:\\:\\: \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "\n",
    "    is as a **constrained** optimization problem:\n",
    "    \n",
    "    $$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d w_j^2 \\leq Q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e64fa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $Q$ and $\\lambda$ are **inversely related**: the larger $Q$ is, the less of a penalty we're putting on size of $\\vec{w}_\\text{ridge}^*$, so the smaller $\\lambda$ is.<br><small>The exact relationship between $Q$ and $\\lambda$ is outside of the scope of this course, as is the proof of this fact. Take IOE 310!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3af53",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression, visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b66f17",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d w_j^2 \\leq Q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1698f89",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuitively:\n",
    "\n",
    "    - The **loss surface** for just the mean squared error component is in <span style=\"color:blue\"><b>blue</b></span>.\n",
    "    - The constraint, $\\sum_{j = 1}^d w_j^2 \\leq Q$, is in <span style=\"color:green\"><b>green</b></span>.<br><small>The larger $Q$ is, the larger the radius of the ball is.</small>\n",
    "\n",
    "<center><img src=\"imgs/ball.png\" width=500>(<a href=\"https://ds100.org/course-notes-su23/cv_regularization/cv_reg.html\">source</a>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f2d30",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The bigger $Q$ is ‚Äì so, the smaller $\\lambda$ is ‚Äì the larger the <span style=\"color:green\"><b>green circle</b></span> is!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6a6d7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The bigger $Q$ is, the larger the range of possible values for $\\vec{w}_\\text{ridge}^*$ is, and the closer $\\vec{w}_\\text{ridge}^*$ gets to $\\vec{w}_\\text{OLS}^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc9a68",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding $\\vec{w}_\\text{ridge}^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa826c9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We know that the $\\vec{w}_\\text{OLS}^*$ that minimizes mean squared error,\n",
    "    $$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2$$\n",
    "  is the one that satisfies the normal equations, $X^TX \\vec{w} = X^T \\vec{y}$.<br><small>Recall, linear regression that minimizes mean squared error, without any other constraints, is called **ordinary least squares (OLS)**.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958a0bc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sometimes, $\\vec{w}^*_\\text{OLS}$ is unique, and sometimes there are infinitely many possible $\\vec{w}^*_\\text{OLS}$.<br><small>There are infinitely many possible $\\vec{w}^*_\\text{OLS}$ when the design matrix, $X$, is not full rank! All of these infinitely many solutions minimize mean squared error.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e30f7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which vector $\\vec{w}_\\text{ridge}^*$ minimizes the ridge regression objective function?\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2ebc9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It turns out there is **always** a unique solution for $\\vec{w}_\\text{ridge}^*$, even if $X$ is not full rank. It is:\n",
    "    $$\\vec{w}_\\text{ridge}^* = (X^TX + n \\lambda I)^{-1} X^T \\vec{y}$$\n",
    "    <br><small>The proof is outside of the scope of the class, and requires vector calculus.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37266ef",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since there is **always** a unique solution, ridge regression is often used in the presence of multicollinearity!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475cde87",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taking a step back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f88c9e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\vec{w}_\\text{ridge}^*$ **doesn't** minimize mean squared error ‚Äì it minimizes a slightly different objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e566e2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, why would we use ever use ridge regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68deab",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874342f8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Fortunately, `sklearn` can perform ridge regression for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5eabd9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b785af",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Just to experiment, let's set $\\lambda$ to something extremely large and look at the resulting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c710987",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The name of the lambda hyperparameter in sklearn is alpha.\n",
    "model_large_lambda = make_pipeline(PolynomialFeatures(25, include_bias=False), \n",
    "                                   Ridge(alpha=1000000000000000000000000000))\n",
    "model_large_lambda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fbb8c4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the extremely regularized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d467bc8f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the <b><span style=\"color:purple\">resulting predictions</span></b> look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9593648",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X_train['x'], y=y_train, title=\"Sample 1's Training Data\")\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train['x'].sort_values(),\n",
    "    y=model_large_lambda.predict(X_train.sort_values('x')),\n",
    "    mode='lines',\n",
    "    line=dict(width=4, color='purple'),\n",
    "    name='Extremely Regularized Polynomial of Degree 25'\n",
    "))\n",
    "fig.update_layout(width=1000, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa04907",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee247b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_large_lambda.named_steps['ridge'].intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb2d98",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# All 0!\n",
    "model_large_lambda.named_steps['ridge'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e6c47",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855ae30",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using `GridSearchCV` to choose $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb51b1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In general, we won't just arbitrarily choose a value of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3a43a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead, we'll perform $k$-fold cross-validation to choose the $\\lambda$ that leads to predictions that work best on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58646f90",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'ridge__alpha': 10.0 ** np.arange(-2, 15) # Try 0.01, 0.1, 1, 10, 100, 1000, ... \n",
    "}\n",
    "model_regularized = GridSearchCV(\n",
    "    estimator=make_pipeline(PolynomialFeatures(25, include_bias=False), Ridge()),\n",
    "    param_grid=hyperparams,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "model_regularized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2411aa4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's check the optimal $\\lambda$ it found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a096ee2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_regularized.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb78b19",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the regularized degree 25 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e56b09",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the <b><span style=\"color:green\">resulting predictions</span></b> look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174079a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X_train['x'], y=y_train, title=\"Sample 1's Training Data\")\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train['x'].sort_values(),\n",
    "    y=model.predict(X_train.sort_values('x')),\n",
    "    mode='lines',\n",
    "    line=dict(width=4),\n",
    "    name='Unregularized Polynomial of Degree 25'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train['x'].sort_values(),\n",
    "    y=model_regularized.predict(X_train.sort_values('x')),\n",
    "    mode='lines',\n",
    "    line=dict(width=4, color='green'),\n",
    "    name='Regularized Polynomial of Degree 25'\n",
    "))\n",
    "fig.update_layout(width=1000, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc26c2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that the <b><span style=\"color:green\">regularized polynomial</span></b> is _less_ overfit to the specific noise in the training data than the <b><span style=\"color:#ff7f0f\">unregularized polynomial</span></b>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2559aa71",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The largest coefficients are all much smaller now, too.\n",
    "<br><small>The coefficient on $x^{20}$ is 0.000136.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f32634",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.display_features(model_regularized.best_estimator_.named_steps['ridge'], precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de2fa9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that none of them are exactly 0, but many of them are close!<br><small>This will be important later.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f75b7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tuning multiple hyperparameters at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316dde9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What if we don't want to fix a polynomial degree in advance, and instead want to choose the degree using cross-validation, while also using ridge regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016a61a6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- No problem!<br><small>Note that the next cell takes much longer than the previous call to `fit` took, since it needs to try every combination of $\\alpha$ and polynomial degree.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad275cb",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'ridge__alpha': 10.0 ** np.arange(-2, 15),\n",
    "    'polynomialfeatures__degree': range(1, 26)\n",
    "}\n",
    "model_regularized_degree = GridSearchCV(\n",
    "    estimator=make_pipeline(PolynomialFeatures(include_bias=False), Ridge()),\n",
    "    param_grid=hyperparams,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "model_regularized_degree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc627385",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, let's check the optimal $\\lambda$ **and** polynomial degree it found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15bf5cf",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_regularized_degree.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc91a73",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the regularized degree 3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46320891",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the <b><span style=\"color:skyblue\">resulting predictions</span></b> look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c5c164",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X_train['x'], y=y_train, title=\"Sample 1's Training Data\")\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train['x'].sort_values(),\n",
    "    y=model.predict(X_train.sort_values('x')),\n",
    "    mode='lines',\n",
    "    line=dict(width=4),\n",
    "    name='Unregularized Polynomial of Degree 25'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train['x'].sort_values(),\n",
    "    y=model_regularized.predict(X_train.sort_values('x')),\n",
    "    mode='lines',\n",
    "    line=dict(width=4, color='green'),\n",
    "    name='Regularized Polynomial of Degree 25'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train['x'].sort_values(),\n",
    "    y=model_regularized_degree.predict(X_train.sort_values('x')),\n",
    "    mode='lines',\n",
    "    line=dict(width=4, color='skyblue'),\n",
    "    name='Regularized Polynomial of Degree 3'\n",
    "))\n",
    "polyfig = fig.update_layout(width=1000, height=800)\n",
    "polyfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a6de8f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.display_features(model_regularized_degree.best_estimator_.named_steps['ridge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d2738",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run the cell below to set up the next slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c4027a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "unregularized_train = mean_squared_error(y_train, model.predict(X_train))\n",
    "unregularized_test = mean_squared_error(y_test, model.predict(X_test))\n",
    "regularized_lambda_train = mean_squared_error(y_train, model_regularized.predict(X_train))\n",
    "regularized_lambda_validation = (-model_regularized.cv_results_['mean_test_score']).min()\n",
    "regularized_lambda_test = mean_squared_error(y_test, model_regularized.predict(X_test))\n",
    "regularized_lambda_degree_train = mean_squared_error(y_train, model_regularized_degree.predict(X_train))\n",
    "regularized_lambda_degree_validation = (-model_regularized_degree.cv_results_['mean_test_score']).min()\n",
    "regularized_lambda_degree_test = mean_squared_error(y_test, model_regularized_degree.predict(X_test))\n",
    "results_df = pd.DataFrame(index=['training MSE', 'average validation MSE (across all folds)', 'test MSE']).assign(\n",
    "    unregularized=[unregularized_train, np.nan, unregularized_test],\n",
    "    regularized_lambda_only=[regularized_lambda_train, regularized_lambda_validation, regularized_lambda_test],\n",
    "    regularized_lambda_and_degree=[regularized_lambda_degree_train, regularized_lambda_degree_validation, regularized_lambda_degree_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905e1cd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reprs = {'unregularized': '<b><span style=\"color:#ff7f0f\">Unregularized (Degree 25)</span></b>',\n",
    "         'regularized_lambda_only': '<b><span style=\"color:green\">Regularized (Degree 25)<br><small>Used cross-validation to choose $\\lambda$</span></b>',\n",
    "         'regularized_lambda_and_degree': '<b><span style=\"color:skyblue\">Regularized (Degree 3)<br><small>Used cross-validation to choose $\\lambda$ and degree</small></span></b>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409bc7f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results_df_str = results_df.to_html()\n",
    "for rep in reprs:\n",
    "    results_df_str = results_df_str.replace(rep, reprs[rep])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eaff9d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing training, validation, and test errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141746d8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's compare the training and testing error of the three polynomials below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c7f41",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "polyfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844a32f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display(HTML(results_df_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5438cc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that the regularized polynomial, in which we used cross-validation to choose both the regularization penalty $\\lambda$ **and** degree, generalizes best to unseen data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07584f09",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda0152",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Could we have chosen a different method of penalizing each $w_j$ other than $w_j^2$?<br><small>We're about to see another option!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d1791",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Ridge regression's objective function happened to have a closed-form solution.<br>What if we want to minimize a function that **can't** be minimized by hand?<br><small>We'll talk about how towards the end of lecture!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37046dd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why is it called ridge regression?<br><small>See Homework 10!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838349ff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have about ridge regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c0bfba",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LASSO\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86ee19",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Penalizing large parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724230f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The ridge regression objective function,\n",
    "    $$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "    minimizes mean squared error, **plus** a **squared** penalty on the size of the fit coefficients, $w_1^*, w_2^*, ..., w_d^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547200d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We called the regularization strategy above \"$L_2$ regularization.\" Could we have regularized another way?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca09d70",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **LASSO** objective function uses $L_1$ regularization, which penalizes the **absolute value** of each coefficient:\n",
    "\n",
    "    $$R_\\text{LASSO}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d |w_j| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4bf683",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LASSO stands for \"least absolute shrinkage and selection operator.\"<br><small>We'll make sense of this name shortly.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0501204",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LASSO in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44d758",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Unlike with ridge regression or ordinary least squares, there is no general closed-form solution for $\\vec{w}_\\text{LASSO}^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd584d0f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, it can be estimated using numerical methods, which `sklearn` uses under-the-hood. Let's test it out.<br><small>More on numerical methods soon!</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca912b3a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285f421",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use LASSO to fit a degree 25 polynomial to Sample 1.<br><small>Here, we'll **fix** the degree, and cross-validate to find $\\lambda$.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e8325",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'lasso__alpha': 10.0 ** np.arange(-2, 15)\n",
    "}\n",
    "model_regularized_lasso = GridSearchCV(\n",
    "    estimator=make_pipeline(PolynomialFeatures(25, include_bias=False), Lasso()),\n",
    "    param_grid=hyperparams,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "model_regularized_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a1f482",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our cross-validation routine ends up choosing $\\lambda = 0.1$, though on its own, this doesn't really tell us anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766d812",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_regularized_lasso.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422902fa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the regularized degree 25 model, fit with LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26c331",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the <b><span style=\"color:red\">resulting predictions</span></b> look like, relative to the fit polynomials from earlier in the lecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bedb4ab",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "polyfig.add_trace(go.Scatter(\n",
    "    x=X_train['x'].sort_values(),\n",
    "    y=model_regularized_lasso.predict(X_train.sort_values('x')),\n",
    "    mode='lines',\n",
    "    line=dict(width=4, color='red'),\n",
    "    name='Regularized Polynomial of Degree 25, using LASSO'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80ade7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do you notice about the coefficients of the polynomial themselves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579db7d4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.display_features(model_regularized_lasso.best_estimator_.named_steps['lasso'], precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89279c6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Important**: Note that we fit a degree 25 polynomial, but many of the higher-order terms are missing, since their coefficients ended up being 0!<br><small>There's are no $x^{18}, x^{19}, x^{20}, ..., x^{25}$ terms above, and also no $x$ term.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ff8211",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The <b><span style=\"color:red\">resulting polynomial</span></b> ends up being of degree 17."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5bf9e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "#### Reference Slide\n",
    "    \n",
    "### Comparing training, validation, and test errors, again\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764f652",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Run the cell below to set up the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290c2d9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "regularized_lasso_train = mean_squared_error(y_train, model_regularized_lasso.predict(X_train))\n",
    "regularized_lasso_validation = (-model_regularized_lasso.cv_results_['mean_test_score']).min()\n",
    "regularized_lasso_test = mean_squared_error(y_test, model_regularized_lasso.predict(X_test))\n",
    "results_df['regularized_lasso'] = [regularized_lasso_train, regularized_lasso_validation, regularized_lasso_test]\n",
    "results_df_str = results_df.to_html()\n",
    "reprs['regularized_lasso'] = '<b><span style=\"color:red\">Regularized using LASSO (Degree 25)<br><small>Used cross-validation to choose $\\lambda$</small></span></b>'\n",
    "for rep in reprs:\n",
    "    results_df_str = results_df_str.replace(rep, reprs[rep])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d2874b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- How does our <b><span style=\"color:red\">LASSO-regularized polynomial</span></b> compare, in terms of errors, to our earlier polynomials?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb595476",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "display(HTML(results_df_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2cfcb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When using LASSO, many coefficients are set to 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a159b8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When using $L_1$ regularization ‚Äì that is, when performing LASSO ‚Äì many of the optimal coefficients $w_1^*, w_2^*, ..., w_d^*$ end up being **exactly 0**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f17f0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This was not the case in ridge regression ‚Äì there, the optimal coefficients were all very small, but none were exactly 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12833cf",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('#### Fit using Ridge:'))\n",
    "util.display_features(model_regularized.best_estimator_.named_steps['ridge'], precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448c1ec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If a feature has a coefficient of 0, it means it's not being used at all in making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a0c55",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('#### Fit using LASSO (notice the larger coefficient on $x^3$):'))\n",
    "util.display_features(model_regularized_lasso.best_estimator_.named_steps['lasso'], precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6937ae78",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LASSO implicitly performs **feature selection** for us ‚Äì it automatically tells us which features we don't need to use.<br><small>Here, it told us \"don't use $x$, don't use $x^{18}$, don't use $x^{19}$, ..., don't use $x^{25}$, and instead weigh the $x^2$ and $x^3$ terms more.\"</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7da64d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is where the name \"least absolute shrinkage and **selection** operator\" comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54212ccc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why does LASSO encourage sparsity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61623e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The fact that many of the optimal coefficients ‚Äì $w_1^*, w_2^*, ..., w_d^*$ ‚Äì are 0 when performing LASSO is often stated as:\n",
    "\n",
    "<center><b>LASSO encourages <i>sparsity</i>.</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc662f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To make sense of this, let's look at the equivalent formulation of LASSO as a **constrained optimization problem**.\n",
    "\n",
    "    $$\\text{minimize} \\:\\:\\:\\: \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d | w_j |$$\n",
    "\n",
    "    is equivalent to:\n",
    "    \n",
    "    $$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d | w_j | \\leq Q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53b5496",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Again, $Q$ and $\\lambda$ are **inversely related**: the larger $Q$ is, the less of a penalty we're putting on size of $\\vec{w}_\\text{LASSO}^*$, so the smaller $\\lambda$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ffce0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LASSO, visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6323a2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d | w_j | \\leq Q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e880ac2e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Again:\n",
    "    - The **loss surface** for just the mean squared error component is in <span style=\"color:blue\"><b>blue</b></span>.\n",
    "    - The constraint, $\\sum_{j = 1}^d |w_j| \\leq Q$, is in <span style=\"color:green\"><b>green</b></span>.<br><small>The larger $Q$ is, the larger the side length of the diamond is.</small>\n",
    "\n",
    "<center><img src=\"imgs/lasso-diamond.png\" width=500>(<a href=\"https://ds100.org/course-notes-su23/cv_regularization/cv_reg.html\">source</a>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70630caf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Notice that the <span style=\"color:green\"><b>constraint set</b></span> has clearly defined \"corners,\" which lie on the axes. The axes are where the parameter values, $w_1$ and $w_2$ here, are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30792554",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Due to the shape of the constraint set, it's likely that the minimum value of <b><span style=\"color:blue\">mean squared error</span></b>, among all options in the <b><span style=\"color:green\">green diamond</span></b>, will occur at a corner, where some of the parameter values are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c863f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have about LASSO, or regularization in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb38fc9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Commute times\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f27ef",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another example: Commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c3d05",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Last class, **before we learned about regularization**, we used $k$-fold cross-validation to choose between the following five models that predict commute time in `'minutes'`.\n",
    "\n",
    "<center><img src=\"imgs/five-pipelines.png\" width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db65f3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The most complicated model, labeled `departure_hour with poly features + day OHE + month OHE + week`, didn't generalize well to unseen data, relative to more simple models.<br><small>At least, not when we used ordinary least squares to train it.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d47bbe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use ordinary least squares, ridge regression, **and** LASSO to train commute time models, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dadf15",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/commute-times.csv')\n",
    "df['day_of_month'] = pd.to_datetime(df['date']).dt.day\n",
    "df['month'] = pd.to_datetime(df['date']).dt.month_name()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a486b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('minutes', axis=1), df['minutes'], random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727592c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ordinary least squares for commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44a736",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since we'll do the feature transformations repeatedly, we'll save them as a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01efa0fe",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "week_converter = FunctionTransformer(lambda s: 'Week ' + ((s - 1) // 7 + 1).astype(str),\n",
    "                                     feature_names_out='one-to-one')\n",
    "day_of_month_transformer = make_pipeline(week_converter, OneHotEncoder(drop='first'))\n",
    "# Note the include_bias=False once again!\n",
    "commute_feature_pipe = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (PolynomialFeatures(3, include_bias=False), ['departure_hour']),\n",
    "        (OneHotEncoder(drop='first', handle_unknown='ignore'), ['day', 'month']),\n",
    "        (day_of_month_transformer, ['day_of_month']),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0797cf3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, we'll fit a \"vanilla\" linear regression model, i.e. one that just minimizes mean squared error, with no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5cf20e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_ols = make_pipeline(commute_feature_pipe, LinearRegression())\n",
    "commute_model_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a52acb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are no hyperparameters to grid search for here, so we'll just fit the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ded99",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_ols.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1473b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll keep `commute_model_ols` aside for now, and compare its performance to the fit regularized models in a few moments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f78b12d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression for commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd0165",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Again, let's instantiate a Pipeline for the steps we want to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff7f9f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_pipe_ridge = make_pipeline(commute_feature_pipe, Ridge())\n",
    "commute_pipe_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901bb1b2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, since we need to choose the regularization penalty, $\\lambda$, we'll fit a `GridSearchCV` instance with a hyperparameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd6f37",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lambdas = 10.0 ** np.arange(-10, 15)\n",
    "hyperparams = {\n",
    "    'ridge__alpha': lambdas \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e538e79",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "commute_model_ridge = GridSearchCV(\n",
    "    commute_pipe_ridge,\n",
    "    param_grid = hyperparams,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "commute_model_ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49725111",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which $\\lambda$ did it choose?<br><small>On its own, this value of $\\lambda$ doesn't really tell us anything.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae7120f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_ridge.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d91202",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: average validation error vs. $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217a868",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How did the average validation MSE change with $\\lambda$?<br><small>Here, large values of $\\lambda$ mean **less complex models**, not more complex.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c449fa3",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pd.Series(-commute_model_ridge.cv_results_['mean_test_score'], \n",
    "              index=np.log10(lambdas))\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .plot(kind='line', x='index', y=0)\n",
    "    .update_layout(xaxis_title='$\\log(\\lambda)$', yaxis_title='Average Validation MSE')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e51202",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LASSO for commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5a497",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's instantiate a third Pipeline for the steps we want to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99e1ee",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_pipe_lasso = make_pipeline(commute_feature_pipe, Lasso())\n",
    "commute_pipe_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6630979",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, since we need to choose the regularization penalty, $\\lambda$, we'll fit a `GridSearchCV` instance with a hyperparameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c845628",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lambdas = 10.0 ** np.arange(-10, 15)\n",
    "hyperparams = {\n",
    "    'lasso__alpha': lambdas \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd354f20",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "commute_model_lasso = GridSearchCV(\n",
    "    commute_pipe_lasso,\n",
    "    param_grid = hyperparams,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "commute_model_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3454cf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which $\\lambda$ did it choose?<br><small>On its own, this value of $\\lambda$ doesn't really tell us anything.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7c691",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_lasso.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99bad26",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run the cell below to set up the next slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd824ee",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_results = pd.concat([\n",
    "    util.display_commute_coefs(commute_model_ols),\n",
    "    util.display_commute_coefs(commute_model_ridge.best_estimator_),\n",
    "    util.display_commute_coefs(commute_model_lasso.best_estimator_)\n",
    "], axis=1)\n",
    "commute_results.columns = ['ols', 'ridge', 'lasso']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7cf1f6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing coefficients across models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842dbaa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the resulting coefficients look like in all three models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65817905",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display_df(commute_results, rows=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd15753d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The coefficients in the OLS model tend to be the largest in magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0e1fd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the ridge model, the coefficients are all generally small, but none are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d878685",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the LASSO model, many coefficients are 0 exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcac26",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing training and test errors across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9303c3dd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_dict = {'ols': commute_model_ols, 'ridge': commute_model_ridge, 'lasso': commute_model_lasso}\n",
    "for model in model_dict:\n",
    "    display(Markdown(f'#### {model.upper()} model for commute times'))\n",
    "    display(Markdown(f'Training error: {mean_squared_error(y_train, model_dict[model].predict(X_train))}<br>Test error: {mean_squared_error(y_test, model_dict[model].predict(X_test))}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa8134",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The best-fitting LASSO model seems to have a lower training and testing MSE than the best-fitting ridge model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd82b4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, in general, sometimes LASSO performs better on unseen data, and sometimes ridge does. Cross-validate!<br><small>Sometimes, machine learning practitioners say \"there's no free lunch\" ‚Äì there's no universal always-best technique to use to make predictions, it always depends on the specific data you have.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882e7b6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Standardize when regularizing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1193d86",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we discussed a few lectures ago, by **standardizing** our features, we bring them all to the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354c14c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Standardizing features in ordinary least squares doesn't change our model's **performance**; rather, it impacts the interpretability of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c2aa68",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, when regularizing, we're penalizing the sizes of the coefficients, which can be on wildly different scales if the features are on different scales.\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\mathbf{+} \\underbrace{\\lambda \\sum_{j = 1}^d w_j^2}_{\\text{penalty!}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eae4ab",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, **when regularizing a linear model, you should standardize the features first**, so the coefficients for all features are on the same scale, and are penalized equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9908a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# In other words, commute_feature_pipe should've been this!\n",
    "make_pipeline(commute_feature_pipe, StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1abb33",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have about regularization in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1cb240",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent intuition\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e09fa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing empirical risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751e26a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Repeatedly, we've been tasked with **minimizing** the value of empirical risk functions.<br><small>Why? To help us find the **best** model parameters, $h^*$ or $w^*$, which help us make the **best** predictions!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00548b45",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We've minimized empirical risk functions in various ways.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b5fe5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$R_\\text{sq}(h) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - h)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90946480",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\displaystyle R_\\text{abs}(w_0, w_1) = \\frac{1}{n} \\sum_{i = 1}^n |y_i - (w_0 + w_1 x)|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08512c2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\displaystyle R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff7d4a3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\displaystyle R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2011a9f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing arbitrary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c53b6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Assume $f(w)$ is some **differentiable** function.<br><small>For now, we'll assume $f$ takes in a single number, $w$, as input and returns a single number as its output.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840f1d8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When tasked with minimizing $f(w)$, our general strategy has been to:<br>\n",
    "    1. Find $\\frac{df}{dw}(w)$, the derivative of $f$.\n",
    "    2. Find the input $w^*$ such that $\\frac{df}{dw}(w^*) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915d6af",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- However, there are cases where we can find $\\frac{df}{dw}(w)$, but **it is either difficult or impossible to solve $\\frac{df}{dw}(w^*) = 0$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e7a71",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$f(w) = 5w^4 - w^3 - 5w^2 + 2w - 9$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b02665",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Then what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cfcc22",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "util.draw_f()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae3b8f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does the derivative of a function tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6270532",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Goal**: Given a **differentiable** function $f(w)$, find the input $w^*$ that minimizes $f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dedbc3e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does $\\frac{d}{dw} f(w)$ mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5af06",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "interact(util.show_tangent, w0=(-1.5, 1.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e251326",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's go hiking!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905c44c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose you're at the top of a mountain üèîÔ∏è and need to get **to the bottom**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05886be1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Further, suppose it's really cloudy ‚òÅÔ∏è, meaning you can only see a few feet around you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7568856",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **How** would you get to the bottom?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b1e466",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"imgs/mountain.jpeg\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26011a2f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Searching for the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f1de37",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're given an initial _guess_ for a value of $w$ that minimizes $f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93340cd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"imgs/positive-slope.png\" width=500>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0a0cf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the <span style=\"color:red\">**slope of the tangent line at $f(w)$**</span> is **positive** üìà:\n",
    "    - Increasing $w$ **increases** $f$.\n",
    "    - This means the minimum must be to the **left** of the point $(w, f(w))$.\n",
    "    - Solution: **Decrease** $w$ ‚¨áÔ∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be441112",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The steeper the slope is, the further we must be from the minimum ‚Äì so, the steeper the slope, the quicker we should decrease $w$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92bc7a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Searching for the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e470e1d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're given an initial _guess_ for a value of $w$ that minimizes $f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91f463",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"imgs/negative-slope.png\" width=500>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb11938",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the <span style=\"color:red\">**slope of the tangent line at $f(w)$**</span> is **negative** üìâ:\n",
    "    - Increasing $w$ **decreases** $f$.\n",
    "    - This means the minimum must be to the **right** of the point $(w, f(w))$.\n",
    "    - Solution: **Increase** $w$ ‚¨ÜÔ∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b54c8b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The steeper the slope is, the further we must be from the minimum ‚Äì so, the steeper the slope, the quicker we should increase $w$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb48138",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a98ea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To minimize $f(w)$, start with an initial guess for the minimizing input, $w^{(0)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95fed1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Where do we go next?\n",
    "    - If $\\frac{df}{dw}(w^{(0)}) > 0$, **decrease $w^{(0)}$**.\n",
    "    - If $\\frac{df}{dw}(w^{(0)}) < 0$, **increase $w^{(0)}$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10ef70",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One way to accomplish this:\n",
    "\n",
    "$$w^{(1)} = w^{(0)} - \\frac{df}{dw}(w^{(0)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515ef47",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A consequence of the above **update rule**: the larger $\\frac{df}{dw}$ is, the bigger a step we take!<br><small>This matches our intuition from the previous flew slides ‚Äì the further we are from the minimum, the bigger of a step we should take!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b48c5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44796627",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To minimize a **differentiable** function $f$:\n",
    "    1. Pick a positive number, $\\alpha$. This number is called the **learning rate**, or **step size**.<br><small>Think of $\\alpha$ as a hyperparameter of the minimization process.</small>\n",
    "    2. Pick an **initial guess**, $w^{(0)}$.\n",
    "    3. Then, repeatedly update your guess using the **update rule**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02988fa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$w^{(t+1)} = w^{(t)} - \\alpha \\frac{df}{dw}(w^{(i)})$$\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37efef21",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Repeat this process until **convergence** ‚Äì that is, when $w$ doesn't change much from iteration to iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3aa861",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This procedure is called **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3d0b1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ef91b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient descent is a numerical method for finding the input to a function $f$ that minimizes the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc8959",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is called **gradient descent** because the gradient is the extension of the derivative to functions of multiple variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0267e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **numerical method** is a technique for approximating the solution to a mathematical problem, often by using the computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeaf642",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient descent is **widely used** in machine learning, to train models from linear regression to neural networks and transformers (includng ChatGPT)!<br><small>In machine learning, we use gradient descent to minimize empirical risk when we can't minimize it by hand, which is true in most, more sophisticated cases.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fc4af",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b801d9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In practice, we typically don't implement gradient descent ourselves ‚Äì we rely on existing implementations of it. But, we'll implement it here ourselves to understand what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e5c76",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's start with an initial guess $w^{(0)} = 0$ and a learning rate $\\alpha = 0.01$.\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\alpha \\frac{df}{dw}(w^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed281c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec8e18",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We see that pretty quickly, $w^{(i)}$ converges to $-0.727$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34dfb2f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 0, \\alpha = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83030bc6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=0, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc54c4d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 1.1, \\alpha = 0.01$\n",
    "\n",
    "What if we start with a different initial guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bc24b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=1.1, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab534156",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 0, \\alpha = 0.1$\n",
    "\n",
    "What if we use a different learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305da0a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=0, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef40ca9a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 0, \\alpha = 1$\n",
    "\n",
    "Some learning rates are so large that the values of $t$ explode towards infinity! Watch what happens when we use a learning rate of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68acbd67",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w = 0\n",
    "for i in range(50):\n",
    "    print(round(w, 4), round(util.f(w), 4))\n",
    "    w = w - 1 * util.df(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf14950",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent and empirical risk minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d41db19",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- While gradient descent can minimize other kinds of differentiable functions, its most common use case is in **minimizing empirical risk**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cfa058",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, consider:\n",
    "    - The constant model, $H(x) = h$.\n",
    "    - The dataset $-4, -2, 2, 4$.\n",
    "    - The initial guess $h_0 = 4$ and the learning rate $\\alpha = \\frac{1}{4}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642cc9d2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Exercise**: Find $h_1$ and $h_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273374a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- See the annotated slides for the solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b99b5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf1ca3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lingering questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f052c6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When is gradient descent _guaranteed_ to converge to a global minimum? What kinds of functions work well with gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928e597",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do we choose a step size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653a0f85",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do we use gradient descent to minimize functions of multiple variables, e.g.:\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da25e9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: Why **can't** we use gradient descent to find $\\vec{w}_\\text{LASSO}^*$?\n",
    "\n",
    "$$R_\\text{LASSO}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d |w_d|$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
