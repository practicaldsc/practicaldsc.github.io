{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333bb6fa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *\n",
    "import lec22_util as util\n",
    "from ipywidgets import FloatSlider, interact\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e20428",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 22\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "### EECS 398-003: Practical Data Science, Fall 2024\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/fa24\">github.com/practicaldsc/fa24</a></small>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    "  MathJax.Hub.Config({\n",
    "    TeX: {\n",
    "      extensions: [\"color.js\"],\n",
    "      packages: {\"[+]\": [\"color\"]},\n",
    "    }\n",
    "  });\n",
    "  </script>\n",
    "  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5185d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements üì£\n",
    "\n",
    "- The Portfolio Homework has been released! Read all about it [**here**](https://practicaldsc.org/portfolio). It has two due dates:\n",
    "    - A checkpoint (worth 15 points / 100) is due on **Monday, November 25th** (no slip days!).\n",
    "    - The full homework is due on **Saturday, December 7th** (no slip days!).\n",
    "- Homework 10 will be out later this week.\n",
    "- The [**Grade Report**](https://www.gradescope.com/courses/823979/assignments/5191081) now includes scores and slip days through Homework 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc49233",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Gradient descent intuition.\n",
    "- When is gradient descent guaranteed to work?\n",
    "- Gradient descent for multivariate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff1a6e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141bd63",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent intuition\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e6aad",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing arbitrary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84634e09",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Assume $f(w)$ is some **differentiable** function.<br><small>For now, we'll assume $f$ takes in a single number, $w$, as input and returns a single number as its output.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae862595",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When tasked with minimizing $f(w)$, our general strategy has been to:<br>\n",
    "    1. Find $\\frac{df}{dw}(w)$, the derivative of $f$.\n",
    "    2. Find the input $w^*$ such that $\\frac{df}{dw}(w^*) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae69be38",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- However, there are cases where we can find $\\frac{df}{dw}(w)$, but **it is either difficult or impossible to solve $\\frac{df}{dw}(w^*) = 0$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba19dd6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$f(w) = 5w^4 - w^3 - 5w^2 + 2w - 9$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3aa0a4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Then what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7b18b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "util.draw_f()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a4084",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does the derivative of a function tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76600aff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Goal**: Given a **differentiable** function $f(w)$, find the input $w^*$ that minimizes $f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a296b511",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does $\\frac{d}{dw} f(w)$ mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fbee4e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "interact(util.show_tangent, w0=(-1.5, 1.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d255ad3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's go hiking!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ae444",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose you're at the top of a mountain üèîÔ∏è and need to get **to the bottom**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3329aac",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Further, suppose it's really cloudy ‚òÅÔ∏è, meaning you can only see a few feet around you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f36c974",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **How** would you get to the bottom?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d5f6f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"imgs/mountain.jpeg\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00377603",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Searching for the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e06a2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're given an initial _guess_ for a value of $w$ that minimizes $f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185b926",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"imgs/positive-slope.png\" width=500>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aab7f14",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the <span style=\"color:red\">**slope of the tangent line at $f(w)$**</span> is **positive** üìà:\n",
    "    - Increasing $w$ **increases** $f$.\n",
    "    - This means the minimum must be to the **left** of the point $(w, f(w))$.\n",
    "    - Solution: **Decrease** $w$ ‚¨áÔ∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd428ea3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The steeper the slope is, the further we must be from the minimum ‚Äì so, the steeper the slope, the quicker we should decrease $w$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41921c26",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Searching for the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15c5a0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're given an initial _guess_ for a value of $w$ that minimizes $f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58107a5d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"imgs/negative-slope.png\" width=500>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4a0ca",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the <span style=\"color:red\">**slope of the tangent line at $f(w)$**</span> is **negative** üìâ:\n",
    "    - Increasing $w$ **decreases** $f$.\n",
    "    - This means the minimum must be to the **right** of the point $(w, f(w))$.\n",
    "    - Solution: **Increase** $w$ ‚¨ÜÔ∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b0a9ba",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The steeper the slope is, the further we must be from the minimum ‚Äì so, the steeper the slope, the quicker we should increase $w$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb2a378",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd1a09",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To minimize $f(w)$, start with an initial guess for the minimizing input, $w^{(0)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be996e99",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Where do we go next?\n",
    "    - If $\\frac{df}{dw}(w^{(0)}) > 0$, **decrease $w^{(0)}$**.\n",
    "    - If $\\frac{df}{dw}(w^{(0)}) < 0$, **increase $w^{(0)}$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb2cad8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One way to accomplish this:\n",
    "\n",
    "$$w^{(1)} = w^{(0)} - \\frac{df}{dw}(w^{(0)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485ffad",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A consequence of the above **update rule**: the larger $\\frac{df}{dw}$ is, the bigger a step we take!<br><small>This matches our intuition from the previous flew slides ‚Äì the further we are from the minimum, the bigger of a step we should take!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1caa2e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888562b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To minimize a **differentiable** function $f$:\n",
    "    1. Pick a positive number, $\\alpha$. This number is called the **learning rate**, or **step size**.<br><small>Think of $\\alpha$ as a hyperparameter of the minimization process.</small>\n",
    "    2. Pick an **initial guess**, $w^{(0)}$.\n",
    "    3. Then, repeatedly update your guess using the **update rule**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ecd1c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$w^{(t+1)} = w^{(t)} - \\alpha \\frac{df}{dw}(w^{(t)})$$\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530f250",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Repeat this process until **convergence** ‚Äì that is, when $w$ doesn't change much from iteration to iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ccc54",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This procedure is called **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4ab83",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda2fca",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient descent is a numerical method for finding the input to a function $f$ that minimizes the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57e6ba",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is called **gradient descent** because the gradient is the extension of the derivative to functions of multiple variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6c565",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **numerical method** is a technique for approximating the solution to a mathematical problem, often by using the computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7ae53",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient descent is **widely used** in machine learning, to train models from linear regression to neural networks and transformers (includng ChatGPT)!<br><small>In machine learning, we use gradient descent to minimize empirical risk when we can't minimize it by hand, which is true in most, more sophisticated cases.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a65667",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c1908a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In practice, we typically don't implement gradient descent ourselves ‚Äì we rely on existing implementations of it. But, we'll implement it here ourselves to understand what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0634b0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's start with an initial guess $w^{(0)} = 0$ and a learning rate $\\alpha = 0.01$.\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\alpha \\frac{df}{dw}(w^{(t)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170e4f4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeeaa33",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We see that pretty quickly, $w^{(t)}$ converges to $-0.727$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a5d2a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 0, \\alpha = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41a934",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=0, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c658563",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 1.1, \\alpha = 0.01$\n",
    "\n",
    "What if we start with a different initial guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14730079",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=1.1, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff87b9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 0, \\alpha = 0.1$\n",
    "\n",
    "What if we use a different learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d92a18",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=0, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b7f8d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 0, \\alpha = 1$\n",
    "\n",
    "Some learning rates are so large that the values of $w$ explode towards infinity! Watch what happens when we use a learning rate of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f49b95",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w = 0\n",
    "for t in range(50):\n",
    "    print(round(w, 4), round(util.f(w), 4))\n",
    "    w = w - 1 * util.df(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c764d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent and empirical risk minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6feb1ab",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- While gradient descent can minimize other kinds of differentiable functions, its most common use case is in **minimizing empirical risk**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204953c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, consider:\n",
    "    - The constant model, $H(x) = h$.\n",
    "    - The dataset $-4, -2, 2, 4$.\n",
    "    - The initial guess $h_0 = 4$ and the learning rate $\\alpha = \\frac{1}{4}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3db9f6c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Exercise**: Find $h_1$ and $h_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e7540",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- See the annotated slides for the solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629e1f8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3269e99",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lingering questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e786d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When is gradient descent _guaranteed_ to converge to a global minimum? What kinds of functions work well with gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5de5d4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do we choose a step size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed7944",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do we use gradient descent to minimize functions of multiple variables, e.g.:\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7c22a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: Why **can't** we use gradient descent to find $\\vec{w}_\\text{LASSO}^*$?\n",
    "\n",
    "$$R_\\text{LASSO}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d |w_d|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6506e656",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When is gradient descent guaranteed to work?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d2293",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What makes a function convex?\n",
    "\n",
    "<div style=\"width: 100%;\">\n",
    "<div style=\"width: 50%; float: left\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"imgs/convex.png\">\n",
    "<center>A <b>convex</b> function ‚úÖ.</center>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 50%\" markdown=\"1\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"imgs/non-convex.png\">\n",
    "<center>A <b>non-convex</b> function ‚ùå.</center>\n",
    "\n",
    "</div/>\n",
    "</div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c346e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intuitive definition of convexity\n",
    "\n",
    "<div style=\"width: 100%;\">\n",
    "<div style=\"width: 50%; float: left\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/convex.png\" width=70%></center>\n",
    "<center>A <b>convex</b> function ‚úÖ.</center>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 50%\" markdown=\"1\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/non-convex.png\" width=70%></center>\n",
    "<center>A <b>non-convex</b> function ‚ùå.</center>\n",
    "\n",
    "</div/>\n",
    "</div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0511f1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A function $f$ is **convex** if, for **every** $a, b$ in the domain of $f$, the line segment between:\n",
    "\n",
    "  $$(a, f(a)) \\text{ and } (b, f(b))$$\n",
    "\n",
    "  does not go below the plot of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048d157",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Formal definition of convexity\n",
    "\n",
    "<div style=\"width: 100%;\">\n",
    "<div style=\"width: 55%; float: left\"> \n",
    "\n",
    "- A function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is **convex** if, for **every** $a, b$ in the domain of $f$, and for every $t \\in [0, 1]$:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\boxed{(1 - t) f(a) + t f(b) \\geq f((1-t)a + tb)}$$\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "- This is a formal way of restating the definition from the previous slide.\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 57%\" markdown=\"1\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"imgs/convex-definition.png\" width=100%>\n",
    "\n",
    "</center>\n",
    "\n",
    "</div/>\n",
    "</div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f4464",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's an interactive version of the formal definition of convexity from the previous slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71627e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interact(util.convexity_visual, a=(-20, 5, 0.1), b=(5, 20, 0.1), t=FloatSlider(min=0, max=1, step=0.01, value=0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f3cab",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "### Activity\n",
    "    \n",
    "\n",
    "Which of these functions are **not** convex?\n",
    "\n",
    "- A. $f(x) = |x|$.\n",
    "- B. $f(x) = e^x$.\n",
    "- C. $f(x) = \\sqrt{x-1}$.\n",
    "- D. $f(x) = (x-3)^{24}$.\n",
    "- E. More than one of the above are non-convex.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43151ace",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Second derivative test for convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0837f4f1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $f(t)$ is a function of a single variable and is **twice** differentiable, then $f(w)$ is convex **if and only if**:\n",
    "\n",
    "$$\\frac{d^2f}{dw^2}(w) \\geq 0, \\:\\:\\: \\forall \\: w$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a7dea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: $f(x) = x^4$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9cbc8c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why does convexity matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c658887",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Convex functions are (relatively) easy to minimize with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e96c9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Theorem**: If $f(w)$ is convex and differentiable, then gradient descent converges to a **global minimum** of $f$, as long as the step size is small enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec5c13c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Why?**\n",
    "  - Gradient descent converges when the derivative is 0.\n",
    "  - For convex functions, the derivative is 0 only at one place ‚Äì the global minimum.\n",
    "  - In other words, if $f$ is convex, gradient descent won't get \"stuck\" and terminate in places that aren't global minimums (local minimums, saddle points, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d9487",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nonconvex functions and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825fb0e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We say a function is **nonconvex** if it does not meet the criteria for convexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d092042c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Nonconvex functions are (relatively) difficult to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089076e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient descent **might** still work, but it's not guaranteed to find a global minimum.\n",
    "  - We saw this at the start of the lecture, when trying to minimize $f(w) = 5w^4 - w^3 - 5w^2 + 2w - 9$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713096b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing a step size in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc78ab0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In practice, choosing a step size involves a lot of trial-and-error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f93176",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In this class, we've only touched on \"constant\" step sizes, i.e. where $\\alpha$ is a constant.\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\alpha \\frac{df}{dw}(w^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f441b4d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Remember**: $\\alpha$ is the \"step size\", but the amount that our guess for $w$ changes is $\\alpha \\frac{df}{dw}(w^{(t)})$, not just $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c75eb7e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In future courses, you may learn about \"decaying\" step sizes, where the value of $\\alpha$ decreases as the number of iterations increases.<br><small>Intuition: take much bigger steps at the start, and smaller steps as you progress, as you're likely getting closer to the minimum.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2830132",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent for empirical risk minimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93393f5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing functions of multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d016b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider the function:\n",
    "\n",
    "$$f(x_1, x_2) = (x_1-2)^2 + 2x_1 - (x_2-3)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068536fd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It has two **partial derivatives**: $\\frac{\\partial f}{\\partial x_1}$ and $\\frac{\\partial f}{\\partial x_2}$.<br><small>See the annotated slides for what they are and how we find them.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be78413",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The gradient vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fc5c6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $f(\\vec{x})$ is a function of multiple variables, then its **gradient**, $\\nabla f (\\vec{x})$, is a vector containing its partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90562e74",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: \n",
    "\n",
    "$$f(\\vec{x}) = (x_1-2)^2 + 2x_1 - (x_2-3)^2$$\n",
    "\n",
    "$$\\nabla f(\\vec{x}) = \\begin{bmatrix} 2(x_1 - 1) \\\\ -2(x_2 - 3)  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9795d54",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example:\n",
    "\n",
    "$$f(\\vec{x}) = \\vec{x}^T \\vec{x}$$\n",
    "\n",
    "$$\\nabla f(\\vec{x}) = 2 \\vec{x}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12f02b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"imgs/surface.png\" width=70%>\n",
    "    \n",
    "At any given point, there are many directions in which you can go \"up\", but there's only one \"steepest direction up\", and that's the direction of the gradient!\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230cead",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for functions of multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad64fd26",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: \n",
    "\n",
    "$$f(x_1, x_2) = (x_1-2)^2 + 2x_1 - (x_2-3)^2$$\n",
    "\n",
    "$$\\nabla f (\\vec{x}) = \\begin{bmatrix} 2(x_1 - 1) \\\\ -2(x_2 - 3)  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7e64f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The minimizer of $f$ is a vector, $\\vec{x}^* = \\begin{bmatrix} x_1^* \\\\ x_2^* \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b876b2b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We start with an initial guess, $\\vec{x}^{(0)}$, and step size $\\alpha$, and update our guesses using:\n",
    "\n",
    "$$\\vec{x}^{(t+1)} = \\vec{x}^{(t)} - \\alpha \\nabla f(\\vec{x}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ce86d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "### Activity\n",
    "\n",
    "$$f(x_1, x_2) = (x_1-2)^2 + 2x_1 - (x_2-3)^2$$\n",
    "    \n",
    "<br>\n",
    "\n",
    "$$\\nabla f (\\vec{x}) = \\begin{bmatrix} 2(x_1 - 1) \\\\ -2(x_2 - 3)  \\end{bmatrix}$$\n",
    "    \n",
    "<br>\n",
    "\n",
    "$$\\vec{x}^{(t+1)} = \\vec{x}^{(t)} - \\alpha \\nabla f(\\vec{x}^{(t)})$$\n",
    "    \n",
    "<br>\n",
    "\n",
    "Given an initial guess of $\\vec{x}^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ and a step size of $\\alpha = \\frac{1}{3}$, perform **two** iterations of gradient descent. What is $\\vec{x}^{(2)}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08048b79",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23038a4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Gradient descent for simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d2d3f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find optimal model parameters for the model $H(x) = w_0 + w_1 x$ and squared loss, we minimized empirical risk:\n",
    "\n",
    "$$R_\\text{sq}(w_0, w_1) = R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n ( y_i - (w_0 + w_1 x_i ))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7389165",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is a function of multiple variables, and is differentiable, so it has a gradient!\n",
    "\n",
    "$$\\nabla R(\\vec{w}) = \\begin{bmatrix} \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1 x_i)) \\\\ \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1x_i))x_i  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7507f6f5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key idea**: To find $\\vec{w}^* = \\begin{bmatrix} w_0^* \\\\ w_1^* \\end{bmatrix}$, we _could_ use gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0ed39",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why would we, when closed-form solutions exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fee618",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for simple linear regression, visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5fd998",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('oMk6sP7hrbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2cb5f7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for simple linear regression, implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fec83b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use gradient descent to fit a simple linear regression model to predict commute time in `'minutes'` from `'departure_hour'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ba1dd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/commute-times.csv')\n",
    "df[['departure_hour', 'minutes']]\n",
    "util.make_scatter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a571488",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = df['departure_hour']\n",
    "y = df['minutes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bded0ec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, let's remind ourselves what $w_0^*$ and $w_1^*$ are supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bdfd60",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "slope = np.corrcoef(x, y)[0, 1] * np.std(y) / np.std(x)\n",
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c25bba",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "intercept = np.mean(y) - slope * np.mean(x)\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62cb376",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08121890",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n ( y_i - (w_0 + w_1 x_i ))^2$$\n",
    "\n",
    "$$\\nabla R(\\vec{w}) = \\begin{bmatrix} \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1 x_i)) \\\\ \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1x_i))x_i  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd80e4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dR_w0(w0, w1):\n",
    "    return -2 * np.mean(y - (w0 + w1 * x))\n",
    "def dR_w1(w0, w1):\n",
    "    return -2 * np.mean((y - (w0 + w1 * x)) * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d6a56",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f12d8a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The update rule we'll follow is:\n",
    "\n",
    "$$\\vec{w}^{(t+1)} = \\vec{w}^{(t)} - \\alpha \\nabla R(\\vec{w}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef9a20",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can treat this as two separate update equations:\n",
    "\n",
    "$$w_0^{(t+1)} = w_0^{(t)} - \\alpha \\frac{\\partial R}{\\partial w_0} (\\vec{w}^{(t)}) \\\\ w_1^{(t+1)} = w_1^{(t)} - \\alpha \\frac{\\partial R}{\\partial w_1} (\\vec{w}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f54c75",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's initialize $w_0^{(0)} = 100$ and $w_1^{(0)} = -50$, and choose the step size $\\alpha = 0.01$.<br><small>The initial guesses were just parameters that we thought might be close.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd3086",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We'll store our guesses so far, so we can look at them later.\n",
    "def gradient_descent_for_regression(w0_initial, w1_initial, alpha, threshold=0.0001):\n",
    "    w0, w1 = w0_initial, w1_initial\n",
    "    w0_history = [w0]\n",
    "    w1_history = [w1]\n",
    "    while True:\n",
    "        w0 = w0 - alpha * dR_w0(w0, w1)\n",
    "        w1 = w1 - alpha * dR_w1(w0, w1)\n",
    "        w0_history.append(w0)\n",
    "        w1_history.append(w1)\n",
    "        if np.abs(w0_history[-1] - w0_history[-2]) <= threshold:\n",
    "            break\n",
    "    return w0_history, w1_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51627696",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w0_history, w1_history = gradient_descent_for_regression(0, 0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21151b7",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w0_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d17aaed",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w1_history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b586673",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that we converge at the right value! But how many iterations did it take? What could we do to speed it up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c5581",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(w0_history)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
