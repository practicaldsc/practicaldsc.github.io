{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74cbe7c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *\n",
    "import lec23_util as util\n",
    "from IPython.display import YouTubeVideo\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcedae7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 23\n",
    "\n",
    "# Classification\n",
    "\n",
    "### EECS 398-003: Practical Data Science, Fall 2024\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/fa24\">github.com/practicaldsc/fa24</a></small>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    "  MathJax.Hub.Config({\n",
    "    TeX: {\n",
    "      extensions: [\"color.js\"],\n",
    "      packages: {\"[+]\": [\"color\"]},\n",
    "    }\n",
    "  });\n",
    "  </script>\n",
    "  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b168523",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements üì£\n",
    "\n",
    "- The Portfolio Homework's checkpoint is due on **Monday, November 25th** ‚Äì no slip days allowed!<br><small>The full homework is due on **Saturday, December 7th** (no slip days!).</small>\n",
    "- Homework 10 will be out by tomorrow ‚Äì sorry for the delay!<br><small>We'll adjust the deadline accordingly.</small>\n",
    "- The [**Grade Report**](https://www.gradescope.com/courses/823979/assignments/5191081) now includes scores and slip days through Homework **9** ‚Äì make sure it's accurate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe7201",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Recap: Gradient descent for multivariate functions.\n",
    "- Classification overview.\n",
    "- Survey of classification methods.\n",
    "    - $k$-Nearest Neighbors  üè°üè†.\n",
    "    - Decision trees üéÑ.\n",
    "    - Logistic regression üìà.\n",
    "- Evaluating classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3bab2d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb860ca",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Gradient descent for multivariate functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e9cde7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Gradient descent for simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918a022",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find optimal model parameters for the model $H(x) = w_0 + w_1 x$ and squared loss, we minimized empirical risk:\n",
    "\n",
    "$$R_\\text{sq}(w_0, w_1) = R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n ( y_i - (w_0 + w_1 x_i ))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8219d3c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is a function of multiple variables, and is differentiable, so it has a gradient!\n",
    "\n",
    "$$\\nabla R(\\vec{w}) = \\begin{bmatrix} \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1 x_i)) \\\\ \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1x_i))x_i  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fd482",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key idea**: To find $\\vec{w}^* = \\begin{bmatrix} w_0^* \\\\ w_1^* \\end{bmatrix}$, we _could_ use gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822e3fb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why would we, when closed-form solutions exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71157d9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"imgs/surface.png\" width=700><br>At any point, there are many directions in which you can go \"up\", but there's only one \"steepest direction up\", and that's the direction of the gradient!</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec6263",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for simple linear regression, visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57675a20",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('oMk6sP7hrbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db807977",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for simple linear regression, implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d20d65",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use gradient descent to fit a simple linear regression model to predict commute time in `'minutes'` from `'departure_hour'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d79439",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/commute-times.csv')\n",
    "df[['departure_hour', 'minutes']]\n",
    "util.make_scatter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5264cc3e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = df['departure_hour']\n",
    "y = df['minutes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd0b26c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, let's remind ourselves what $w_0^*$ and $w_1^*$ are supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f461a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "slope = np.corrcoef(x, y)[0, 1] * np.std(y) / np.std(x)\n",
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34d075",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "intercept = np.mean(y) - slope * np.mean(x)\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054d8ac",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e43b5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n ( y_i - (w_0 + w_1 x_i ))^2$$\n",
    "\n",
    "$$\\nabla R(\\vec{w}) = \\begin{bmatrix} \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1 x_i)) \\\\ \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1x_i))x_i  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ceeea0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dR_w0(w0, w1):\n",
    "    return -2 * np.mean(y - (w0 + w1 * x))\n",
    "def dR_w1(w0, w1):\n",
    "    return -2 * np.mean((y - (w0 + w1 * x)) * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da851b0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd569f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The update rule we'll follow is:\n",
    "\n",
    "$$\\vec{w}^{(t+1)} = \\vec{w}^{(t)} - \\alpha \\nabla R(\\vec{w}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb1a1f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can treat this as two separate update equations:\n",
    "\n",
    "$$w_0^{(t+1)} = w_0^{(t)} - \\alpha \\frac{\\partial R}{\\partial w_0} (\\vec{w}^{(t)}) \\\\ w_1^{(t+1)} = w_1^{(t)} - \\alpha \\frac{\\partial R}{\\partial w_1} (\\vec{w}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a96390",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's initialize $w_0^{(0)} = 100$ and $w_1^{(0)} = -50$, and choose the step size $\\alpha = 0.01$.<br><small>The initial guesses were just parameters that we thought might be close.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cced64b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We'll store our guesses so far, so we can look at them later.\n",
    "def gradient_descent_for_regression(w0_initial, w1_initial, alpha, threshold=0.0001):\n",
    "    w0, w1 = w0_initial, w1_initial\n",
    "    w0_history = [w0]\n",
    "    w1_history = [w1]\n",
    "    while True:\n",
    "        w0 = w0 - alpha * dR_w0(w0, w1)\n",
    "        w1 = w1 - alpha * dR_w1(w0, w1)\n",
    "        w0_history.append(w0)\n",
    "        w1_history.append(w1)\n",
    "        if np.abs(w0_history[-1] - w0_history[-2]) <= threshold:\n",
    "            break\n",
    "    return w0_history, w1_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb6d51",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w0_history, w1_history = gradient_descent_for_regression(0, 0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2be6c5",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w0_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150fea5e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w1_history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ab588",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that we converge at the right value! But how many iterations did it take? What could we do to speed it up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d3553",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(w0_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce6bbf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification overview\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e3893",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The taxonomy of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca2fe5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So far, we've focused on building **regression** models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131c83c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Regression is a form of **supervised learning**, in which the target variable (i.e., the $y$-values we're trying to predict) is **numerical**.<br><small>For example, a predicted commute time could technically be any real number.</small>\n",
    "\n",
    "<center><img src=\"imgs/taxonomy.svg\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfcbfe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Next, we'll focus on **classification**, a form of supervised learning in which the target variable is **categorical**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c51f1a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5432b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Does this person have diabetes?<br><small>This is an example of **binary classification** ‚Äì there are only two possible classes, or categories. In binary classification, the two classes are typically **1** (yes) and **0** (no).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d56fe5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Is this digit a 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9?<br><small>This is an example of multi-class classification, where there are multiple possible classes.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f780e03",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Will Michigan win this week?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2542dac1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Is this picture of a dog, cat, zebra, or hamster?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba9a43",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39330e31",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When we introduced regression, we **started** by understanding the theoretical foundations on paper, and then learned how to build models in `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39136269",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This time, we'll do the reverse: we'll start by learning how to use classifiers in `sklearn`, and then over the next few lectures, we'll dive deeper into the internals of a few.\n",
    "    - $k$-Nearest Neighbors.\n",
    "    - Decision trees.\n",
    "    - Logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d3146",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb87060",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our first classification example will involve predicting whether or not a patient has diabetes, given other information about their health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430c3fd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "display_df(diabetes, cols=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c80ff",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 0 means no diabetes, 1 means yes diabetes.\n",
    "diabetes['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa001a39",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `'Glucose'` is measured in mg/dL (milligrams per deciliter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed029c02",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `'BMI'` is calculated as $\\text{BMI} = \\frac{\\text{weight (kg)}}{\\left[ \\text{height (m)} \\right]^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2bd987",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's start by using `'Glucose'` and `'BMI'` to predict whether or not a patient has diabetes (`'Outcome'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ba732",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But first, a train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb30f18",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(diabetes[['Glucose', 'BMI']], diabetes['Outcome'], random_state=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3c31a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f04fa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's visualize the relationship between `X_train` and `y_train`. There are three numeric variables at play here ‚Äì `'Glucose'`, `'BMI'`, and `'Outcome'` ‚Äì so we can use a 3D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9aaab6",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "px.scatter_3d(X_train.assign(Outcome=y_train), \n",
    "              x='Glucose', y='BMI', z='Outcome', \n",
    "              title='Relationship between Glucose, BMI, and Diabetes',\n",
    "              width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b78edc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since there are only two possible `'Outcome'`s, we can draw a 2D scatter plot of `'BMI'` vs. `'Glucose'` and color each point by `'Outcome'`. Below, <span style='color: orange'><b>class 0 (orange) is \"no diabetes\"</b></span> and <span style='color: blue'><b>class 1 (blue) is \"diabetes\"</b></span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa64cc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = (\n",
    "    X_train.assign(Outcome=y_train.astype(str).replace({'0': 'no diabetes', '1': 'yes diabetes'}))\n",
    "            .plot(kind='scatter', x='Glucose', y='BMI', color='Outcome', \n",
    "                  color_discrete_map={'no diabetes': 'orange', 'yes diabetes': 'blue'},\n",
    "                  title='Relationship between Glucose, BMI, and Diabetes')\n",
    "            .update_layout(width=800)\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d825803b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Using this dataset, how can we classify whether someone (not already in the dataset) has diabetes, given their `'Glucose'` and `'BMI'`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3755b9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Intuition**: If a new person's feature vector is <b><span style=\"color:blue\">close to the blue points</span></b>, we'll predict <b><span style=\"color:blue\">blue (diabetes)</span></b>; if they're <b><span style=\"color:orange\">close to the orange points</span></b>, we'll predict <b><span style=\"color:orange\">orange (no diabetes)</span></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2425c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifier 1: $k$-Nearest Neighbors  üè°üè†\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996bb7cd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-Nearest Neighbors üè°üè†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3237d3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're given a new individual, $\\vec{x}_\\text{new} = \\begin{bmatrix} \\text{Glucose}_\\text{new} \\\\ \\text{BMI}_\\text{new} \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f38ac",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $k$-Nearest Neighbors classifier ($k$-NN for short) classifies $\\vec{x}_\\text{new}$ by:\n",
    "    1. Finding the $k$ **closest points** in the training set to $\\vec{x}_\\text{new}$.\n",
    "    1. Predicting that $\\vec{x}_\\text{new}$ belongs to the **most common class** among those $k$ closest points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d3f5f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7badb7a8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: Suppose $k = 6$. If, among the 6 closest points to $\\vec{x}_\\text{new}$, there are <b><span style=\"color:blue\">4 blue</span></b> and <b><span style=\"color:orange\">2 orange</span></b> points, we'd predict <b><span style=\"color:blue\">blue (diabetes)</span></b>.\n",
    "<br><small>What if there are ties? Read [here](https://stats.stackexchange.com/questions/144718/how-does-scikit-learn-resolve-ties-in-the-knn-classification).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c9e148",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $k$ is a hyperparameter that should be chosen through cross-validation.<br><small>As we've seen in Homework 9 (and 10!) in the context of $k$-NN regression, smaller values of $k$ tend to overfit significantly.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bebc45d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `KNeighborsClassifier` in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c253ca",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccecf0d2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's fit a `KNeighborsClassifier` by using cross-validation to choose a value of $k$ from 1 through 50.<br><small>Note that `KNeighborsClassifier`s have several other hyperparameters. One of them is the metric used to measure distances; the default is the standard Euclidean (Pythagorean) distance, e.g. $\\text{dist}(\\vec u, \\vec v) = \\sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2 + ... + (u_d - v_d)^2}$.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0aa303",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid = {'n_neighbors': range(1, 51)}\n",
    ")\n",
    "model_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9ed72",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_knn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f0814",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Cross-validation chose $k = 28$. With the resulting model, we can make predictions using the `predict` method, just like with regressors.<br><small>Note that all of the work in making the prediction ‚Äì finding the 28 nearest neighbors, for instance ‚Äì is done when we call `predict`. \"Training\" does very little.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58653190",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# To know what reasonable values for 'Glucose' and 'BMI' might be, let's look at the plot again.\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac7c65",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_knn.predict(pd.DataFrame([{\n",
    "    'Glucose': 125,\n",
    "    'BMI': 40\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24194c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does the resulting model **look like**? Can we visualize it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7203453e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f62a67",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **decision boundaries** of a classifier visualize the regions in the feature space that separate different predicted classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1d7e2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The decision boundaries for `model_knn` are visualized below.<br><small>If a new person's feature vector lies in the <b><span style=\"color:blue\">blue region</span></b>, we'd predict they <b><span style=\"color:blue\">do have diabetes</span></b>, <b><span style=\"color:orange\">otherwise</span></b>, we'd predict <b><span style=\"color:orange\">they don't</span></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f2555",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_decision_boundary(model_knn, X_train, y_train, title='Decision Boundary when $k = 28$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab23c69d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What would the decision boundaries look like if $k$ increased or decreased?<br><small>Play with the slider below to find out!</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cc72b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "interact(lambda k: util.visualize_k(k, X_train, y_train), k=(1, 51));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e409b64",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What if $k = n$, the number of points in the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ecf5c7",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.visualize_k(576, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d70b27",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quantifying the performance of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa83882",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For regression models, our default evaluation metric was **mean squared error**.<br><small>Error is bad, so **lower** values indicate **better** model performance.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff78de",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The most common evaluation metric in classification is **accuracy**:\n",
    "\n",
    "    $$\\text{accuracy} = \\frac{\\text{# data points classified correctly}}{\\text{# data points}}$$\n",
    "    \n",
    "    Accuracy ranges from 0 to 1, i.e. 0% to 100%. **Higher** values indicate **better** model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad4eab",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Equivalent to 75%.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e949679",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is the default metric that the `score` method of a classifier computes, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576110f2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca993eed",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For future reference.\n",
    "test_scores = pd.Series()\n",
    "...\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0237388d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Accuracy is **not** the only metric we care about, and can sometimes be misleading. More on this soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab31fd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "### Activity\n",
    "    \n",
    "It seems that a $k$-NN classifier that uses $k = 1$ should achieve 100% training accuracy. Why **doesn't** the model defined below have 100% training accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8210cf4f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_k1 = KNeighborsClassifier(n_neighbors=1)\n",
    "model_k1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd012f0e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training accuracy ‚Äì high, but not 100%.\n",
    "model_k1.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79d585",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Accuracy on test set is lower than when k = 28!\n",
    "model_k1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd8ccd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_scores['knn with k = 1'] = model_k1.score(X_test, y_test)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4211d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "### Discussion\n",
    "    \n",
    "Why should we generally **standardize** features before using a $k$-NN classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f7789",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled['Glucose * 2'] = X_train_scaled['Glucose'] * 2\n",
    "(\n",
    "    X_train_scaled.assign(Outcome=y_train.astype(str).replace({'0': 'no diabetes', '1': 'yes diabetes'}))\n",
    "    .plot(kind='scatter', x='Glucose * 2', y='BMI', color='Outcome', \n",
    "                  color_discrete_map={'no diabetes': 'orange', 'yes diabetes': 'blue'},\n",
    "                  title='Relationship between Glucose * 2, BMI, and Diabetes')\n",
    "            .update_layout(width=1300)\n",
    "            .update_xaxes(tickvals=np.arange(0, 500, 100))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e787863",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d4c1ce",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parametric vs. non-parametric models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec55ad1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $k$-Nearest Neighbors classifier is an example of a **non-parametric** machine learning method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c153e0b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Linear regression, on the other hand, is **parametric**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe2f91",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Some differences between parametric and non-parametric models:\n",
    "\n",
    "| Parametric | Non-Parametric |\n",
    "| --- | --- |\n",
    "| There's a fixed set of coefficients (parameters), $w_0, w_1, ..., w_d$ that we'll use for making predictions, and the number of coefficients is independent of the training set size. | No fixed set of parameters; model complexity increases as the training set size increases. |\n",
    "| Parametric methods make assumptions about the shape of the data and/or its underlying probability distribution.<br><small>For instance, linear models assume a linear relationship between the features $X$ and target $\\vec{y}$.<br>There's a connection between the squared loss function and maximum likelihood estimation, too.</small> | Non-parametric methods make no assumptions about the shape of the data. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319271b8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifier 2: Decision trees üéÑ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef5ed5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision trees üéÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23702ce0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're given a new individual, $\\vec{x}_\\text{new} = \\begin{bmatrix} \\text{Glucose}_\\text{new} \\\\ \\text{BMI}_\\text{new} \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b5b0f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The decision tree classifier classifies $\\vec{x}_\\text{new}$ by:\n",
    "    1. Asking a series of yes/no questions about $\\text{Glucose}_\\text{new}$ and $\\text{BMI}_\\text{new}$, e.g.:\n",
    "    <br>\n",
    "    <center>Is $\\text{Glucose}_\\text{new} \\leq 129.5$?<br>If so, is $\\text{BMI}_\\text{new} \\leq 26.3$?\n",
    "    <br>If not, is $\\text{BMI}_\\text{new} \\leq 29.95$?<br>$\\vdots$</center>\n",
    "    2. Once it runs out of questions to ask, it predicts that $\\vec{x}_\\text{new}$ belongs to the **most common class** among training set points that had the same answers as $\\vec{x}_\\text{new}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f3667",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Visually, a fit decision tree may look like:\n",
    "\n",
    "<center><img src=\"imgs/example-dt.png\" width=500</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f7465",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Decision trees are also **non-parametric**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65be3ee",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `DecisionTreeClassifier` in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524cd3ee",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef89978",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's fit a `DecisionTreeClassifier`.<br><small>One of the main hyperparameters is `max_depth`, the number of questions to ask before making a prediction. Typically, we fit this with cross-validation, but for now we'll hard-code it.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d345a77",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeClassifier(max_depth=3)\n",
    "model_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487bc30",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The decision tree achieves a slightly higher test set accuracy than the cross-validated $k$-NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1e758",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3bc6cd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_scores['decision tree with depth = 3'] = model_tree.score(X_test, y_test)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b0aa1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But what does it **look like**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d7948",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision boundaries for a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b5b133",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_decision_boundary(model_tree, X_train, y_train, title='Decision Boundary for a Tree of Depth 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786fa5d7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Observe that the decision boundaries ‚Äì at least when we set `max_depth` to 3 ‚Äì look less \"jagged\" than with the $k$-NN classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c23154",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c26e3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our fit decision tree is like a \"flowchart\", made up of a series of questions.<br><small>It turns out `sklearn` provides us with a convenient way of visualizing this flowchart.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff0a40",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As before, <span style='color: orange'><b>orange is \"no diabetes\"</b></span> and <span style='color: blue'><b>blue  is \"diabetes\"</b></span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5890eb",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(13, 5))\n",
    "plot_tree(model_tree, feature_names=X_train.columns, class_names=['no db', 'yes db'], \n",
    "          filled=True, fontsize=10, impurity=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99224e93",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To **classify a new data point**, we start at the top and answer the first question (i.e. \"Glucose <= 129.5\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c443152",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the answer is \"**Yes**\", we move to the **left** branch, otherwise we move to the right branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660779ff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We repeat this process until we end up at a leaf node, at which point we predict the most common class in that node.<br><small>Note that each node has a `value` attribute, which describes the number of **training** individuals of each class that fell in that node.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003f8f4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0baf4f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Increasing tree depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df8a35",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One of the many hyperparameters we can tune is tree depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a9d302",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What happens to the decision boundary of the resulting classifier if we increase `max_depth`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d510258",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interact(lambda depth: util.visualize_depth(depth, X_train, y_train), depth=(1, 51));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd044c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What happens to the flowchart representation of the resulting classifier if we increase `max_depth`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fbac31",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# By default, there is pre-specified maximum depth.\n",
    "# The training algorithm keeps \n",
    "model_tree_no_max = DecisionTreeClassifier()\n",
    "model_tree_no_max.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364aa03a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Uncomment this!\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plot_tree(model_tree_no_max, feature_names=X_train.columns, class_names=['no db', 'yes db'], \n",
    "#           filled=True, fontsize=10, impurity=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9b6de",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The tree is **extremely overfit** to the training set, and very deep!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad06c4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training accuracy. This number should look familiar!\n",
    "model_tree_no_max.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168298f2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_tree_no_max.tree_.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7ea0e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Worse test set performance than when we used max_depth = 3!\n",
    "test_scores['decision tree with no specified max depth'] = model_tree_no_max.score(X_test, y_test)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103dc0b1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lingering questions about decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30859075",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **How** are decision trees fit ‚Äì that is, how do they decide what questions to ask?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a75213",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Other than limiting the depth of a decision tree, how else can we scale back the complexity of a decision tree (to hopefully help with model generalizability)? What is a **random forest**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258890fe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can they be used for **regression**, too?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360794a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll address these ideas **next week**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0f5a5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "### Activity\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><img src=\"imgs/chicken-class.png\" width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c40ff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifier 3: Logistic regression üìà \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261e937",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression üìà "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf5fec0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Logistic **regression** is a linear **classification** technique that builds upon linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d9a79",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It models **the probability of belonging to class 1, given a feature vector**:\n",
    "    \n",
    "$$P(y = 1 | \\vec{x}) = \\sigma (\\underbrace{w_0 + w_1 x^{(1)} + w_2 x^{(2)} + ... + w_d x^{(d)}}_{\\text{linear regression model}}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}) \\right)$$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c2d2e9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ is the **sigmoid** function; its outputs are between 0 and 1, which means they can be interpreted as probabilities.<br><small>The predictions of a \"regular\" linear regression model can be anything from $-\\infty$ to $+\\infty$, meaning they can't be interpreted as probabilities.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f6f03",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the existence of coefficients, $w_0, w_1, ... w_d$, that we need to learn from the data, tells us that logistic regression is a **parametric** method!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c091c8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting probabilities vs. predicting classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ce4e3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$P(y = 1 | \\vec{x}) = \\sigma (\\underbrace{w_0 + w_1 x^{(1)} + w_2 x^{(2)} + ... + w_d x^{(d)}}_{\\text{linear regression model}}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}) \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa312f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ü§î **Question**: Suppose our logistic regression model predicts the probability that someone has diabetes is 0.75. What do we predict ‚Äì diabetes or no diabetes? What if the predicted probability is 0.3?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a328c9d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üôã **Answer**: We have to pick a threshold (for example, 0.5)!\n",
    "    - If the predicted probability is above the threshold, we predict diabetes (1).\n",
    "    - Otherwise, we predict no diabetes (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcca7e6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d5846",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The sigmoid function, also known as the **logistic** function, resembles an $S$-shape.\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + e^{-t}} = \\frac{1}{1 + \\text{exp}(-t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc67878",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Below, we'll look at the shape of $y = \\sigma(w_0 + w_1 x)$ for different values of $w_0$ and $w_1$.\n",
    "    - $w_0$ controls the position of the curve on the $x$-axis.\n",
    "    - $w_1$ controls the \"steepness\" of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee32f8",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_three_sigmoids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f42fb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Below, interact with the sliders to change the values of $w_0$ and $w_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0bfe0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interact(util.plot_sigmoid, w0=(-15, 15), w1=(-15, 15));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2299b3b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LogisticRegression` in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb348370",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18882aca",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's fit a `LogisticRegression` classifier. Specifically, this means we're asking `sklearn` to learn the optimal parameters $w_0^*$, $w_1^*$, and $w_2^*$ in:\n",
    "\n",
    "$$P(y = 1 | \\vec{x}) = \\sigma \\left( w_0 + w_1 \\cdot \\text{Glucose} + w_2 \\cdot \\text{BMI} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907d1ec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The most common loss function for logistic regression **isn't** squared loss, rather it's **cross-entropy** loss (also known as **log loss**); more on this next class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a8f48",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, `sklearn` uses $L_2$ regularization for logistic regression. It doesn't cross-validate for $\\lambda$ unless we tell it to; by default, it uses $\\lambda = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35228e0e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression()\n",
    "model_logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4209c22",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The test accuracy, without any cross-validation for the regularization hyperparameter, is about the same as the other not-super-overfit models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8faa8ce",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591495d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_scores['logistic regression'] = model_logistic.score(X_test, y_test)\n",
    "test_scores.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4bbf37",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But again, we should ask, what does it **look like**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1608ec1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting probabilities vs. predicting classes, revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e26f9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, the `predict` method of a fit `LogisticRegression` model predicts a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d1a60",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.predict(pd.DataFrame([{\n",
    "    'Glucose': 125,\n",
    "    'BMI': 40\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb6b9f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, logistic regression is designed to predict **probabilities**. We can access these predicted probabilities using the `predict_proba` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480d552",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.predict_proba(pd.DataFrame([{\n",
    "    'Glucose': 125,\n",
    "    'BMI': 40\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab45e9a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The above is telling us that the model thinks this person has:\n",
    "    - A 52% chance of belonging to class 0 (no diabetes).\n",
    "    - A 48% chance of belonging to class 1 (diabetes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220bf6ff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, it uses a threshold of 0.5, i.e. it predicts the larger probability. As we'll soon discuss, this may not be what we want!<br><small>Unfortunately, `sklearn` doesn't let us change the threshold ourselves. If we want a different threshold, we need to manually implement it using the results of `predict_proba`.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a301f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision boundaries for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601e188",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_decision_boundary(model_logistic, X_train, y_train, title='Decision Boundary for Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae0f971",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Unlike other models, this decision boundary is linear! Next class, we'll see how to find the equation of the line that separates the two classes, and how it relates to the coefficients and intercept below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad613e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.intercept_, model_logistic.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899aaef8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But where does the sigmoid curve $\\sigma(t)$ appear, in the context of making predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2591aef",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the probability of belonging to <span style=\"color:blue\">class 1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a91565",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall, the logistic regression model is trained to predict the probability of <b><span style=\"color:blue\">class 1 (diabetes)</span></b>.\n",
    "\n",
    "$$P(y = 1 | \\vec{x}) = \\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose} + w_2^* \\cdot \\text{BMI} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c418d5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The graph below shows the predicted probabilities of <b><span style=\"color:blue\">class 1 (diabetes)</span></b> for different combinations of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef376823",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic(model_logistic, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e4708c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Play with the slider below to change the **threshold** that's used to classify an individual as <b><span style=\"color:blue\">class 1 (diabetes)</span></b> or <b><span style=\"color:orange\">class 0 (no diabetes)</span></b>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcbe05e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interact(lambda t: util.show_logistic(model_logistic, X_train, y_train, show_threshold=True, t=t), t=(0, 1, 0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297397bb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The introduction of the threshold creates a decision boundary, which is reflected in the 2D plot we saw a few slides ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18793cd5",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_decision_boundary(model_logistic, X_train, y_train, title='Decision Boundary for Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa2cf3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lingering questions about logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a16f388",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What **loss function** do we use to find optimal model parameters for logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef19252",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do we interpret the resulting **coefficients**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b010bb1c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What **assumptions** does the logistic regression model make?<br><small>The linear regression model assumes the output is a linear combination of features. Part of the logistic regression model resembles the linear regression model, so presumably, **something** in logistic regression is a linear combination of features, but what?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640b7bf2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll address these ideas **next class**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1203bc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifier evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc9a5e8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accuracy isn't everything!\n",
    "\n",
    "$$\n",
    "\\text{accuracy} = \\frac{\\text{# data points classified correctly}}{\\text{# data points}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5be11",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Accuracy is defined as the proportion of predictions that are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccad231",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It weighs all **correct** predictions the same, and weighs all **incorrect** predictions the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d6f37",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But some incorrect predictions may be worse than others!\n",
    "    - Suppose you take a COVID test ü¶†. Which is worse:\n",
    "        - The test **saying you have COVID**, when you really don't, or\n",
    "        - The test **saying you don't have COVID**, when you really do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd8b76",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outcomes in binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fcb95a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When performing **binary** classification, there are four possible outcomes.<br><small>Note: A \"positive prediction\" is a prediction of 1, and a \"negative prediction\" is a prediction of 0.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad73f3b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|Outcome of Prediction|Definition|True Class|\n",
    "|---|---|---|\n",
    "|**True** positive (TP) ‚úÖ|The predictor **correctly** predicts the positive class.|P|\n",
    "|False negative (FN) ‚ùå|The predictor incorrectly predicts the negative class.|P|\n",
    "|**True** negative (TN) ‚úÖ|The predictor **correctly** predicts the negative class.|N|\n",
    "|False positive (FP) ‚ùå|The predictor incorrectly predicts the positive class.|N|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b285e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We typically organize the four quantities above into a **confusion matrix**.\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN ‚úÖ | FP ‚ùå |\n",
    "| **Actually Positive** | FN ‚ùå | TP ‚úÖ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11820b12",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that in the four acronyms ‚Äì TP, FN, TN, FP ‚Äì the **first letter** is whether the prediction is correct, and the **second letter** is what the prediction is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f026e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: COVID testing ü¶†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e34cf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Michigan Medicine administers hundreds of COVID tests a day. The tests are not fully accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de774c2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each test comes back either:\n",
    "    - positive, indicating that the individual has COVID, or\n",
    "    - negative, indicating that the individual does not have COVID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705cf20",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question:** What is a TP in this scenario? FP? TN? FN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad413ece",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **TP:** The test predicted that the individual has COVID, and they do ‚úÖ. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df5d55",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **FP:** The test predicted that the individual has COVID, but they don't ‚ùå."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e871d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **TN:** The test predicted that the individual doesn't have COVID, and they don't ‚úÖ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f3143",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **FN:** The test predicted that the individual doesn't have COVID, but they do ‚ùå."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e968f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accuracy of COVID tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f636f2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The results of 100 Michigan Medicine COVID tests are given below.\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 90 ‚úÖ | FP = 1 ‚ùå |\n",
    "| **Actually Positive** | FN = 8 ‚ùå | TP = 1 ‚úÖ |\n",
    "\n",
    "<center><i><small>Michigan Medicine test results.</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591e240",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ü§î **Question:** What is the accuracy of the test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f558d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **üôã Answer:** $$\\text{accuracy} = \\frac{TP + TN}{TP + FP + FN + TN} = \\frac{1 + 90}{100} = 0.91$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784499da",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Followup:** At first, the test seems good. But, suppose we build a classifier that predicts that **nobody has COVID**. What would its accuracy be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ad034",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Answer to followup:** Also 0.91! There is severe **class imbalance** in the dataset, meaning that most of the data points are in the same class (no COVID). Accuracy doesn't tell the full story."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b269e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 90 ‚úÖ | FP = 1 ‚ùå |\n",
    "| <span style='color:orange'><b>Actually Positive</b></span> | <span style='color:orange'>FN = 8</span> ‚ùå | <span style='color:orange'>TP = 1</span> ‚úÖ |\n",
    "\n",
    "<center><i><small>Michigan Medicine test results</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee4733",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ü§î **Question:** What proportion of individuals who actually have COVID did the test **identify**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e589a031",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **üôã Answer:** $\\frac{1}{1 + 8} = \\frac{1}{9} \\approx 0.11$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561267f9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- More generally, the **recall** of a binary classifier is the proportion of <span style='color:orange'><b>actually positive instances</b></span> that are correctly classified. We'd like this number to be as close to 1 (100%) as possible.\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{\\text{# actually positive}} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab049b6a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To compute recall, look at the <span style='color:orange'><b>bottom (positive) row</b></span> of the above confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5344b6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall isn't everything, either!\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d040d04",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ü§î **Question:** Can you design a \"COVID test\" with perfect recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98acce32",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **üôã Answer:** Yes ‚Äì **just predict that everyone has COVID!**\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 0 ‚úÖ | FP = 91 ‚ùå |\n",
    "| <span style='color:orange'><b>Actually Positive</b></span> | <span style='color:orange'>FN = 0</span> ‚ùå | <span style='color:orange'>TP = 9</span> ‚úÖ |\n",
    "\n",
    "<center><i><small>everyone-has-COVID classifier</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b483bb3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{recall} = \\frac{TP}{TP + FN} = \\frac{9}{9 + 0} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75738e9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Like accuracy, recall on its own is not a perfect metric. Even though the classifier we just created has perfect recall, it has 91 false positives!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303dd9a7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision\n",
    "\n",
    "| | Predicted Negative | <span style='color:orange'>Predicted Positive</span> |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 0 ‚úÖ | <span style='color:orange'>FP = 91</span> ‚ùå |\n",
    "| **Actually Positive** | FN = 0 ‚ùå | <span style='color:orange'>TP = 9</span> ‚úÖ |\n",
    "\n",
    "<center><i><small>everyone-has-COVID classifier</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308dbc09",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **precision** of a binary classifier is the proportion of <span style='color:orange'><b>predicted positive instances</b></span> that are correctly classified. We'd like this number to be as close to 1 (100%) as possible.\n",
    "\n",
    "$$\\text{precision} = \\frac{TP}{\\text{# predicted positive}} = \\frac{TP}{TP + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66ba9c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To compute precision, look at the <span style='color:orange'><b>right (positive) column</b></span> of the above confusion matrix.<br><small>**Tip:** A good way to remember the difference between precision and recall is that in the denominator for üÖøÔ∏èrecision, both terms have üÖøÔ∏è in them (TP and FP).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db98c7c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the \"everyone-has-COVID\" classifier has perfect recall, but a precision of $\\frac{9}{9 + 91} = 0.09$, which is quite low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b2233",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üö® **Key idea:** There is a \"tradeoff\" between precision and recall. Ideally, you want both to be high. For a particular prediction task, one may be important than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3889a855",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Next class, we'll see how to weigh this tradeoff in the context of selecting a threshold for classification in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3784bc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision and recall\n",
    "\n",
    "<center><img src=\"imgs/Precisionrecall.svg.png\" width=30%></center>\n",
    "\n",
    "<center>(<a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">source</a>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf85c3ef",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "### Discussion\n",
    "    \n",
    "$$\\text{precision} = \\frac{TP}{TP + FP} \\: \\: \\: \\:  \\: \\: \\: \\: \\text{recall} = \\frac{TP}{TP + FN}$$\n",
    "    \n",
    "- ü§î When might high **precision** be more important than high recall?\n",
    "\n",
    "- ü§î When might high **recall** be more important than high precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f510858e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Activity</h3>\n",
    "\n",
    "\n",
    "Consider the confusion matrix shown below.\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 22 ‚úÖ | FP = 2 ‚ùå |\n",
    "| **Actually Positive** | FN = 23 ‚ùå | TP = 18 ‚úÖ |\n",
    "\n",
    "What is the accuracy of the above classifier? The precision? The recall?\n",
    "\n",
    "<br>\n",
    "\n",
    "After calculating all three on your own, click below to see the answers.\n",
    "\n",
    "<details>\n",
    "    <summary><b>üëâ Accuracy</b></summary>\n",
    "    (22 + 18) / (22 + 2 + 23 + 18) = 40 / 65\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary><b>üëâ Precision</b></summary>\n",
    "    18 / (18 + 2) = 9 / 10\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary><b>üëâ Recall</b></summary>\n",
    "    18 / (18 + 23) = 18 / 41\n",
    "</details>    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e53064",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Activity</h3>\n",
    "\n",
    "After fitting a `BillyClassifier`, we use it to make predictions on an unseen test set. Our results are summarized in the following confusion matrix.\n",
    "\n",
    "| | **Predicted Negative** | **Predicted Positive** |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | ??? | 30 |\n",
    "| **Actually Positive** | 66 | 105 |\n",
    "\n",
    "- **Part 1**: What is the recall of our classifier? Give your answer as a fraction (it does not need to be simplified).<br>\n",
    "\n",
    "- **Part 2**: The accuracy of our classifier is $\\frac{69}{117}$. How many **true negatives** did our classifier have? Give your answer as an integer.<br>\n",
    "\n",
    "- **Part 3**: True or False: In order for a binary classifier's precision and recall to be equal, the number of mistakes it makes must be an even number.<br>\n",
    "\n",
    "- **Part 4**: Suppose we are building a classifier that listens to an audio source (say, from your phone‚Äôs microphone) and predicts whether or not it is Soulja Boy‚Äôs 2008 classic ‚ÄúKiss Me thru the Phone.\" Our classifier is pretty good at detecting when the input stream is ‚ÄùKiss Me thru the Phone\", but it often incorrectly predicts that similar sounding songs are also ‚ÄúKiss Me thru the Phone.\"\n",
    "\n",
    "Complete the sentence: Our classifier has...\n",
    "- low precision and low recall.\n",
    "- low precision and high recall.\n",
    "- high precision and low recall.\n",
    "- high precision and high recall.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9dfaa4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Combining precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ff31cb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we care equally about a model's precision $PR$ and recall $RE$, we can combine the two using a single metric called the **F1-score**:\n",
    "\n",
    "$$\\text{F1-score} = \\text{harmonic mean}(PR, RE) = 2\\frac{PR \\cdot RE}{PR + RE}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b720da8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Both F1-score and accuracy are overall measures of a binary classifier's performance. But remember, accuracy is misleading in the presence of class imbalance, and doesn't take into account the kinds of errors the classifier makes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30419522",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other evaluation metrics for binary classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c759d51",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We just scratched the surface! This [excellent table from Wikipedia](https://en.wikipedia.org/wiki/Template:Diagnostic_testing_diagram) summarizes the many other metrics that exist.\n",
    "\n",
    "<center><img src='imgs/wiki-table.png' width=75%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4395a2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- If you're interested in exploring further, a good next metric to look at is **true negative rate (i.e. specificity)**, which is the analogue of recall for true negatives."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
