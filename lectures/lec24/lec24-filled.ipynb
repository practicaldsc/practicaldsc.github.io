{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9d911",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to get everything set up.\n",
    "from lec_utils import *\n",
    "import lec24_util as util\n",
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "diabetes = diabetes[(diabetes['Glucose'] > 0) & (diabetes['BMI'] > 0)]\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(diabetes[['Glucose', 'BMI']], diabetes['Outcome'], random_state=11)\n",
    ")\n",
    "from ipywidgets import interact\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a618d360",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 24\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "### EECS 398-003: Practical Data Science, Fall 2024\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/fa24\">github.com/practicaldsc/fa24</a></small>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    "  MathJax.Hub.Config({\n",
    "    TeX: {\n",
    "      extensions: [\"color.js\"],\n",
    "      packages: {\"[+]\": [\"color\"]},\n",
    "    }\n",
    "  });\n",
    "  </script>\n",
    "  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a48374",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements üì£\n",
    "\n",
    "- The Portfolio Homework's checkpoint is due on **Monday, November 25th** ‚Äì no slip days allowed!<br><small>The full homework is due on **Saturday, December 7th** (no slip days!).</small>\n",
    "\n",
    "- Homework 10 is (finally!) out, and is due on **Monday, December 2nd**.<br><small>Plan to finish it earlier, since we won't be able to offer much help over Thanksgiving.<br>There will still be a Homework 11, but it'll be max 3 questions.</small>\n",
    "\n",
    "- Consider entering the Big Ten Data Viz Championship. Submissions are due on January 15th. Read more [**here**](https://it.umich.edu/community/data-viz-championship).<br><small>Help Michigan defend its title!</small>\n",
    "\n",
    "- Enrollment begins today. Some suggested courses for next semester can be found in [**#306 on Ed**](https://edstem.org/us/courses/61012/discussion/5723634).<br><small>And please help spread the word about 398!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d1834",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Recap: Classification techniques and classifier evaluation.\n",
    "- Predicting probabilities.\n",
    "- Cross-entropy loss.\n",
    "- From probabilities to decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af735824",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44295e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Classification techniques and classifier evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb499ada",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1991bb3c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **regression** problem is one in which we're given a feature vector $\\vec{x}$ and need to predict a **real-valued** target variable, $y$.<br><small>Example: Given today's temperature, precipitation, and wind chill, what will tomorrow's high temperature be?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc0027",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **classification** problem is one in which we're given a feature vector $\\vec{x}$ and need to predict a **categorical** target variable, $y$.<br><small>Example: Given today's temperature, precipitation, and wind chill, will it snow tomorrow?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fa47d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In **binary classification**, there are only two possible values of the target variable (typically 1 and 0); in **multi-class classification**, there can be more than two possible values of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f922f9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Last class, we learned about two classification techniques:\n",
    "    - $k$-Nearest Neighbors üè°üè†.\n",
    "    - Decision trees üéÑ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f5146",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accuracy of COVID tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0b8c2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The results of 100 Michigan Medicine COVID tests are given below.\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 90 ‚úÖ | FP = 1 ‚ùå |\n",
    "| **Actually Positive** | FN = 8 ‚ùå | TP = 1 ‚úÖ |\n",
    "\n",
    "<center><i><small>Michigan Medicine test results.</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10cc904",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ü§î **Question:** What is the accuracy of the test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b72fc9c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **üôã Answer:** $$\\text{accuracy} = \\frac{TP + TN}{TP + FP + FN + TN} = \\frac{1 + 90}{100} = 0.91$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f30a307",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Followup:** At first, the test seems good. But, suppose we build a classifier that predicts that **nobody has COVID**. What would its accuracy be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf3e43",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Answer to followup:** Also 0.91! There is severe **class imbalance** in the dataset, meaning that most of the data points are in the same class (no COVID). Accuracy doesn't tell the full story."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b5d5c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 90 ‚úÖ | FP = 1 ‚ùå |\n",
    "| <span style='color:orange'><b>Actually Positive</b></span> | <span style='color:orange'>FN = 8</span> ‚ùå | <span style='color:orange'>TP = 1</span> ‚úÖ |\n",
    "\n",
    "<center><i><small>Michigan Medicine test results</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f232e1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ü§î **Question:** What proportion of individuals who actually have COVID did the test **identify**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852fd95e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **üôã Answer:** $\\frac{1}{1 + 8} = \\frac{1}{9} \\approx 0.11$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a99a7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- More generally, the **recall** of a binary classifier is the proportion of <span style='color:orange'><b>actually positive instances</b></span> that are correctly classified. We'd like this number to be as close to 1 (100%) as possible.\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{\\text{# actually positive}} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ffcb2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To compute recall, look at the <span style='color:orange'><b>bottom (positive) row</b></span> of the above confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82600517",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall isn't everything, either!\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defaf049",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ü§î **Question:** Can you design a \"COVID test\" with perfect recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d9cd1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **üôã Answer:** Yes ‚Äì **just predict that everyone has COVID!**\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 0 ‚úÖ | FP = 91 ‚ùå |\n",
    "| <span style='color:orange'><b>Actually Positive</b></span> | <span style='color:orange'>FN = 0</span> ‚ùå | <span style='color:orange'>TP = 9</span> ‚úÖ |\n",
    "\n",
    "<center><i><small>everyone-has-COVID classifier</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c6726",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{recall} = \\frac{TP}{TP + FN} = \\frac{9}{9 + 0} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e15ed3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Like accuracy, recall on its own is not a perfect metric. Even though the classifier we just created has perfect recall, it has 91 false positives!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3125034",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision\n",
    "\n",
    "| | Predicted Negative | <span style='color:orange'>Predicted Positive</span> |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 0 ‚úÖ | <span style='color:orange'>FP = 91</span> ‚ùå |\n",
    "| **Actually Positive** | FN = 0 ‚ùå | <span style='color:orange'>TP = 9</span> ‚úÖ |\n",
    "\n",
    "<center><i><small>everyone-has-COVID classifier</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c2ace",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **precision** of a binary classifier is the proportion of <span style='color:orange'><b>predicted positive instances</b></span> that are correctly classified. We'd like this number to be as close to 1 (100%) as possible.\n",
    "\n",
    "$$\\text{precision} = \\frac{TP}{\\text{# predicted positive}} = \\frac{TP}{TP + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a3d09",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To compute precision, look at the <span style='color:orange'><b>right (positive) column</b></span> of the above confusion matrix.<br><small>**Tip:** A good way to remember the difference between precision and recall is that in the denominator for üÖøÔ∏èrecision, both terms have üÖøÔ∏è in them (TP and FP).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f5207",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the \"everyone-has-COVID\" classifier has perfect recall, but a precision of $\\frac{9}{9 + 91} = 0.09$, which is quite low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79b5db",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üö® **Key idea:** There is a \"tradeoff\" between precision and recall. Ideally, you want both to be high. For a particular prediction task, one may be important than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bb403",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Later today, we'll see how to weigh this tradeoff in the context of selecting a threshold for classification in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057fc0d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision and recall\n",
    "\n",
    "<center><img src=\"imgs/Precisionrecall.svg.png\" width=30%></center>\n",
    "\n",
    "<center>(<a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">source</a>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069870d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "### Discussion\n",
    "    \n",
    "$$\\text{precision} = \\frac{TP}{TP + FP} \\: \\: \\: \\:  \\: \\: \\: \\: \\text{recall} = \\frac{TP}{TP + FN}$$\n",
    "    \n",
    "- ü§î When might high **precision** be more important than high recall?\n",
    "\n",
    "- ü§î When might high **recall** be more important than high precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6656976",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Activity</h3>\n",
    "\n",
    "\n",
    "Consider the confusion matrix shown below.\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 22 ‚úÖ | FP = 2 ‚ùå |\n",
    "| **Actually Positive** | FN = 23 ‚ùå | TP = 18 ‚úÖ |\n",
    "\n",
    "What is the accuracy of the above classifier? The precision? The recall?\n",
    "\n",
    "<br>\n",
    "\n",
    "After calculating all three on your own, click below to see the answers.\n",
    "\n",
    "<details>\n",
    "    <summary><b>üëâ Accuracy</b></summary>\n",
    "    (22 + 18) / (22 + 2 + 23 + 18) = 40 / 65\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary><b>üëâ Precision</b></summary>\n",
    "    18 / (18 + 2) = 9 / 10\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary><b>üëâ Recall</b></summary>\n",
    "    18 / (18 + 23) = 18 / 41\n",
    "</details>    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d33fea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Activity</h3>\n",
    "\n",
    "After fitting a `BillyClassifier`, we use it to make predictions on an unseen test set. Our results are summarized in the following confusion matrix.\n",
    "\n",
    "| | **Predicted Negative** | **Predicted Positive** |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | ??? | 30 |\n",
    "| **Actually Positive** | 66 | 105 |\n",
    "\n",
    "- **Part 1**: What is the recall of our classifier? Give your answer as a fraction (it does not need to be simplified).<br>\n",
    "\n",
    "- **Part 2**: The accuracy of our classifier is $\\frac{69}{117}$. How many **true negatives** did our classifier have? Give your answer as an integer.<br>\n",
    "\n",
    "- **Part 3**: True or False: In order for a binary classifier's precision and recall to be equal, the number of mistakes it makes must be an even number.<br>\n",
    "\n",
    "- **Part 4**: Suppose we are building a classifier that listens to an audio source (say, from your phone‚Äôs microphone) and predicts whether or not it is Soulja Boy‚Äôs 2008 classic ‚ÄúKiss Me thru the Phone.\" Our classifier is pretty good at detecting when the input stream is ‚ÄùKiss Me thru the Phone\", but it often incorrectly predicts that similar sounding songs are also ‚ÄúKiss Me thru the Phone.\"\n",
    "\n",
    "Complete the sentence: Our classifier has...\n",
    "- low precision and low recall.\n",
    "- low precision and high recall.\n",
    "- high precision and low recall.\n",
    "- high precision and high recall.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd5c40e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Combining precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03257ff2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we care equally about a model's precision $PR$ and recall $RE$, we can combine the two using a single metric called the **F1-score**:\n",
    "\n",
    "$$\\text{F1-score} = \\text{harmonic mean}(PR, RE) = 2\\frac{PR \\cdot RE}{PR + RE}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba7cb7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Both F1-score and accuracy are overall measures of a binary classifier's performance. But remember, accuracy is misleading in the presence of class imbalance, and doesn't take into account the kinds of errors the classifier makes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d5c2b4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other evaluation metrics for binary classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3491d7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We just scratched the surface! This [excellent table from Wikipedia](https://en.wikipedia.org/wiki/Template:Diagnostic_testing_diagram) summarizes the many other metrics that exist.\n",
    "\n",
    "<center><img src='imgs/wiki-table.png' width=75%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8508806b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If you're interested in exploring further, a good next metric to look at is **true negative rate (i.e. specificity)**, which is the analogue of recall for true negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d256731",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting probabilities\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd3cc8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"imgs/needle.png\" width=900>\n",
    "<br>\n",
    "The New York Times maintained <a href=\"https://www.nytimes.com/interactive/2024/11/05/us/elections/results-president-forecast-needle.html\">needles</a><br>that displayed the probabilities of various outcomes in the election.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef363ce",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation: Predicting probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3336863",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Often, we're interested in predicting the **probability** of an event occurring, given some other information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99d93f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Given that the score at the start of the second half is Michigan 23-Northwestern 15,<br>what's the probability that Michigan wins?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d529877",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Here's a picture of an animal. What's the probability it's of a dog? Cat? Hamster? Zebra?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058bf54a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>What's the probability that it snows on campus tomorrow?<br><small>In the context of weather apps, this is a nuanced question; <a href=\"https://xkcd.com/1985\">here's a meme about it</a>.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7f8fe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we're able to predict the probability of an event, we can **classify** the event by using a threshold.<br><small>For example, if we predict there's a 70% chance of Michigan winning, we could predict that Michigan will win. Here, we implicitly used a threshold of 50%.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525835a6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The two classification techniques we've seen so far ‚Äì $k$-Nearest Neighbors and decision trees ‚Äì **don't** directly use probabilities in their decision-making process.<br><small>But sometimes it's helpful to model uncertainty and to be able to state a level of confidence along with a prediction!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156660b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Predicting diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a03f64",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's try to predict whether or not a patient has diabetes (`'Outcome'`) given just their `'Glucose'` level.<br><small>Last class, we used both `'Glucose'` and `'BMI'`; we'll start with just one feature for now.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a3603",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As before, <span style='color: orange'><b>class 0 (orange) is \"no diabetes\"</b></span> and <span style='color: blue'><b>class 1 (blue) is \"diabetes\"</b></span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19bb32",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de394009",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that as a patient's `'Glucose'` value increases, the **chances they have diabetes** also increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c947d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can we model this probability directly, as a function of `'Glucose'`?<br>In other words, can we find some $f$ such that:\n",
    "\n",
    "$$P(y = 1 | \\text{Glucose}) = f(\\text{Glucose})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c6bd9a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An attempt to predict probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285acfe9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's try and fit a simple linear model to the data from the previous slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682106c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_linear_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94607f97",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The <span style=\"color:#097054\"><b>simple linear model</b></span> above predicts values greater than 1 and less than 0! This means we can't interpret the outputs as probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892145a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We could, technically, **clip** the outputs of the linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded7b30",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_linear_model_clipped(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a6c81",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bins and proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be83a3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another approach we could try is to:\n",
    "    - Place `'Glucose'` values into **bins**, e.g. 50 to 55, 55 to 60, 60 to 65, etc.\n",
    "    - Within each bin, compute the proportion of patients in the training set who had diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4648fe",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the source code in lec24_util.py to see how we did this!\n",
    "# We've hidden a lot of the plotting code in the notebook to make it cleaner.\n",
    "util.make_prop_plot(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cda3e4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, the point near a `'Glucose'` value of 100 has a $y$-axis value of ~0.25. This means that about 25\\% of patients with a `'Glucose'` value near 100 had diabetes in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d8932",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, if a new person comes along with a `'Glucose'` value near 100, we'd predict there's a 25\\% chance they have diabetes (so they likely do not)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d50f75",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Notice that the points form an S-shaped curve!**<br><small>Can we incorporate this S-shaped curve in how we predict probabilities?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff79eca",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The logistic function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5b146",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **logistic function** resembles an $S$-shape.\n",
    "\n",
    "    $$\\sigma(t) = \\frac{1}{1 + e^{-t}} = \\frac{1}{1 + \\text{exp}(-t)}$$\n",
    "    \n",
    "    <br><small>The logistic function is an example of a <b>sigmoid function</b>, which is the general term for an S-shaped function. Sometimes, we use the terms \"logistic function\" and \"sigmoid function\" interchangeably.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d5529",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Below, we'll look at the shape of $y = \\sigma(w_0 + w_1 x)$ for different values of $w_0$ and $w_1$.\n",
    "    - $w_0$ controls the position of the curve on the $x$-axis.\n",
    "    - $w_1$ controls the \"steepness\" of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4d057",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_three_sigmoids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1cd2b8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Notice that $0 < \\sigma(t) < 1$, for all $t$, which means **we can interpret the outputs of $\\sigma(t)$ as probabilities**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40df10c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Below, interact with the sliders to change the values of $w_0$ and $w_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18127530",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interact(util.plot_sigmoid, w0=(-15, 15), w1=(-3, 3, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847a4fe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2587eae",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Logistic **regression** is a linear **classification** technique that builds upon linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a3e75",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It models **the probability of belonging to class 1, given a feature vector**:\n",
    "    \n",
    "$$P(y = 1 | \\vec{x}) = \\sigma (\\underbrace{w_0 + w_1 x^{(1)} + w_2 x^{(2)} + ... + w_d x^{(d)}}_{\\text{linear regression model}}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76ab3c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the existence of coefficients, $w_0, w_1, ... w_d$, that we need to learn from the data, tells us that logistic regression is a **parametric** method!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34e301",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LogisticRegression` in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e438f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c310c61",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's fit a `LogisticRegression` classifier. Specifically, this means we're asking `sklearn` to learn the optimal parameters $w_0^*$ and $w_1^*$ in:\n",
    "\n",
    "$$P(y = 1 | \\text{Glucose}) = \\sigma \\left( w_0 + w_1 \\cdot \\text{Glucose} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eea9de",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression()\n",
    "model_logistic.fit(X_train[['Glucose']], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4dee5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We get a test accuracy that's roughly in line with the test accuracies of the two models we saw last class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b0a2f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.score(X_test[['Glucose']], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d67ba",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does our fit model **look like**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e65dc2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing a fit logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bed91d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The values of $w_0^*$ and $w_1^*$ `sklearn` found are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8c057",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.intercept_[0], model_logistic.coef_[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d022dd3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, our fit model is:\n",
    "\n",
    "$$P(y = 1 | \\text{Glucose}) = \\sigma(-5.9015855 + 0.04240496 \\cdot \\text{Glucose})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2e725",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f117749f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, if a patient has a `'Glucose'` level of 150, the model's predicted probability that they have diabetes is:\n",
    "\n",
    "$$\\sigma(-5.9015855 + 0.04240496 \\cdot 150) \\approx \\sigma(0.46) \\approx 0.61$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4e9c5",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.predict_proba([[150]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e819c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><big>How did <code>sklearn</code> find $w_0^*$ and $w_1^*$?<br>What <b>loss function</b> did it use?</big></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0d2a1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-entropy loss\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246b5fa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The modeling recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450ec20",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To train a **parametric model**, we always follow the same three steps.\n",
    "<br><small>$k$-Nearest Neighbors and decision trees didn't quite follow the same process.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0424f535",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Choose a model.\n",
    "\n",
    "$$P(y = 1 | \\vec{x}) = \\sigma (w_0 + w_1 x^{(1)} + w_2 x^{(2)} + ... + w_d x^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}) \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da3b5c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Choose a loss function.\n",
    "\n",
    "<center>???</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eed5eb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Minimize average loss to find optimal model parameters.<br><small>As we've now seen, average loss could also be regularized!</small>\n",
    "\n",
    "<center>???</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb76f5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attempting to use squared loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc312b30",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our default loss function has always been squared loss, so we could try and use it here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da95e5a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2307161",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Unfortunately, there's no closed form solution for $\\vec{w}^*$, so we'll need to use gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c599460",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Before doing so, let's visualize the **loss surface** in the case of our \"simple\" logistic model:\n",
    "\n",
    "$$P(y = 1 | \\text{Glucose}) = \\sigma(w_0 + w_1 \\cdot \\text{Glucose})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179accb6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Specifically, we'll visualize:\n",
    "\n",
    "$$R_\\text{sq}(w_0, w_1) = \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - \\sigma(w_0 + w_1 \\underbrace{x_i}_{\\text{Glucose}_i} ) \\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79864c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic_mse_surface(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef66b01",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **What do you notice?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48396b7f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mean squared error doesn't work well with logistic regression! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fbb2dd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The following function is **not** convex:\n",
    "\n",
    "$$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c541a49",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are two flat \"valleys\" with gradients near 0, where gradient descent could get trapped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7882647",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Additionally, squared loss doesn't penalize bad predictions nearly enough. The largest possible value of:\n",
    "\n",
    "    $$\\left( y_i - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)^2$$\n",
    "\n",
    "    is 1, since both $y_i$ and $\\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$ are **bounded** between 0 and 1, and $(1 - 0)^2 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9213a31",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Suppose $y_i = 1$. Then, the graph of the squared loss of the prediction $p_i$ is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1fef2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_squared_loss_individual()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df7342",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Predicted $p_i$ values near 0 are really bad, since $y_i = 1$, but the loss for $p_i = 0$ is not very high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f1a5ee",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems like we need a loss function that more **steeply penalizes incorrect probability predictions** ‚Äì and hopefully, one that is convex for the logistic regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf1ddf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92333917",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A common loss function in this setting is **log loss**, i.e. **cross-entropy loss**.<br><small>The term \"entropy\" comes from information theory. Watch [**this short video**](https://www.youtube.com/watch?v=ErfnhcEV1O8) for more details.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23299841",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can define the cross-entropy loss function piecewise. If $y_i$ is an observed value and $p_i$ is a predicted **probability**, then: \n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = \\begin{cases} - \\log(p_i) & \\text{if $y_i = 1$} \\\\ -\\log(1 - p_i) & \\text{if $y_i = 0$} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d26a7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that in the two cases ‚Äì $y_i = 1$ and $y_i = 0$ ‚Äì the cross-entropy loss function resembles squared loss, but is unbounded when the predicted probabilities $p_i$ are far from $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cbfce0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_ce_loss_individual_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f88280",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_ce_loss_individual_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedae2b0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A non-piecewise definition of cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be7c65",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can define the cross-entropy loss function piecewise. If $y_i$ is an observed value and $p_i$ is a predicted **probability**, then: \n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = \\begin{cases} - \\log(p_i) & \\text{if $y_i = 1$} \\\\ -\\log(1 - p_i) & \\text{if $y_i = 0$} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1d14f5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An equivalent formulation of $L_\\text{ce}$ that isn't piecewise is:\n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = - \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aee5bb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This formulation is easier to work with algebraically!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75c73c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Average cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee98305",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Cross-entropy loss** for an observed value $y_i$ and predicted **probability** $p_i = P(y = 1 | \\vec{x}_i) = \\sigma \\left(\\vec w \\cdot \\text{Aug}(\\vec x_i) \\right)$ is:\n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = - \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd41b8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find $\\vec{w}^*$, then, we minimize **average cross-entropy loss**:\n",
    "\n",
    "\\begin{align*}R_\\text{ce}(\\vec{w}) &= - \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right) \\\\ &= - \\frac{1}{n} \\sum_{i = 1}^n \\left[ y_i \\log \\left( \\sigma \\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)  + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)\\right) \\right]\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc293bb1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Cross-entropy loss is the default loss function used to find optimal parameters in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8743fd0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There's still no closed-form solution for $\\vec{w}^* = \\underset{\\vec{w}}{\\text{argmin}} \\: R_\\text{ce}(\\vec{w})$, so we'll need to use gradient descent, or some other numerical method.<br><small>But don't worry ‚Äì we'll leave this to `sklearn`!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457876f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Fortunately, average cross-entropy loss is convex, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e17079e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic_ce_surface(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ab92f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- And, it can be regularized!<br><small>By default, `sklearn` applies regularization when performing logistic regression.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc175c4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic_ce_surface(X_train, y_train, reg_lambda=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007206e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The modeling recipe, revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd3dd5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Choose a model.\n",
    "\n",
    "$$P(y = 1 | \\vec{x}) = \\sigma (w_0 + w_1 x^{(1)} + w_2 x^{(2)} + ... + w_d x^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}) \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca1ba7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Choose a loss function.\n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = - \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right)$$\n",
    "\n",
    "$$\\text{where} \\: p_i = P(y = 1 | \\vec{x}_i) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ec506",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Minimize average loss to find optimal model parameters.<br><small>As we've now seen, average loss could also be regularized!</small>\n",
    "\n",
    "    \\begin{align*}R_\\text{ce}(\\vec{w}) &= - \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right) \\\\ &= - \\frac{1}{n} \\sum_{i = 1}^n \\left[ y_i \\log \\left( \\sigma \\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)  + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)\\right) \\right]\\end{align*}\n",
    "\n",
    "    <br>\n",
    "\n",
    "    The actual minimization here is done using numerical methods, through `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7f693",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LogisticRegression` in `sklearn`, revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df5ab2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The `LogisticRegression` class in `sklearn` has a lot of hidden, default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a13ca",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb8331",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It performs $L_2$ regularization (\"ridge logistic regression\") **by default**. The hyperparameter for regularization strength, $C$, is the **inverse** of $\\lambda$; by default, it sets $C = 1$.\n",
    "\n",
    "$$C = \\frac{1}{\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd685a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, for a given value of $C$, it minimizes:\n",
    "\n",
    "$$R_\\text{ce-reg}(\\vec{w}) - \\frac{1}{n} \\sum_{i = 1}^n \\left[ y_i \\log \\left( \\sigma \\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)  + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)\\right) \\right] + \\frac{1}{C} \\sum_{j = 1}^d w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d39f4d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It also specifies `solver='lbfgs'`, i.e. it doesn't use gradient descent per-se, but another more sophisticated numerical method.<br><small>Read more about LBFGS [here](https://en.wikipedia.org/wiki/Limited-memory_BFGS).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e624763",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f121a3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From probabilities to decisions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da684cd2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting probabilities vs. predicting classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ef7ab",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$P(y = 1 | \\vec{x}) = \\sigma (w_0 + w_1 x^{(1)} + w_2 x^{(2)} + ... + w_d x^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}) \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa6ce45",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ü§î **Question**: Suppose our logistic regression model predicts the probability that someone has diabetes is 0.75. What do we predict ‚Äì diabetes or no diabetes? What if the predicted probability is 0.3?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0443bbd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üôã **Answer**: We have to pick a threshold (for example, 0.5)!\n",
    "    - If the predicted probability is above the threshold, we predict diabetes (1).\n",
    "    - Otherwise, we predict no diabetes (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f2158",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting probabilities vs. predicting classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ece508",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, the `predict` method of a fit `LogisticRegression` model predicts a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc37d00",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.predict(pd.DataFrame([{\n",
    "    'Glucose': 150,\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3aece",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, logistic regression is designed to predict **probabilities**. We can access these predicted probabilities using the `predict_proba` method, as we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836408b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.predict_proba(pd.DataFrame([{\n",
    "    'Glucose': 150,\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6debb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The above is telling us that the model thinks this person has:\n",
    "    - A 39% chance of belonging to class 0 (no diabetes).\n",
    "    - A 61% chance of belonging to class 1 (diabetes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13551dd3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, it uses a threshold of 0.5, i.e. it predicts the larger probability.<br>As we'll soon discuss, this may not be what we want!<br><small>Unfortunately, `sklearn` doesn't let us change the threshold ourselves. If we want a different threshold, we need to manually implement it using the results of `predict_proba`.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59169441",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thresholding probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea9e4c0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we did with other classifiers, we can visualize the **decision boundary** of a fit logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374c8d5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we pick a threshold of $T$, any patient with a `'Glucose'` value such that: \n",
    "\n",
    "    $$\\sigma(w_0^* + w_1^* \\cdot \\text{Glucose}) \\geq T$$ \n",
    "\n",
    "    is classified as having diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ebd34",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, if $T = 0.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a680d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic_and_y_threshold(X_train, y_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a2ca9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we set $T = 0.5$, then patients with `'Glucose'` values above $\\approx$ 140 are classified as having diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552692b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic_and_x_threshold(X_train, y_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3940d1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **How do we find the exact $x$-axis position of the <span style=\"color:purple\">decision boundary</span> above?**<br><small>If we can, then we'd be able to predict whether someone has diabetes just by looking at their `'Glucose'` value.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b99a8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision boundaries for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187e943",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In our single feature model that predicts `'Outcome'` given just `'Glucose'`, our predicted probabilities are of the form:\n",
    "\n",
    "$$P(y = 1 | \\text{Glucose}) = \\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7d722",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we fix a threshold, $T$. Then, our <b><span style=\"color:purple\">decision boundary</span></b> is of the form:\n",
    "\n",
    "$$\\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose} \\right) = T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e683459a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we can invert $\\sigma(t)$, then we can re-arrange the above to solve for the `'Glucose'` value at the threshold:\n",
    "\n",
    "$$\\text{Glucose}_\\text{T} = \\frac{\\sigma^{-1}(T) - w_0^*}{w_1^*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb037605",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Important**: If $p = \\sigma(t)$, then $\\sigma^{-1}({p}) = \\log \\left( \\frac{p}{1-p} \\right)$ is the inverse of $\\sigma(t)$.<br><small>$\\sigma^{-1}(p)$ is called the **logit** function.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb14ed",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7904e3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose an event occurs with probability $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45567ae8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **odds** of that event are:\n",
    "\n",
    "$$\\text{odds}(p) = \\frac{p}{1-p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab09107",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For instance, if there's a $p = \\frac{3}{4}$ chance that Michigan wins this week, then the **odds** that Michigan wins this week are:\n",
    "\n",
    "    $$\\text{odds} \\left( \\frac{3}{4} \\right) = \\frac{\\frac{3}{4}}{\\frac{1}{4}} = 3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02385c21",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Interpretation: it's 3 times more likely that Michigan wins than loses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c84f2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **We can interpret $\\sigma^{-1}(p) = \\log \\left( \\frac{p}{1-p} \\right)$ as the \"log odds\" of $p$!**<br><small>See the reference slides for more details.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11d78e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving for the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb485b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Previously, we said that if we pick a threshold $T$, then:\n",
    "\n",
    "$$\\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose} \\right) = T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b70e22",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We re-arranged this for the `'Glucose'` value on the threshold, $\\text{Glucose}_T$:\n",
    "\n",
    "$$\\text{Glucose}_\\text{T} = \\frac{\\sigma^{-1}(T) - w_0^*}{w_1^*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a081a6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Using the fact that $\\sigma^{-1}(T) = \\log \\left( \\frac{T}{1 - T} \\right)$ gives us a closed-form formula for $\\text{Glucose}_T$!\n",
    "\n",
    "$$\\text{Glucose}_\\text{T} = \\frac{\\log \\left( \\frac{T}{1-T} \\right) - w_0^*}{w_1^*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8220f4c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **This explains why $\\text{Glucose} \\geq 139.17$ is the <span style=\"color:purple\">decision boundary</span> below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e613d0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w0_star = model_logistic.intercept_[0]\n",
    "w1_star = model_logistic.coef_[0][0]\n",
    "T = 0.5\n",
    "glucose_threshold = (np.log(T / (1 - T)) - w0_star) / w1_star\n",
    "glucose_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa986a47",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic_and_x_threshold(X_train, y_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c906be3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The decision boundary in the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2361b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The decision boundary on the previous slide is:\n",
    "\n",
    "$$\\text{Glucose}_T \\geq 139.17$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263923c9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's visualize this in the **feature space**. We are just using $d = 1$ feature, so let's visualize our decision boundary with a 1D plot, i.e. a number line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016e2b7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_in_1D(X_train, y_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190262e3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression with multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c1c78",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, as we did last class, let's use both `'Glucose'` and `'BMI'` to predict diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cbea36",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.make_two_feature_scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d7ec25",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Specifically, our fit model will look like:\n",
    "\n",
    "$$P(y = 1 | \\text{Glucose}, \\text{BMI}) = \\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose} + w_2^* \\cdot \\text{BMI} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81493c0f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic_multiple = LogisticRegression()\n",
    "model_logistic_multiple.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa6807",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- After minimizing mean (regularized!) cross-entropy loss, we find that our fit model is of the form:\n",
    "\n",
    "$$P(y = 1 | \\text{Glucose}, \\text{BMI}) = \\sigma \\left( -8.1697 + 0.0394 \\cdot \\text{Glucose} + 0.0802 \\cdot \\text{BMI} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f85632",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic_multiple.intercept_, model_logistic_multiple.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4630c9a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing a fit logistic regression model with two features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9561a33",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall, the logistic regression model is trained to predict the probability of <b><span style=\"color:blue\">class 1 (diabetes)</span></b>.\n",
    "\n",
    "$$P(y = 1 | \\text{Glucose}, \\text{BMI}) = \\sigma \\left( -8.1697 + 0.0394 \\cdot \\text{Glucose} + 0.0802 \\cdot \\text{BMI} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf05df9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The graph below shows the predicted probabilities of <b><span style=\"color:blue\">class 1 (diabetes)</span></b> for different combinations of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54a696",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic(model_logistic_multiple, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da0295",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The decision boundary in the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aedc7c5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does the resulting decision boundary look like, in a $d = 2$ dimensional plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c35904",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_decision_boundary(model_logistic_multiple, X_train, y_train, title='Decision Boundary when Using Both Glucose and BMI \\n and T = 0.5 (the default)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd9eb7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that unlike the decision boundaries for $k$-Nearest Neighbors and decision trees, this decision boundary is **linear**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b39111",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Specifically, the decision boundary in the feature space is of the form:\n",
    "\n",
    "$$a \\cdot \\text{Glucose} + b \\cdot \\text{BMI} + c = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52bc2d0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **In the homework, you'll solve for $a$, $b$, and $c$ in a similar example!**<br><small>It involves retracing the steps we followed in the single-feature case.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c266d9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b509532",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lingering questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29c57b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, a fit `LogisticRegression` object's `predict` method uses a threshold of $T = 0.5$ to decide when to predict class 1 vs. class 0. What if we want to use a different threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072cdd7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "\n",
    "### Properties of the logistic function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2915ccf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- The logistic function, $\\sigma(t)$, obeys several interesting properties. \n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe9b57",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- It is **symmetric**.\n",
    "\n",
    "$$\\sigma(-t) = 1 - \\sigma(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0585daa5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Its **derivative** is conveniently calculated:\n",
    "\n",
    "$$\\frac{d}{dt}\\sigma(t) = \\sigma(t) (1 - \\sigma(t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c247eaaf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- But, most relevant to us right now, its **inverse** is:\n",
    "\n",
    "$$p = \\sigma(t) \\implies t = \\sigma^{-1}(p) = \\log \\left( \\frac{p}{1-p} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f452b9ce",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "\n",
    "### Linearity of log odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352821ca",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Let $p$ represent our predicted probability.\n",
    "\n",
    "$$p = P(y = 1 | \\text{Glucose} ) = \\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb9b47",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Using the inverse of the logistic function, we have that:\n",
    "\n",
    "$$w_0^* + w_1^* \\cdot \\text{Glucose} = \\log \\left( \\frac{p}{1 - p} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea42c782",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- On the left, we have a **linear function of $\\text{Glucose}$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610eb990",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- On the right, we have the **log of the odds** of $p$.<br><small>We call the \"log of the odds\" the \"log odds\".</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b37360",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- **Important**: The logistic regression model assumes that **the log of the odds of $P(y = 1 | \\vec{x})$ is linear!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011822fa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "\n",
    "### Implications of the linearity of log odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec2e2a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Suppose that $w_0^* = -6$ and $w_1^* = 0.05$. Then:\n",
    "\n",
    "$$P(y = 1 | \\text{Glucose}) = \\sigma(-6 + 0.05 \\cdot \\text{Glucose})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f82f7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- It's hard to interpret the role of the coefficient $0.05$ directly. But, we know that:\n",
    "\n",
    "$$-6 + 0.05 \\cdot \\text{Glucose} = \\log \\left( \\frac{p}{1 - p} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d1081",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Example: Suppose my `'Glucose'` level increases by 1 unit. Then, the predicted log odds that I have diabetes increases by 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652cc071",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- But, since:\n",
    "\n",
    "$$e^{-6 + 0.05 \\cdot \\text{Glucose}} = \\frac{p}{1-p} = \\text{odds}(p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ced1d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- And:\n",
    "\n",
    "$$e^{-6 + 0.05 \\cdot (Glucose + 1)} = e^{-6 + 0.05 \\cdot \\text{Glucose}} \\cdot e^{0.05}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3d5d3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- We can say that **if my `'Glucose'` level increases by 1 unit, then my predicted odds of diabetes increases by a _factor_ of $e^{0.05}$**, or more generally $e^{w_1^*}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae990b9e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- You'll need this interpretation in Homework 10, Question 6!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
